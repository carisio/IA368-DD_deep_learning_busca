# Repositório para a disciplina IA368-DD Deep Learning aplicado a sistemas de buscas
*Leandro Carísio Fernandes*

<br>

## 0. Seleção para aluno especial

Projeto: Foi solicitada a construção de um sistema de recuperação de informação usando o algoritmo BM25, com resultados sendo avaliados na base de dados CISI.

- [Relatório](./0%20-%20selecao%20-%20bm25%20e%20cisi%20collection/README.md)
- Implementação: [Jupyter notebook](./0%20-%20selecao%20-%20bm25%20e%20cisi%20collection/notebook/bm25-cisi.ipynb) / [Colab](https://colab.research.google.com/drive/1au_hUeSkTk5u6d4Se2wZJAZqP-avW510?usp=sharing)

<br> 

## Aula 1. Buscador Simples: Booleano, TF-IDF, BM25

Leitura: Seção 1 do artigo ["Pretrained Transformers for Text Ranking: BERT and Beyond"](https://arxiv.org/abs/2010.06467)

Projeto: (1) Usar o BM25 implementado pelo pyserini para buscar queries no TREC-DL 2020; (2) Implementar um buscador booleano/bag-of-words; (3) Implementar um buscador com TF-IDF; (4) Avaliar implementações 1, 2, e 3 no TREC-DL 2020 e calcular o nDCG@10.

Entregas: 

- [Apresentação da leitura](./1%20-%20bm25%20-%20bow%20-%20tfidf/apresentacao/capitulo_1.pdf)

- Implementação: [Jupyter notebook](./1%20-%20bm25%20-%20bow%20-%20tfidf/notebook/Aula1_bm25_bow_tfidf.ipynb) / [Colab](https://colab.research.google.com/drive/1hELJYqsvUyja9HPeDzc9FU8okqdIjODE?usp=sharing)

<br>

## Aula 2. Classificador binário: Análise de Sentimento e Ranqueamento

Leitura: Seção 3 (até 3.2.2) do artigo ["Pretrained Transformers for Text Ranking: BERT and Beyond"](https://arxiv.org/abs/2010.06467)

Projeto: Reranqueamento usando um modelo estilo-BERT com o treinamento no dataset do MS MARCO e avaliação no TREC-DL 2020

- Apresentação da leitura

- Implementação: [Jupyter notebook](./2%20-%20classificador%20binario%20-%20reranking%20com%20minilm/notebook/Aula2_classificador_binario_mini_bert.ipynb) / [Colab](https://colab.research.google.com/drive/1Xcz-h7uHpbuKNZLlOrlIqj8Ao4V3iQos?usp=sharing) / [Apresentação](./2%20-%20classificador%20binario%20-%20reranking%20com%20minilm/notebook/apresentacao_notebook.pdf)

<br>

## Aula 3. Aplicar LLM's Zero e Few-shot (aplicação escolhida pelo aluno)

Leitura: [GPT-3: Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)

Projeto:

<br>

## Aula 4. Transformer avançado: Implementação e treinamento (modelagem de linguagem)

Leitura: [A neural probabilistic language model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) ou [GPT-2](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)

Projeto:

<br>

## Aula 5. Modelo seq2seq: T5 para expansão de documentos (doc2query)

Projeto:

<br>

## Aula 6. Buscadores Densos: DPR

Projeto:

<br>

## Aula 7. Buscadores Esparsos: SPLADE

Projeto:

<br>

## Aula 8. InPars: Adaptação de modelos para novas tarefas

Projeto:

<br>

## Aula 9. Destilação

Projeto:

<br>

## Aula 10. Multi-document QA: Visconde

Projeto:

<br>

