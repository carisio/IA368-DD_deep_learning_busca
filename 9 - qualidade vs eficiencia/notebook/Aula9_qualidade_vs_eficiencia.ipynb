{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Aula 9 - Qualidade versus eficiência\n",
        "\n",
        "Leandro Carísio Fernandes\n",
        "\n",
        "<br>\n",
        "\n",
        "Projeto:\n",
        "\n",
        "O objetivo do exercício desta semana é construir alguns pipelines de busca e analisá-los em termos das seguintes métricas:\n",
        "\n",
        "- Qualidade dos resultados: nDCG@10\n",
        "- Latência (seg/query)\n",
        "- USD por query assumindo utilização \"perfeita\": assim que terminou de processar uma query, já tem outra para ser processada\n",
        "- USD/mês para deixar o sistema rodando para poucos usuários (ex: 100 queries/dia)\n",
        "- Custo de indexação em USD\n",
        "\n",
        "Iremos avaliar os pipelines no TREC-COVID.\n",
        "\n",
        "A latência precisa ser menor que 2 segundos por query.\n",
        "\n",
        "Considerar:\n",
        "\n",
        "- 1,50 USD/hora por A100 ou 0,21 USD/hora por T4 ou 0,50 USD/hora por V100\n",
        "- 0,03 USD/hora por CPU core\n",
        "- 0,005 USD/hora por GB de CPU RAM\n",
        "\n",
        "\n",
        "Dicas:\n",
        "- Utilizar modelos de busca \"SOTA\" já treinados no MS MARCO como parte do pipeline, como o [SPLADE distil](https://huggingface.co/naver/splade-cocondenser-selfdistil) (esparso), [contriever](https://huggingface.co/facebook/contriever-msmarco) (denso), [Colbert-v2](https://github.com/stanford-futuredata/ColBERT) (denso), [miniLM](https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-6-v2) (reranker), [monoT5-3B](https://huggingface.co/castorini/monot5-3b-msmarco) (reranker), [doc2query minus-minus](https://github.com/terrierteam/pyterrier_doc2query) (expansão de documentos + filtragem com reranqueador na etapa de indexação)\n",
        "- Variar parametros como número de documentos retornados em cada estagio. Por exemplo, BM25 retorna 1000 documentos, um modelo denso ou esparso pode reranquea-los, e passar os top 50 para o miniLM/monoT5 fazer um ranqueamento final."
      ],
      "metadata": {
        "id": "UCHkcMmug77n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5cADUpcXtHZ"
      },
      "outputs": [],
      "source": [
        "# Diretório para a aula 9\n",
        "aula9_dir = '/content/drive/My Drive/IA368-DD_deep_learning_busca/Aula9-qualidade-vs-eficiencia/'\n",
        "\n",
        "# Arquivo com as queries do TREC_COVID já expandidas, recuperado da aula 5\n",
        "arquivo_docs_queries_expandidas = '/content/drive/My Drive/IA368-DD_deep_learning_busca/Aula5-t5-doc2query/doc_com_queries_expandidas.pickle'\n",
        "\n",
        "# Parâmetros usados na Aula7 para gerar o índice usando SPLADE.\n",
        "\n",
        "# Esse conjunto de parâmetros é pro índice SPLADE com os textos originais\n",
        "# nDCG de 0.7354 só splade (batch: 32 e max_seq_length: 256)\n",
        "# param_splade = {\n",
        "#    'agg': 'max',\n",
        "#    'nome_modelo': 'naver/splade-cocondenser-selfdistil',\n",
        "#    'manter_contribuicao_CLS_SEP_da_matriz_doc': True,\n",
        "#    'nome_arquivo_indice_invertido_docs': 'idx_splade_sem_doc2query_batch32_seq_256.pickle',\n",
        "#    'indexar_com_doc2query': False,\n",
        "#    'indexar_so_expansao': False,\n",
        "#    'batch_size': 32,\n",
        "#    'max_seq_length': 256,\n",
        "#    'gerar_arquivo_indice': False # Se for True, gera e salva. Se for False reutiliza o arquivo já gerado\n",
        "# }\n",
        "\n",
        "# Esse conjunto de parâmetros é pro índice SPLADE considerando expansão dos docs via doc2query.\n",
        "# nDCG de 0.7298 com doc2query + splade (batch: 32 e max_seq_length: 256)\n",
        "param_splade = {\n",
        "   'agg': 'max',\n",
        "   'nome_modelo': 'naver/splade-cocondenser-selfdistil',\n",
        "   'manter_contribuicao_CLS_SEP_da_matriz_doc': True,\n",
        "   'nome_arquivo_indice_invertido_docs': 'idx_splade_com_doc2query_batch32_seq_256.pickle',\n",
        "   'indexar_com_doc2query': True,\n",
        "   'indexar_so_expansao': False,\n",
        "   'batch_size': 32,\n",
        "   'max_seq_length': 256,\n",
        "   'gerar_arquivo_indice': False # Se for True, gera e salva. Se for False reutiliza o arquivo já gerado\n",
        "}\n",
        "\n",
        "# Esse conjunto de parâmetros é pro índice SPLADE considerando apenas a expansão dos documentos via doc2query (ou seja, descartando o documento original)\n",
        "# nDCG de 0.6193 com doc2query (desconsiderando o doc. original) + splade (batch: 32 e max_seq_length: 256)\n",
        "#     -> Obs.: o BM25 só com a expansão dava 0.5225\n",
        "# param_splade = {\n",
        "#    'agg': 'max',\n",
        "#    'nome_modelo': 'naver/splade-cocondenser-selfdistil',\n",
        "#    'manter_contribuicao_CLS_SEP_da_matriz_doc': True,\n",
        "#    'nome_arquivo_indice_invertido_docs': 'idx_splade_com_doc2query_so_expansao_batch32_seq_256.pickle',\n",
        "#    'indexar_com_doc2query': True,\n",
        "#    'indexar_so_expansao': True,\n",
        "#    'batch_size': 32,\n",
        "#    'max_seq_length': 256,\n",
        "#    'gerar_arquivo_indice': False\n",
        "# }\n",
        "\n",
        "# Parâmetros para reranking com InPars. O modelo usado nesse notebook é o fine-tuning do cross-encoder/ms-marco-MiniLM-L-6-v2\n",
        "# com o fine-tuning feito na Aula anterior (8).\n",
        "param_inpars = {\n",
        "    'nome_modelo': '/content/drive/My Drive/IA368-DD_deep_learning_busca/Aula8-inpars/modelos/1neg_1pos_teste_apaga_token_type_id_no_treino/validacao',\n",
        "    'nome_tokenizer': '/content/drive/My Drive/IA368-DD_deep_learning_busca/Aula8-inpars/modelos/1neg_1pos_teste_apaga_token_type_id_no_treino/validacao', #'cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
        "    'max_seq_length': 512,\n",
        "    'batch_size': 64,\n",
        "    'considerar_expansao_no_reranking': True\n",
        "}\n",
        "\n",
        "# Dados do TREC-COVID\n",
        "url_trec_covid = 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/trec-covid.zip'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dvrYMGnX872",
        "outputId": "c5ad4216-a777-47f8-b095-31bb0d147a1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAGzwWMVg4O6",
        "outputId": "6bd2ba3f-6998-40ef-a297-1ba0a3c40661"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed May 10 15:30:55 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    42W / 300W |   3442MiB / 16384MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets -q\n",
        "!pip install transformers sentence-transformers -q\n",
        "!pip install pyserini -q\n",
        "!pip install faiss-gpu -q"
      ],
      "metadata": {
        "id": "K5pPTbxLam5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carrega queries e documentos do TREC-COVID"
      ],
      "metadata": {
        "id": "KtrmoKcwblkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "if not Path('./collections/trec-covid.zip').is_file():\n",
        "  !wget {url_trec_covid} -P collections # type: ignore\n",
        "  !unzip -o collections/trec-covid.zip -d ./collections # type: ignore\n",
        "\n",
        "# Converte o qrels que veio no trec-covid.zip pra o formato esperado:\n",
        "with open('./collections/trec-covid/qrels/test.tsv', 'r') as fin:\n",
        "  data = fin.read().splitlines(True)\n",
        "with open('./collections/trec-covid/qrels/test_corrigido.tsv', 'w') as fout:\n",
        "  for linha in data[1:]:\n",
        "    campos = linha.split()\n",
        "    fout.write(f'{campos[0]}\\t0\\t{campos[1]}\\t{campos[2]}\\n')\n",
        "\n",
        "def carrega_corpus_trec_covid():\n",
        "  retorno = []\n",
        "  with open('./collections/trec-covid/corpus.jsonl') as corpus:\n",
        "    for i, line in enumerate(corpus):\n",
        "      doc = json.loads(line)\n",
        "      #retorno.append({\n",
        "      #    'id': doc['_id'],\n",
        "      #    'doc': f\"{doc['title']} {doc['text']}\"\n",
        "      #})\n",
        "      retorno.append(\n",
        "          (doc['_id'], f\"{doc['title']} {doc['text']}\")\n",
        "      )\n",
        "      if (i % 10000 == 0):\n",
        "        print(f'Processado {i} documentos')\n",
        "    return retorno\n",
        "\n",
        "def carrega_queries_trec_covid():\n",
        "  retorno = []\n",
        "  with open('./collections/trec-covid/queries.jsonl') as queries:\n",
        "    for line in queries:\n",
        "      query = json.loads(line)\n",
        "      # Faz apenas uma pequena tradução de _id para id e text para texto\n",
        "      retorno.append({'id': query['_id'], 'texto': query['text']})\n",
        "  return retorno\n",
        "\n",
        "queries_trec_covid = carrega_queries_trec_covid()\n",
        "corpus_trec_covid = carrega_corpus_trec_covid()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TprdeyYgbo4p",
        "outputId": "c5395a79-0bc8-44fc-f4a5-b5c779a1ec58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processado 0 documentos\n",
            "Processado 10000 documentos\n",
            "Processado 20000 documentos\n",
            "Processado 30000 documentos\n",
            "Processado 40000 documentos\n",
            "Processado 50000 documentos\n",
            "Processado 60000 documentos\n",
            "Processado 70000 documentos\n",
            "Processado 80000 documentos\n",
            "Processado 90000 documentos\n",
            "Processado 100000 documentos\n",
            "Processado 110000 documentos\n",
            "Processado 120000 documentos\n",
            "Processado 130000 documentos\n",
            "Processado 140000 documentos\n",
            "Processado 150000 documentos\n",
            "Processado 160000 documentos\n",
            "Processado 170000 documentos\n",
            "CPU times: user 1.48 s, sys: 60.4 ms, total: 1.54 s\n",
            "Wall time: 1.52 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_trec_covid[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubNQt-bWHFM5",
        "outputId": "46e15892-e14a-4a0b-c871-f96f1ba5fde0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('ug7v899j',\n",
              " 'Clinical features of culture-proven Mycoplasma pneumoniae infections at King Abdulaziz University Hospital, Jeddah, Saudi Arabia OBJECTIVE: This retrospective chart review describes the epidemiology and clinical features of 40 patients with culture-proven Mycoplasma pneumoniae infections at King Abdulaziz University Hospital, Jeddah, Saudi Arabia. METHODS: Patients with positive M. pneumoniae cultures from respiratory specimens from January 1997 through December 1998 were identified through the Microbiology records. Charts of patients were reviewed. RESULTS: 40 patients were identified, 33 (82.5%) of whom required admission. Most infections (92.5%) were community-acquired. The infection affected all age groups but was most common in infants (32.5%) and pre-school children (22.5%). It occurred year-round but was most common in the fall (35%) and spring (30%). More than three-quarters of patients (77.5%) had comorbidities. Twenty-four isolates (60%) were associated with pneumonia, 14 (35%) with upper respiratory tract infections, and 2 (5%) with bronchiolitis. Cough (82.5%), fever (75%), and malaise (58.8%) were the most common symptoms, and crepitations (60%), and wheezes (40%) were the most common signs. Most patients with pneumonia had crepitations (79.2%) but only 25% had bronchial breathing. Immunocompromised patients were more likely than non-immunocompromised patients to present with pneumonia (8/9 versus 16/31, P = 0.05). Of the 24 patients with pneumonia, 14 (58.3%) had uneventful recovery, 4 (16.7%) recovered following some complications, 3 (12.5%) died because of M pneumoniae infection, and 3 (12.5%) died due to underlying comorbidities. The 3 patients who died of M pneumoniae pneumonia had other comorbidities. CONCLUSION: our results were similar to published data except for the finding that infections were more common in infants and preschool children and that the mortality rate of pneumonia in patients with comorbidities was high.')"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recupera queries expandidas usando doc2query treinado na Aula 5"
      ],
      "metadata": {
        "id": "6YiPmqFBYxO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "import pickle\n",
        "\n",
        "dict_trec_covid_com_e_sem_expansao = OrderedDict({})\n",
        "with open(arquivo_docs_queries_expandidas, 'rb') as f:\n",
        "  dict_trec_covid_com_e_sem_expansao = pickle.load(f)"
      ],
      "metadata": {
        "id": "3PMr01U4YAT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apenas checa o formato dos dados (Dicionário com chave sendo a key do documento do TREC-COVID e o valor é um {doc_original: string, query_expandida: string})\n",
        "dict_trec_covid_com_e_sem_expansao['mu5u5bvj']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkP35w9bYhrw",
        "outputId": "8f8baa2f-5505-4845-c006-cdb9a7824bf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'doc_original': 'Relative cost and outcomes in the intensive care unit of acute lung injury (ALI) due to pandemic influenza compared with other etiologies: a single-center study BACKGROUND: Critical illness due to 2009 H1N1 influenza has been characterized by respiratory complications, including acute lung injury (ALI) or acute respiratory distress syndrome (ARDS), and associated with high mortality. We studied the severity, outcomes, and hospital charges of patients with ALI/ARDS secondary to pandemic influenza A infection compared with ALI and ARDS from other etiologies. METHODS: A retrospective review was conducted that included patients admitted to the Cleveland Clinic MICU with ALI/ARDS and confirmed influenza A infection, and all patients admitted with ALI/ARDS from any other etiology from September 2009 to March 2010. An itemized list of individual hospital charges was obtained for each patient from the hospital billing office and organized by billing code into a database. Continuous data that were normally distributed are presented as the mean ± SD and were analyzed by the Student’s t test. The chi-square and Fisher exact tests were used to evaluate differences in proportions between patient subgroups. Data that were not normally distributed were compared with the Wilcoxon rank-sum test. RESULTS: Forty-five patients were studied: 23 in the H1N1 group and 22 in the noninfluenza group. Mean ± SD age was similar (44 ± 13 and 51 ± 17 years, respectively, p = 0.15). H1N1 patients had lower APACHE III scores (66 ± 20 vs. 89 ± 32, p = 0.015) and had higher Pplat and PEEP on days 1, 3, and 14. Hospital and ICU length of stay and duration of mechanical ventilation were comparable. SOFA scores over the first 2 weeks in the ICU indicate more severe organ failure in the noninfluenza group (p = 0.017). Hospital mortality was significantly higher in the noninfluenza group (77 vs. 39%, p = 0.016). The noninfluenza group tended to have higher overall charges, including significantly higher cost of blood products in the ICU. CONCLUSIONS: ALI/ARDS secondary to pandemic influenza infection is associated with more severe respiratory compromise but has lower overall acuity and better survival rates than ALI/ARDS due to other causes. Higher absolute charges in the noninfluenza group are likely due to underlying comorbid medical conditions.',\n",
              " 'query_expandida': 'how many patients were compared to patients with ali how many patients were in the hospital with a drh due to the influenza how many patients are hospitalized for influenza how are aridenza patients diagnosed what was the sex mortality of ali/ARDS patients after pandemic influenza how many people were diagnosed with ali due to pandemic influenza how do hospital charges for a patient with a h1n1 infection compare to other etiologies how old is the patient of ali how many patients are suffering from ali after pandemic average mortality for patients with acute lung injury due to pandemic influenza'}"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_trec_covid, docs_trec_covid = list(dict_trec_covid_com_e_sem_expansao.keys()), list(dict_trec_covid_com_e_sem_expansao.values())\n",
        "textos_trec_covid_expandidos = [f\"{doc['doc_original']} {doc['query_expandida']}\" for doc in docs_trec_covid]\n",
        "textos_trec_covid_original = [doc['doc_original'] for doc in docs_trec_covid]\n",
        "textos_trec_covid_so_expansao = [doc['query_expandida'] for doc in docs_trec_covid]"
      ],
      "metadata": {
        "id": "Ry8jzQ8gcuuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline 1: doc2query + SPLADE"
      ],
      "metadata": {
        "id": "n1ELOf2BZGqm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recuperando o código SPLADE (aula 7)"
      ],
      "metadata": {
        "id": "xEuZq3X1gQ6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "import torch\n",
        "from torch.nn.functional import relu\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def carregar_tokenizador_e_modelo_splade(nome):\n",
        "  tokenizer_splade = AutoTokenizer.from_pretrained(nome)\n",
        "  model_splade = AutoModelForMaskedLM.from_pretrained(nome).to(device)\n",
        "\n",
        "  return tokenizer_splade, model_splade\n",
        "\n",
        "def representacao_esparsa_do_texto(model_splade, tokenizer_splade, texto, add_special_tokens=True, manter_contribuicao_cls_sep=False):\n",
        "  # Tokeniza o texto\n",
        "  # Para texto = \"The quick brown jumps over the lazy dog\",\n",
        "  # tokens = ['the', 'quick', 'brown', 'jumps', 'over', 'the', 'lazy', 'dog'] (tamanho 8)\n",
        "  # token_ids = [1996, 4248, 2829, 103, 14523, 2058, 1996, 13971, 3899] (tamanho 8)\n",
        "\n",
        "  # Roda o modelo\n",
        "  inputs = tokenizer_splade(texto, add_special_tokens=add_special_tokens,\n",
        "                     return_special_tokens_mask=True,\n",
        "                     return_tensors='pt',\n",
        "                     truncation=True,\n",
        "                     max_length=256)\n",
        "\n",
        "  with torch.autocast(device_type=str(device), dtype=torch.float16, enabled=True):\n",
        "    with torch.no_grad():\n",
        "      outputs = model_splade(input_ids=inputs['input_ids'].to(device), attention_mask=inputs['attention_mask'].to(device))\n",
        "\n",
        "  # Acessa os logits  \n",
        "  # outputs.logits.size() = torch.Size([1, 8, 30522])\n",
        "  # logits.size() = torch.Size([8, 30588])\n",
        "  logits = outputs.logits[0, :]\n",
        "\n",
        "  # Pelo artigo, agora a gente calcula somatório [ log(1 + ReLU(w_ij)) ]\n",
        "  # relu(logits) vai manter o mesmo tamanho: [8, 30588]\n",
        "  # 1 + relu(logits) também vai manter o mesmo tamanho: [8, 30588]\n",
        "  # log(1 + relu(logits)) também vai manter o mesmo tamanho: [8, 30588]\n",
        "  # Feito isso, calcula o somatório na dim=0, o que vai gerar um vetor de tamanho\n",
        "  # 30588, que é o tamanho do vocabulário:\n",
        "      \n",
        "  mask_tokens_validos = 1 - inputs['special_tokens_mask'].to(device)\n",
        "  mask = mask_tokens_validos.squeeze().unsqueeze(-1).expand(logits.size())\n",
        "\n",
        "  if manter_contribuicao_cls_sep:\n",
        "    mask = torch.ones(mask.size()).to(device) # Como não tem batch envolvido, isso é o mesmo que a attention mask\n",
        "\n",
        "  if param_splade['agg'] == 'sum':\n",
        "    wj = torch.sum(torch.log(1 + relu(logits*mask)), dim=0)\n",
        "  else:\n",
        "    wj, _ = torch.max(torch.log(1 + relu(logits*mask)), dim=0)\n",
        "\n",
        "  # Agora temos que armazenar esse vetor de forma esparsa...\n",
        "  return wj.to_sparse()\n",
        "\n",
        "def converte_token_ids_para_tokens(tokenizer_splade, ids_tokens):\n",
        "  return tokenizer_splade.convert_ids_to_tokens(ids_tokens)"
      ],
      "metadata": {
        "id": "UZWYOimiZN4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_splade, model_splade = carregar_tokenizador_e_modelo_splade(param_splade['nome_modelo'])\n",
        "model_splade.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1tPNRs9a3uc",
        "outputId": "b7fd9e7e-ec0e-41c8-c8a4-016033ce7932"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForMaskedLM(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (cls): BertOnlyMLMHead(\n",
              "    (predictions): BertLMPredictionHead(\n",
              "      (transform): BertPredictionHeadTransform(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (transform_act_fn): GELUActivation()\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils import data\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Definição do Dataset\n",
        "class DatasetSplade(data.Dataset):\n",
        "    # Recebe apenas um vetor de textos\n",
        "    def __init__(self, tokenizer_splade, textos, max_seq_length):\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.tokenizer_splade = tokenizer_splade\n",
        "        self.textos = textos\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.textos)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # Aqui é só uma passada no eval, não precisa de cache\n",
        "        item = self.tokenizer_splade(self.textos[idx],\n",
        "                       padding=True,\n",
        "                       return_special_tokens_mask=True,\n",
        "                       # No exemplo dos autores eles não removem o CLS/SEP\n",
        "                       # https://github.com/naver/splade/blob/main/inference_splade.ipynb\n",
        "                       add_special_tokens=True, \n",
        "                       truncation=True,\n",
        "                       max_length=self.max_seq_length\n",
        "                )\n",
        "        return item"
      ],
      "metadata": {
        "id": "VDFjUcywbKmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BatchEncoding\n",
        "\n",
        "def collate_fn_splade(batch):\n",
        "    return BatchEncoding(tokenizer_splade.pad(batch, return_tensors='pt'))"
      ],
      "metadata": {
        "id": "wQ1qey4kbNyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "def representacao_esparsa_dataloader(model_splade, dataloader, func_executar_apos_batch = lambda idx_batch, wj_batch : None):\n",
        "  with torch.autocast(device_type=str(device), dtype=torch.float16, enabled=True):\n",
        "    with torch.no_grad():\n",
        "      for i_batch, batch in enumerate(tqdm(dataloader)):\n",
        "        outputs = model_splade(input_ids = batch['input_ids'].to(device), attention_mask = batch['attention_mask'].to(device))\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Na hora de recuperar os logits, temos duas opções. Ou consideramos só a attention mask (ou seja, \n",
        "        # exclui apenas os PAD) ou consideramos a special tokens mask. Nessa situação remove (CLS, SEP e PAD).\n",
        "        # Deixa configurável pra testar os dois\n",
        "              # OBS.: No modelo do autor ele usa special_tokens e só remove o attention_mask. Vamos \n",
        "              # Mas tem uma classe SpaceDoc que parece que depois tira manualmente. Então vamos tentar simular\n",
        "              # tirando os special_tokens_mask e depois se não der certo, tentamos tirar só o attention_mask\n",
        "              # implementar assim também então: https://github.com/naver/splade/blob/main/splade/models/transformer_rep.py\n",
        "        if param_splade['manter_contribuicao_CLS_SEP_da_matriz_doc']:\n",
        "          mask_tokens_validos = batch['attention_mask'].to(device)\n",
        "        else:\n",
        "          mask_tokens_validos = 1 - batch['special_tokens_mask'].to(device)\n",
        "        # Expande a máscara criando uma terceira dimensão (vocab_size) \n",
        "        # e colocando do mesmo tamanho que os logits (batch_size, x, vocab_size):\n",
        "        mask = mask_tokens_validos.unsqueeze(-1).expand(logits.size())\n",
        "\n",
        "        # Calcula a saída (os pesos wj)\n",
        "        if param_splade['agg'] == 'sum':\n",
        "          wj = torch.sum(torch.log(1 + relu(logits*mask)), dim=1)\n",
        "        else:\n",
        "          wj, _ = torch.max(torch.log(1 + relu(logits*mask)), dim=1)\n",
        "\n",
        "        # Antes eu estava retornando wj.to_sparse() pois estava salvando\n",
        "        # a matriz em disco. Pra salvar num índice invertido não tem mais\n",
        "        # necessidade disso.\n",
        "\n",
        "        # Callback\n",
        "        idx_inicio = i_batch * dataloader.batch_size\n",
        "        idx_fim = idx_inicio + min(dataloader.batch_size, wj.size()[0])\n",
        "        indices_tratados = list(range(idx_inicio, idx_fim))\n",
        "        func_executar_apos_batch(indices_tratados, wj)"
      ],
      "metadata": {
        "id": "CpWAAeVEbPzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Nesse ponto, textos_trec_covid_expandidos está pareado com a variável ids_trec_covid\n",
        "\n",
        "dataset_trec_covid_expandido = DatasetSplade(tokenizer_splade, textos_trec_covid_expandidos, param_splade['max_seq_length'])\n",
        "dataloader_trec_covid_expandido = DataLoader(dataset_trec_covid_expandido, batch_size=param_splade['batch_size'], shuffle=False, collate_fn=collate_fn_splade)\n",
        "\n",
        "dataset_trec_covid_original = DatasetSplade(tokenizer_splade, textos_trec_covid_original, param_splade['max_seq_length'])\n",
        "dataloader_trec_covid_original = DataLoader(dataset_trec_covid_original, batch_size=param_splade['batch_size'], shuffle=False, collate_fn=collate_fn_splade)\n",
        "\n",
        "dataset_trec_covid_so_expansao = DatasetSplade(tokenizer_splade, textos_trec_covid_so_expansao, param_splade['max_seq_length'])\n",
        "dataloader_trec_covid_so_expansao = DataLoader(dataset_trec_covid_so_expansao, batch_size=param_splade['batch_size'], shuffle=False, collate_fn=collate_fn_splade)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMtZ5dISba9a",
        "outputId": "a1c19e7c-ef4a-4db6-d457-326eea710c32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 211 µs, sys: 3 µs, total: 214 µs\n",
            "Wall time: 221 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import array\n",
        "import pickle\n",
        "import math\n",
        "\n",
        "# Definição de uma classe para índice invertido\n",
        "class IndiceInvertidoSplade:\n",
        "\n",
        "  def __init__(self):\n",
        "    # Cria um índice invertido vazio\n",
        "    self.indice = {}\n",
        "\n",
        "  def adiciona_docs(self, ids_docs, wjs_docs):\n",
        "    nonzero = wjs_docs.nonzero()\n",
        "\n",
        "    for i in range(wjs_docs.size()[0]):\n",
        "      idx_linha_i = nonzero[:, 0] == i # i'ésimo doc do batch\n",
        "      idx_token_em_wj_i = nonzero[idx_linha_i, 1] # além de ser o índice na matriz wj, é tb o id do token\n",
        "      val_token_em_wj_i = wjs_docs[i, idx_token_em_wj_i]\n",
        "\n",
        "      self.adiciona_doc(ids_docs[i], idx_token_em_wj_i, val_token_em_wj_i)\n",
        "\n",
        "  def adiciona_doc(self, id_doc, idx_tokens, wj_tokens):\n",
        "    for id, wj in zip(idx_tokens.tolist(), wj_tokens.tolist()):\n",
        "      self.indice.setdefault(id, {\"id_doc\": [], \"wj\": array.array(\"f\", [])})['id_doc'].append(id_doc)\n",
        "      self.indice.setdefault(id, {\"id_doc\": [], \"wj\": array.array(\"f\", [])})['wj'].append(wj)\n",
        "    \n",
        "  def pesquisar(self, wjs_query, splade='v1'):\n",
        "    # Guarda um dicionário onde a chave é o id do documento e o valor é o score desse documento para a query pesquisada\n",
        "    docs_retornado_com_score = Counter({})\n",
        "\n",
        "    # Faz a pesquisa de documentos. Para isso iteramos todos os tokens da query\n",
        "    wjs_query = wjs_query.coalesce()\n",
        "    for id_token_query, wj_do_token_na_query in zip(wjs_query.indices()[0].tolist(), wjs_query.values().tolist()):\n",
        "      # É possível que a query contenha algum termo que não foi indexado. Se isso ocorrer, apenas pula o termo\n",
        "      if id_token_query not in self.indice:\n",
        "        continue\n",
        "\n",
        "      # Pega a lista de documentos que será analisado\n",
        "      docs_que_tem_token = self.indice[id_token_query]['id_doc']\n",
        "      wj_do_token_nos_docs = self.indice[id_token_query]['wj']\n",
        "\n",
        "      # Agora já temos calculado o score de todos os documentos desse token. Só adiciona ao acumulador de score atual\n",
        "      # docs_retornado_com_score += score_dos_docs_deste_token -> Se fosse usar dict direto no índice seria assim, mas a memória não está aguentando guardar os scores de ambos\n",
        "      multiplicador_token_query = wj_do_token_na_query if splade == 'v1' else 1\n",
        "\n",
        "      for id_doc, wj_do_token_no_doc in zip(docs_que_tem_token, wj_do_token_nos_docs):\n",
        "        docs_retornado_com_score[id_doc] += wj_do_token_no_doc * multiplicador_token_query\n",
        "      \n",
        "    # Agora converte esse dict em uma lista de tuplas com a chave (id_doc) e valor (score_do_doc)\n",
        "    docs_com_score = list(docs_retornado_com_score.items())\n",
        "\n",
        "    # E ordena do mais relevante para o menos relevante\n",
        "    return sorted(docs_com_score, key=lambda x: x[1], reverse=True)"
      ],
      "metadata": {
        "id": "0ZXvPW7PcMCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "idx_splade = IndiceInvertidoSplade()\n",
        "\n",
        "def popular_indice_invertido(idx_batch, wj_batch):\n",
        "  ids_doc_batch = [ids_trec_covid[i] for i in idx_batch]\n",
        "  idx_splade.adiciona_docs(ids_doc_batch, wj_batch)\n",
        "\n",
        "def salvar_indice(idx_splade):\n",
        "  nome_arquivo_pickle = param_splade[\"nome_arquivo_indice_invertido_docs\"]\n",
        "  diretorio_destino_cp = f\"'{aula9_dir}'\"\n",
        "  with open(nome_arquivo_pickle, 'wb') as f:\n",
        "    pickle.dump(idx_splade.indice, f)\n",
        "  !cp {nome_arquivo_pickle} {diretorio_destino_cp}\n",
        "\n",
        "def recuperar_indice(idx_splade):\n",
        "  with open(arq_indice_pickle, 'rb') as f:\n",
        "    idx_splade.indice = pickle.load(f)\n",
        "\n",
        "arq_indice_pickle = f'{aula9_dir}{param_splade[\"nome_arquivo_indice_invertido_docs\"]}'\n",
        "\n",
        "if param_splade['gerar_arquivo_indice']:\n",
        "  # Se for gerar o arquivo de índice, checar se é pra gerar usando doc2query ou não\n",
        "  if param_splade['indexar_com_doc2query']:\n",
        "    # Se for usar doc2query, checa se é pra usar só a expansão ou se é pra usar os doc originais + a expansão\n",
        "    dataloader = dataloader_trec_covid_so_expansao if param_splade['indexar_so_expansao'] else dataloader_trec_covid_expandido\n",
        "  else:\n",
        "    dataloader = dataloader_trec_covid_original\n",
        "\n",
        "  representacao_esparsa_dataloader(model_splade, dataloader, popular_indice_invertido)    \n",
        "  salvar_indice(idx_splade)\n",
        "else:\n",
        "  recuperar_indice(idx_splade)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WL42T2HtgXMI",
        "outputId": "d5848940-2d48-4771-9e5a-e3ae71819c0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4.11 s, sys: 201 ms, total: 4.31 s\n",
            "Wall time: 4.31 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Roda queries"
      ],
      "metadata": {
        "id": "fCcthpmSgnSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "print('Carregando as queries do arquivo queries.jsonl...\\n')\n",
        "queries_trec_covid = carrega_queries_trec_covid()\n",
        "\n",
        "def run_all_queries_indice_invertido_splade(file, model_splade, tokenizer_splade, idx_splade, splade='v1'):\n",
        "  tempo_gpu = 0\n",
        "  print(f'Total de queries que serão avaliadas: {len(queries_trec_covid)}')\n",
        "  cnt = 0\n",
        "  with open(file, 'w') as runfile:\n",
        "    for query in queries_trec_covid:\n",
        "      id = query['id']\n",
        "      texto = query['texto']\n",
        "\n",
        "      tempo_inicio_splade = time.time()\n",
        "      wj_query = representacao_esparsa_do_texto(model_splade, tokenizer_splade, texto, True, param_splade['manter_contribuicao_CLS_SEP_da_matriz_doc'])\n",
        "      tempo_fim_splade = time.time()\n",
        "      if cnt % 10 == 0:\n",
        "        print(f'{cnt} queries completadas')\n",
        "\n",
        "      # Usa o índice invertido pra pesquisar\n",
        "      docs_score = idx_splade.pesquisar(wj_query, splade)\n",
        "\n",
        "      for i in range(0, min(1000, len(docs_score))): # Pega os primeiros 1000 resultados\n",
        "        _ = runfile.write('{} Q0 {} {} {:.6f} BM_25\\n'.format(id, docs_score[i][0], i+1, docs_score[i][1]))\n",
        "\n",
        "      cnt += 1\n",
        "      tempo_gpu = tempo_gpu + (tempo_fim_splade - tempo_inicio_splade)\n",
        "\n",
        "    print(f'{cnt} queries completadas')\n",
        "    print(f\"Tempo GPU: {tempo_gpu}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzYeCjZHgqcC",
        "outputId": "326642b9-d35a-41ef-9525-538430708897"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando as queries do arquivo queries.jsonl...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "run_all_queries_indice_invertido_splade('run-doc2query-splade.txt', model_splade, tokenizer_splade, idx_splade, 'v1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEXypJoZgtOj",
        "outputId": "7c97bc24-c7c3-41bb-a95f-ca154bb3f9d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de queries que serão avaliadas: 50\n",
            "0 queries completadas\n",
            "10 queries completadas\n",
            "20 queries completadas\n",
            "30 queries completadas\n",
            "40 queries completadas\n",
            "50 queries completadas\n",
            "Tempo GPU: 1.0339248180389404\n",
            "CPU times: user 1min 10s, sys: 233 ms, total: 1min 10s\n",
            "Wall time: 1min 9s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "!python -m pyserini.eval.trec_eval -c -m ndcg_cut.10 collections/trec-covid/qrels/test_corrigido.tsv run-doc2query-splade.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eF8VXqznKcFZ",
        "outputId": "fb7abb16-a59a-4cde-9bbd-8fe942a1091b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://search.maven.org/remotecontent?filepath=uk/ac/gla/dcs/terrierteam/jtreceval/0.0.5/jtreceval-0.0.5-jar-with-dependencies.jar to /root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar...\n",
            "/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar already exists!\n",
            "Skipping download.\n",
            "Running command: ['java', '-jar', '/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar', '-c', '-m', 'ndcg_cut.10', 'collections/trec-covid/qrels/test_corrigido.tsv', 'run-doc2query-splade.txt']\n",
            "Results:\n",
            "ndcg_cut_10           \tall\t0.7298\n",
            "CPU times: user 52.6 ms, sys: 9.19 ms, total: 61.8 ms\n",
            "Wall time: 5.53 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline 2: SPLADE + reranking InPars"
      ],
      "metadata": {
        "id": "XEVnpHuA_nbX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recuperando (e adaptando) código reranking da Aula 8"
      ],
      "metadata": {
        "id": "Sww_ms-oArPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import BatchEncoding\n",
        "from transformers import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Carrega tokenizador e modelo e cria um optimizer\n",
        "tokenizer_inpars = AutoTokenizer.from_pretrained(param_inpars['nome_tokenizer'])\n",
        "model_inpars = AutoModelForSequenceClassification.from_pretrained(param_inpars['nome_modelo']).to(device)\n",
        "\n",
        "# Cria dataloaders de treino e eval\n",
        "collate_fn_inpars = lambda batch: BatchEncoding(tokenizer_inpars.pad(batch, return_tensors='pt'))"
      ],
      "metadata": {
        "id": "dp2xHQfzdBVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DatasetInpars(data.Dataset):\n",
        "  # Recebe um dataframe do pandas. Precisa ter as colunas query, passage e label (0/1)\n",
        "  def __init__(self, tokenizer_inpars, df, max_seq_length):\n",
        "    self.max_seq_length = max_seq_length\n",
        "    self.tokenizer_inpars = tokenizer_inpars\n",
        "\n",
        "    # Já concatenas as query com as passagens e guarda em uma lista\n",
        "    query_passage = df['query'] + ' [SEP] ' + df['passage']\n",
        "    self.query_passage = query_passage.tolist()\n",
        "    # Converte os labels para inteiros e guarda em uma lista\n",
        "    self.labels = df.label.tolist()\n",
        "    self.labels = [float(x) for x in self.labels]\n",
        "\n",
        "    # Cria um cache vazio. Como tem treino em algumas épocas, guarda o encode no cache\n",
        "    self.cache = {}\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.query_passage)\n",
        "  \n",
        "  def get_token_type_ids(self, input_ids):\n",
        "    idx_sep = input_ids.index(102)+1\n",
        "    tam_seq = len(input_ids)\n",
        "    token_type_ids = [0]*idx_sep + [1]*(tam_seq - idx_sep)\n",
        "\n",
        "    # Apesar do tokenizer fazer isso, não precisa pois o attention_mask já zera.\n",
        "    # for i in range(len(token_type_ids)):\n",
        "    #   token_type_ids[i] = token_type_ids[i] if input_ids[i] != 0 else 0\n",
        "\n",
        "    return token_type_ids\n",
        "\n",
        "  def get_token_type_ids_from_slice(self, idx, matriz_input_ids):\n",
        "    if isinstance(idx, slice):\n",
        "      token_types = []\n",
        "      for i in range(idx.start or 0, idx.stop or len(matriz_input_ids), idx.step or 1):\n",
        "        token_types.append(self.get_token_type_ids(matriz_input_ids[i]))\n",
        "      return token_types\n",
        "    else:\n",
        "      return self.get_token_type_ids(matriz_input_ids)\n",
        "\n",
        "  def get_input_ids_e_labels(self, idx):\n",
        "    input_ids_e_labels = self.tokenizer_inpars(self.query_passage[idx],\n",
        "                                padding=True,\n",
        "                                truncation=True,\n",
        "                                max_length=self.max_seq_length)\n",
        "    input_ids_e_labels['labels'] = self.labels[idx]\n",
        "\n",
        "    input_ids_e_labels['token_type_ids'] = self.get_token_type_ids_from_slice(idx, input_ids_e_labels['input_ids'])\n",
        "\n",
        "    return input_ids_e_labels\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # Guarda os itens tokenizados num dict e apenas recupera de lá, pra não ter que ficar tokenizando a cada época\n",
        "    # Como estamos guardando no dict e idx é um slice, é necessário converter ele pra algo mapeável\n",
        "    self.cache[str(idx)] = self.cache.get(str(idx), self.get_input_ids_e_labels(idx))\n",
        "    return self.cache[str(idx)]"
      ],
      "metadata": {
        "id": "tuYipUaBA2hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from pyserini.search import get_qrels\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "def reranking_docs(model_inpars, query, doc_ids):\n",
        "  model_inpars.eval()\n",
        "  \n",
        "  # Cria um dataset para fazer o reranking\n",
        "  if param_inpars['considerar_expansao_no_reranking']:\n",
        "    passagens = [ f\"{dict_trec_covid_com_e_sem_expansao[id]['doc_original']} {dict_trec_covid_com_e_sem_expansao[id]['query_expandida']}\" for id in doc_ids ]\n",
        "  else:\n",
        "    passagens = [ dict_trec_covid_com_e_sem_expansao[id]['doc_original'] for id in doc_ids ] \n",
        "\n",
        "  queries = [query]*len(doc_ids)\n",
        "  labels = [1]*len(doc_ids) # é eval, tanto faz o valor. Ele não será utilizado, mas precisamos cadastrar aqui\n",
        "  df_reranking = pd.DataFrame({'query': queries, 'passage': passagens, 'label': labels})\n",
        "\n",
        "  dataset_reranking = DatasetInpars(tokenizer_inpars, df_reranking, param_inpars['max_seq_length'])\n",
        "  dataloader_reranking = DataLoader(dataset_reranking, batch_size=param_inpars['batch_size'], shuffle=False, collate_fn=collate_fn_inpars)\n",
        "\n",
        "  score_todos_docs = []\n",
        "  with torch.no_grad():\n",
        "    for batch in dataloader_reranking:\n",
        "      outputs = model_inpars(**batch.to(device))\n",
        "      score = outputs.logits.squeeze().tolist()\n",
        "      score_todos_docs.extend( score )\n",
        "\n",
        "  docs_com_score = list(zip(doc_ids, score_todos_docs))\n",
        "\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  return sorted(docs_com_score, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Roda todas as queries\n",
        "def run_all_queries_splade_e_rerank(file, model_inpars, idx_splade, model_splade, tokenizer_splade, hits=1000):\n",
        "  tempo_gpu = 0\n",
        "\n",
        "  with open(file, 'w') as runfile:\n",
        "    cnt = 0\n",
        "    print('Running {} queries in total'.format(len(queries_trec_covid)))\n",
        "    for query in queries_trec_covid:\n",
        "      id = query['id']\n",
        "      texto_query = query['texto']\n",
        "      \n",
        "      if cnt % 10 == 0:\n",
        "        print(f'{cnt} queries completed')\n",
        "      cnt += 1\n",
        "\n",
        "      tempo_inicio_splade = time.time()\n",
        "      wj_query = representacao_esparsa_do_texto(model_splade, tokenizer_splade, texto_query, True,\n",
        "                                                param_splade['manter_contribuicao_CLS_SEP_da_matriz_doc'])\n",
        "      tempo_fim_splade = time.time()\n",
        "      # Usa o índice invertido Splade pra pesquisar\n",
        "      docs_score = idx_splade.pesquisar(wj_query, 'v1')\n",
        "      # Pega só os 1000 primeiros pra fazer o reranking\n",
        "      n_reranking = min(hits, len(docs_score))\n",
        "      docs_score = docs_score[0:n_reranking]\n",
        "\n",
        "      # Agora faz o reranking com InPars\n",
        "      docs, scores = zip(*docs_score)\n",
        "      tempo_inicio_inpars = time.time()\n",
        "      docs_score = reranking_docs(model_inpars, texto_query, docs)\n",
        "      tempo_fim_inpars = time.time()\n",
        "      \n",
        "      tempo_gpu = tempo_gpu + (tempo_fim_inpars - tempo_inicio_inpars) + (tempo_fim_splade - tempo_inicio_splade)\n",
        "\n",
        "      for i in range(0, len(docs_score)): # Pega os primeiros 1000 resultados\n",
        "        _ = runfile.write('{} Q0 {} {} {:.6f} BM_25_RERANKING_MINILM\\n'.format(id, docs_score[i][0], i+1, docs_score[i][1]))\n",
        "\n",
        "    print(f\"Tempo GPU: {tempo_gpu}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IbGuyri_vnS",
        "outputId": "340f3037-04e6-4b3a-cd93-6f8b2cbcba33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 15 µs, sys: 0 ns, total: 15 µs\n",
            "Wall time: 18.6 µs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "run_all_queries_splade_e_rerank('run-splade-inpars.txt', model_inpars, idx_splade, model_splade, tokenizer_splade, hits=100)"
      ],
      "metadata": {
        "id": "Gc4SMy7nE8aR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01879e71-1ae1-463e-a77a-85145fb5c6c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running 50 queries in total\n",
            "0 queries completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 queries completed\n",
            "20 queries completed\n",
            "30 queries completed\n",
            "40 queries completed\n",
            "Tempo GPU: 22.814547300338745\n",
            "CPU times: user 1min 28s, sys: 1.44 s, total: 1min 29s\n",
            "Wall time: 1min 28s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "!python -m pyserini.eval.trec_eval -c -m ndcg_cut.10 collections/trec-covid/qrels/test_corrigido.tsv run-splade-inpars.txt #type: ignore     "
      ],
      "metadata": {
        "id": "lw904lHjKy3h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac56578e-8250-44c1-8d9b-753b2b94c30b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://search.maven.org/remotecontent?filepath=uk/ac/gla/dcs/terrierteam/jtreceval/0.0.5/jtreceval-0.0.5-jar-with-dependencies.jar to /root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar...\n",
            "/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar already exists!\n",
            "Skipping download.\n",
            "Running command: ['java', '-jar', '/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar', '-c', '-m', 'ndcg_cut.10', 'collections/trec-covid/qrels/test_corrigido.tsv', 'run-splade-inpars.txt']\n",
            "Results:\n",
            "ndcg_cut_10           \tall\t0.7737\n",
            "CPU times: user 52.2 ms, sys: 4.27 ms, total: 56.5 ms\n",
            "Wall time: 5.23 s\n"
          ]
        }
      ]
    }
  ]
}