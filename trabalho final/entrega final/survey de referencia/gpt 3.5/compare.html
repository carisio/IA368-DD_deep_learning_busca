<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Section Title</th>
      <th>Subsection Title</th>
      <th>generated_text</th>
      <th>survey_text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Text Representations for Ranking</td>
      <td>BOW Encodings</td>
      <td>Text Representations for Ranking - BOW Encodings<br/><br/>Bag-of-Words (BOW) encodings are a popular approach for representing text documents in information retrieval systems. In this encoding scheme, each document is represented as a vector of term frequencies, where each dimension corresponds to a unique term in the document collection. BOW encodings have been widely used in traditional retrieval models, and their application in neural information retrieval has also gained attention [REF0].<br/><br/>The BOW encoding scheme assumes that the order of terms in a document does not convey important information for retrieval purposes. Instead, it focuses on the frequency of occurrence of terms within a document. This approach has been motivated by the observation that the presence and frequency of certain terms can be indicative of the document's relevance to a given query [REF1].<br/><br/>One advantage of BOW encodings is their simplicity and efficiency in computation. The encoding process involves tokenizing the text into individual terms, removing stop words, and counting the occurrences of each term. This results in a high-dimensional vector representation of the document, where each dimension corresponds to a unique term in the collection. These vectors can then be used for various ranking algorithms, such as cosine similarity or BM25 [REF2].<br/><br/>The BOW encoding scheme has some limitations. One of the main challenges is the sparsity of the resulting vectors, especially when dealing with large document collections. As the number of unique terms increases, the majority of dimensions in the vector will be zero, leading to computational inefficiency and potentially affecting the retrieval performance. Various techniques, such as dimensionality reduction or term weighting, have been proposed to address this issue [REF3].<br/><br/>Another limitation of BOW encodings is their inability to capture the semantic relationships between terms. Since BOW representations treat each term independently, they do not consider the contextual information or the meaning of the terms within the document. This can result in a loss of important information during the retrieval process [REF4].<br/><br/>Despite these limitations, BOW encodings have been widely used and have shown promising results in information retrieval tasks. They provide a simple and efficient representation of text documents, allowing for fast retrieval and ranking. Moreover, BOW encodings can serve as a baseline for more advanced neural models that aim to capture the semantic relationships between terms and documents [REF5].<br/><br/>In conclusion, BOW encodings have been a popular choice for representing text documents in information retrieval systems. They offer simplicity and efficiency in computation, making them suitable for large-scale retrieval tasks. However, their limitations in capturing semantic relationships and handling sparsity should be taken into consideration when designing retrieval systems. Future research in neural information retrieval can explore ways to enhance BOW encodings or combine them with more advanced techniques to improve retrieval performance [REF6] [REF7] [REF8] [REF9].</td>
      <td>In classical IR, both representation and aggregation functions are designed manually, incorporating some lexical statistics such as number of occurrences of terms in a document or in the whole collection. Classical IR ranking models, e.g., vector space models BIBREF9 , probabilistic models BIBREF10 and statistical language models BIBREF11 , are based on the bag of words (BOW) model, where queries and documents are represented as a set of terms from the vocabulary INLINEFORM0 together with the number of occurrences of the corresponding tokens in the text. More formally, queries and documents are represented as vectors INLINEFORM1 and INLINEFORM2 in INLINEFORM3 , called BOW encodings , where the INLINEFORM4 -th component of both representations encodes the number of occurrences of the term INLINEFORM5 in the corresponding text. The query-document representation function INLINEFORM6 is not present in these ranking functions. The aggregation function INLINEFORM7 over these representations is an explicit formula taking into account the components of the query and document representations, i.e., the in-query and in-document term frequencies, together with other document normalisation operations. These representations are referred to as sparse representations , since most of their components are equal to 0 because they correspond to tokens not appearing in the query/document. Sparse representations can be trivially computed and efficiently stored in specialised data structures called inverted indexes , which represent the backbone of commercial Web search engine BIBREF5 ; see BIBREF0 for more details on inverted indexes and classical IR ranking models.</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Text Representations for Ranking</td>
      <td>LTR Features</td>
      <td>Text Representations for Ranking - LTR Features<br/><br/>Text representations play a crucial role in neural information retrieval systems, particularly in the context of ranking. The goal of ranking is to match processed queries with indexed documents effectively and efficiently [REF1]. Learning to rank (LTR) techniques have emerged as a promising approach to improve ranking models by leveraging machine learning technologies [REF1]. In this section, we discuss the use of LTR features for text representations in ranking.<br/><br/>One important aspect of LTR features is the learning time, which can increase with the sample size [REF0]. To address this, researchers have investigated the number of relevant documents identified for query sets when the number of retrieved documents is unconstrained [REF0]. It has been observed that precision tends to fall below a certain threshold after a certain rank, indicating that retrieving additional documents may yield minimal gains in terms of relevance [REF0]. Therefore, an original sample of a sufficient number of documents, such as 5000, has been deemed suitable for experiments [REF0].<br/><br/>Different approaches have been proposed for learning to rank, including pointwise, pairwise, and listwise approaches [REF1]. These approaches provide frameworks and algorithms for ranking models, each with its own theoretical properties [REF1]. For instance, PRank is a ranker based on the perceptron algorithm, which maps a feature vector to the reals using learned weights [REF4]. PRank also learns the values of increasing thresholds to determine the rank of a document [REF7]. On the other hand, LambdaRank and LambdaMART update their parameters differently, with LambdaRank updating all weights after each query and LambdaMART updating only a few parameters at a time [REF3].<br/><br/>Ordinal regression has also been employed in learning to rank, where the problem is cast as learning the mapping of an input vector to an ordered set of numerical ranks [REF4]. The positions of rank boundaries play a critical role in the final ranking function [REF4]. Additionally, the use of sampling has been motivated by the need for efficient application of a learned model, reducing the number of documents for which features are calculated [REF5]. This is particularly advantageous when certain features are computationally expensive [REF5].<br/><br/>In learning to rank, the independence and identically distributed (i.i.d) assumption does not hold for every example document in the training data [REF6]. Instead, documents associated with each query form a group, where the groups are i.i.d but the documents within a group are not i.i.d [REF6]. Furthermore, the learning time of many LTR techniques increases with the number of documents in the sample [REF6]. Therefore, careful consideration of the sample size is necessary to balance efficiency and effectiveness [REF6].<br/><br/>In summary, text representations for ranking in neural information retrieval systems involve the use of LTR features. These features are designed to optimize the ranking process by considering factors such as learning time, different learning approaches, ordinal regression, and the use of sampling. By leveraging these features, researchers aim to improve the efficiency and effectiveness of ranking models in information retrieval applications.<br/><br/>[REF0] - [REF9]</td>
      <td>With the advent of the Web, new sources of relevance information about the documents have been made available. The importance of a Web page, e.g., PageRank, additional document statistics, e.g., term frequencies in the title or anchors text, and search engine interactions, e.g., clicks, can be exploited as relevance signals. Moreover, collaborative and social platforms such as Wikipedia, Twitter and Facebook represent new sources of relevance signals. These relevance signals have been exploited to build richer query and document representations in LTR. The relevance signals extracted from queries and/or documents are called features . There are various classes of these features BIBREF12 , such as: 1.  query-only features, i.e., components of INLINEFORM0 : query features with the same value for each document, such as query type, query length, and query performance predictors;  2.  query-independent features, i.e., components of INLINEFORM0 : document features with the same value for each query, such as importance score, URL length, and spam score;  3.  query-dependent features, i.e., components of INLINEFORM0 : document features that depend on the query, such as different term weighting models on different fields.  In LTR, the representation functions are hand-crafted: exploiting the relevance signals from heterogeneous information sources, the different components of query and document representations are computed with feature-specific algorithms. Hence, the representations INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 are elements of vector spaces over INLINEFORM3 , but whose dimensions depend on the number of hand-crafted query-only, query-independent, and query-dependent features, respectively. Moreover the different components of these vectors are heterogeneous, and do not carry any specific semantic meaning. Using these representations, in LTR the aggregation function INLINEFORM4 is machine-learned, for example using logistic regression BIBREF14 , gradient-boosted regression trees BIBREF15 or neural networks BIBREF16 ; see BIBREF4 for a detailed survey.</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Text Representations for Ranking</td>
      <td>Word Embeddings</td>
      <td>Text Representations for Ranking - Word Embeddings<br/><br/>Word embeddings have become a popular approach for representing text in neural information retrieval systems. Word embeddings are dense vector representations that capture semantic and syntactic relationships between words. These representations have shown promising results in various natural language processing tasks, including information retrieval and ranking.<br/><br/>One common method for generating word embeddings is through the use of word2vec models [REF5]. Word2vec models, such as skip-gram and continuous bag-of-words (CBOW), learn word embeddings by predicting the context words given a target word or vice versa. These models leverage the co-occurrence statistics of words in a large corpus to learn meaningful representations. The skip-gram model, for example, learns to predict the surrounding words given a target word, while the CBOW model predicts the target word given its context. These models have demonstrated the ability to capture linguistic patterns and relationships between words [REF5].<br/><br/>Another popular approach for generating word embeddings is the GloVe (Global Vectors) model [REF6]. GloVe directly captures the global corpus statistics by training on word-word co-occurrence counts. The model uses a weighted least squares approach to learn word vectors that exhibit meaningful substructure. GloVe has shown state-of-the-art performance on word analogy tasks and word similarity tasks [REF3]. The advantage of GloVe is that it captures both semantic and syntactic relationships between words, making it suitable for various NLP tasks, including information retrieval and ranking.<br/><br/>Word embeddings have been evaluated and compared in the context of ranking tasks. For example, in a study comparing different word embedding models, it was found that the SWOW-RW and SWOW-PMI models outperformed other models in predicting guesser responses [REF4]. Similarly, the associative models, which leverage free association data, were found to emphasize semantic relationships not well-represented within distributional semantic models (DSMs) trained on linguistic corpora [REF0]. These findings suggest that word embeddings derived from different models can have varying degrees of effectiveness in capturing semantic relationships and predicting relevant information for ranking tasks.<br/><br/>It is worth noting that the choice of word embedding model depends on the specific requirements of the ranking task. Different models may excel in different aspects, such as capturing semantic relationships, syntactic patterns, or contextual information. Therefore, it is important to carefully evaluate and select the appropriate word embedding model based on the specific needs of the neural information retrieval system.<br/><br/>In conclusion, word embeddings have emerged as a powerful tool for representing text in neural information retrieval systems. Models such as word2vec and GloVe have shown promising results in capturing semantic relationships and improving ranking performance. However, the choice of word embedding model should be carefully considered based on the specific requirements of the ranking task. Further research and evaluation are needed to explore the effectiveness of different word embedding models in various neural information retrieval scenarios.<br/><br/>[REF0] Jones et al., 2015<br/>[REF1] Kumar, Steyvers, Balota, 2021<br/>[REF3] Pennington et al., 2014<br/>[REF4] De Deyne et al., 2021<br/>[REF5] Mikolov et al., 2013a<br/>[REF6] Pennington et al., 2014</td>
      <td>Both BOW encodings and LTR features are widely adopted in commercial search engines, but they suffer from several limitations. On the one hand, semantically-related terms end up having completely different BOW encodings. Although the two terms catalogue and directory can be considered synonyms, their BOW encodings are completely different, with the single 1 appearing in different components. Similarly, two different documents on a same topic can end up having two unrelated BOW encodings. On the other hand, LTR features create text representations by hand via feature engineering, with heterogeneous components and no explicit concept of similarity. In the 1950s, many linguists formulated the distributional hypothesis : words that occur in the same contexts tend to have similar meanings BIBREF17 . According to this hypothesis, the meaning of words can be inferred by their usage together with other words in existing texts. Hence, by leveraging the large text collections available, it is possible to learn useful representations of terms, and devise new methods to use these representations to build up more complex representations for queries and documents. These learned representations are vectors in INLINEFORM0 , with INLINEFORM1 , called distributional representations or word embeddings . The number of dimensions INLINEFORM2 ranges approximatively from 50 to 1000 components, instead of the vocabulary size INLINEFORM3 . Moreover, the components of word embeddings are rarely 0: they are real numbers, and can also have negative values. Hence, word embeddings are also referred to as dense representations . Among the different techniques to compute these representations, there are algorithms to compute global representations of the words, i.e., a single fixed embedding for each term in the vocabulary, called static word embeddings , and algorithms to compute local representations of the terms, which depend on the other tokens used together with a given term, i.e., its context, called contextualised word embeddings . Static word embeddings used in neural IR are learned from real-world text with no explicit training labels: the text itself is used in a self-supervised fashion to compute word representations. There are different kinds of static word embeddings, for different languages, such as INLINEFORM4 BIBREF18 , INLINEFORM5 BIBREF19 and INLINEFORM6 BIBREF20 . Static word embeddings map terms with multiple senses into an average or most common sense representation based on the training data used to compute the vectors; each term in the vocabulary is associated with a single vector. Contextualised word embeddings map tokens used in a particular context to a specific vector; each term in the vocabulary is associated with a different vector every time it appears in a document, depending on the surrounding tokens. The most popular contextualised word embeddings are learned with deep neural networks such as the Bidirectional Encoder Representations from Transformers ( INLINEFORM7 ) BIBREF21 , the Robustly Optimized INLINEFORM8 Approach ( INLINEFORM9 ) BIBREF22 , and the Generative Pre-Training models ( INLINEFORM10 ) BIBREF23 . In neural IR, word embeddings are used to compute the representation functions INLINEFORM0 , INLINEFORM1 and INLINEFORM2 , and the aggregation function INLINEFORM3 through (deep) neural networks. Depending on the assumptions over the representation functions, the neural ranking models can be classified in interaction-focused models and representation-focused models. In interaction-focused models, the query-document representation function INLINEFORM4 , taking into account the interaction between the query and document contents, is explicitly constructed and used as input to a deep neural network, or it is implicitly generated and directly used by a deep neural network. In representation-focused models, the query-document representation function INLINEFORM5 is not present; query and document representations INLINEFORM6 and INLINEFORM7 are computed independently by deep neural networks In the following, we discuss the main interaction-focused models for ad-hoc ranking (Section SECREF3 ) and representation-focused models for queries and documents (Section SECREF4 ).</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Interaction-focused Systems</td>
      <td>Convolutional Neural Networks</td>
      <td>Interaction-focused Systems - Convolutional Neural Networks<br/><br/>Convolutional Neural Networks (CNNs) have gained significant attention in the field of neural information retrieval due to their ability to capture local patterns and interactions between queries and documents. In this section, we will explore the use of CNNs in interaction-focused systems for information retrieval.<br/><br/>One important consideration in the design of CNN-based models is the selection of hyperparameters. Limited computational resources necessitate the careful determination of hyperparameter ranges based on pilot experiments and domain insights [REF0]. For instance, the choice of the number of filters, kernel size, and pooling sizes can significantly impact the performance of the model [REF1]. Additionally, the selection of hyperparameters such as the dimensionality of the embeddings and the number of layers can also influence the model's effectiveness [REF2].<br/><br/>Positional information plays a crucial role in capturing the interactions between queries and documents. However, incorporating positional information in deep neural IR models is non-trivial. Models like MatchPyramid and local DUET have attempted to account for positional information by incorporating convolutional layers based on similarity matrices between queries and documents [REF1]. Despite these efforts, these models have struggled to outperform the DRMM model, indicating the challenges in effectively utilizing positional information in multi-dimensional interactions [REF1].<br/><br/>One of the key advantages of CNNs in neural IR is their ability to model n-grams. Traditional IR approaches treat n-grams as discrete terms, which can lead to data sparsity and an explosion in the parameter space. CNNs address this issue by learning a convolutional layer that forms n-grams from individual word embeddings, allowing for the matching of n-grams of different lengths [REF2]. This approach enables the model to capture the semantic relationships between query terms and document content, enhancing the retrieval performance [REF2].<br/><br/>Several interaction-focused models have been proposed in the literature. For instance, Deep Match Tree defines interactions in the product space of dependency trees, leveraging a deep neural network to make matching decisions based on these local interactions [REF3]. Match-SRNN, on the other hand, models the recursive matching structure in local interactions to capture long-distance dependencies [REF3]. These models have been evaluated on various tasks, including short text matching, community-based question answering, and paper citation matching [REF3]. However, it is important to note that most of these deep matching models are primarily designed for semantic matching problems, which differ significantly from the relevance matching problem in ad-hoc retrieval [REF3].<br/><br/>The application of deep learning in information retrieval has the potential to revolutionize the field. Deep neural networks have the ability to discover hidden structures and features at different levels of abstraction, which can be beneficial for IR tasks [REF4]. The success of deep learning in other domains, such as computer vision, speech recognition, and natural language processing, suggests that it can have a significant impact on IR as well [REF4].<br/><br/>In summary, CNNs have emerged as a powerful tool in interaction-focused systems for neural information retrieval. By effectively capturing local patterns and interactions between queries and documents, CNN-based models have shown promise in improving retrieval performance. However, challenges remain in incorporating positional information and designing models that can handle multi-dimensional interactions. Further research is needed to explore and enhance the capabilities of CNNs in neural information retrieval.<br/><br/>[REF0] <br/>[REF1] <br/>[REF2] <br/>[REF3] <br/>[REF4]</td>
      <td>A convolutional neural network is a family of neural networks designed to capture local patterns in structured inputs, such as images and texts BIBREF24 . The core component of a convolutional neural network is the convolution layer, used in conjunction with feed forward and pooling layers. A convolutional layer can be seen as a small linear filter, sliding over the input and looking for proximity patterns. Several neural models employ convolutional neural networks over the interactions between queries and documents to produce relevance scores. Typically, in these models, the word embeddings of the query and document tokens are aggregated into an interaction matrix , on top of which convolutional neural networks are used to learn hierarchical proximity patterns such as unigrams, bigrams and so on. Then, the final top-level proximity patterns are fed into a feed forward neural network to produce the relevance score INLINEFORM0 between the query INLINEFORM1 and the document INLINEFORM2 , as illustrated in Figure FIGREF12 . The query INLINEFORM0 and the document INLINEFORM1 are tokenised into INLINEFORM2 and INLINEFORM3 tokens, respectively, and each token is mapped to a corresponding static word embedding. The interaction matrix INLINEFORM4 is composed of the cosine similarities between a query token embedding and a document token embedding. One of the first neural models leveraging the interaction matrix is the Deep Relevance Matching Model ( INLINEFORM0 ) BIBREF25 ). In INLINEFORM1 , the cosine similarities of every query token w.r.t. the document tokens are converted into a discrete distribution using hard bucketing , i.e., into a query token histogram. Then the histogram of each query token is provided as input to a feed forward neural network to compute the final query token-document relevance score. These relevance scores are then aggregated through an IDF-based weighted sum across the different query terms. Instead of using hard bucketing, the Kernel-based Neural Ranking Model ( INLINEFORM2 ) BIBREF26 proposes to use Gaussian kernels to smoothly distribute the contribution of each cosine similarity across different buckets before providing the histograms with soft bucketing to the feed forward neural networks. Both INLINEFORM0 and INLINEFORM1 exploit the interaction matrix, but they do not incorporate any convolutional layer. In the Convolutional INLINEFORM2 model ( INLINEFORM3 ) BIBREF27 , the query and document embeddings are first independently processed through INLINEFORM4 convolutional neural networks, to build unigam, bigram, up to INLINEFORM5 -gram embeddings. These convolutions allow to build word embeddings taking into account multiple close words at the same time. Then, INLINEFORM6 cosine similarity matrices are built, between each combination of query and document INLINEFORM7 -gram embeddings, and these matrices are processed with INLINEFORM8 . In the Position-Aware Convolutional Recurrent Relevant model ( INLINEFORM9 ) BIBREF28 , the interaction matrix is processed through several convolutional and pooling layers to take into account words proximity. Convolutional layers are used also in other similar neural models BIBREF29 .</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Interaction-focused Systems</td>
      <td>Pre-trained Language Models</td>
      <td>Interaction-focused Systems - Pre-trained Language Models<br/><br/>Pre-trained language models have revolutionized the field of neural information retrieval by providing a powerful foundation for various natural language processing tasks. In recent years, there has been a surge of interest in interaction-focused systems that leverage pre-trained language models to enhance the retrieval process. These systems aim to improve the effectiveness and efficiency of information retrieval by incorporating user interactions and feedback into the retrieval process.<br/><br/>One prominent example of interaction-focused systems is the use of pre-trained language models such as BERT (Bidirectional Encoder Representations from Transformers) [REF1]. BERT has demonstrated remarkable performance across a wide range of tasks, including natural language inference, question answering, and sentiment analysis. Its success can be attributed to its ability to capture contextual information and generate meaningful representations of text.<br/><br/>The application of pre-trained language models in interaction-focused systems involves fine-tuning the models on specific retrieval tasks. This process typically involves two stages: pre-training and fine-tuning. During pre-training, the language model is trained on a large corpus of text to learn general language understanding [REF2]. Fine-tuning, on the other hand, involves training the model on task-specific data to adapt it to the retrieval task at hand.<br/><br/>One key advantage of pre-trained language models is their ability to capture semantic relationships between words and phrases. This enables them to understand the context of a query and retrieve relevant information accordingly. For example, BERT has been shown to outperform previous state-of-the-art models on various benchmark datasets, achieving significant improvements in accuracy [REF1].<br/><br/>Another important aspect of interaction-focused systems is the incorporation of user interactions and feedback. These systems aim to leverage user feedback, such as relevance judgments or explicit feedback, to improve the retrieval process. By incorporating user interactions, the models can adapt and refine their retrieval strategies based on user preferences and information needs.<br/><br/>The effectiveness of interaction-focused systems using pre-trained language models has been demonstrated in various studies. For instance, researchers have explored the use of BERT in question answering systems, where user interactions play a crucial role in refining the answers provided [REF4]. By incorporating user feedback, these systems can iteratively improve the quality of the answers and provide more accurate and relevant information.<br/><br/>Furthermore, the use of pre-trained language models in interaction-focused systems has also shown promise in text generation tasks. Models such as BART (Bidirectional and Auto-Regressive Transformers) have been developed to generate coherent and contextually relevant text [REF2]. By fine-tuning these models on specific text generation tasks, they can generate high-quality responses to user queries, enhancing the overall user experience.<br/><br/>In conclusion, pre-trained language models have emerged as a powerful tool in the development of interaction-focused systems for neural information retrieval. These models, such as BERT and BART, provide a solid foundation for capturing semantic relationships and understanding user interactions. By incorporating user feedback and fine-tuning the models on specific retrieval tasks, interaction-focused systems can significantly enhance the effectiveness and efficiency of information retrieval. Future research in this area should focus on exploring novel techniques to further improve the performance of these systems and address the challenges associated with user interactions in the retrieval process.</td>
      <td>Static word embeddings map words with multiple senses into an average or most common-sense representation based on the training data used to compute the vectors. The vector of a word does not change with the other words used in a sentence around it. The transformer is a neural network designed to explicitly take into account the context of arbitrary long sequences of text, thanks to a special neural layer called self-attention , used in conjunction with feed forward and linear layers. The self-attention layer maps input sequences to output sequences of the same length. When computing the INLINEFORM0 -th output element, the layer can access all the INLINEFORM1 input elements (bidirectional self-attention) or only the first INLINEFORM2 input elements (causal self-attention). A self-attention layer allows the network to take into account the relationships among different elements in the same input. When the input elements are tokens of a given text, a self-attention layer computes token representations that take into account their context, i.e., the surrounding words. In doing so, the transformer computes contextualised word embeddings , where the representation of each input token is conditioned by the whole input text. Transformers have been successfully applied to different natual language processing tasks, such as machine translation, summarisation, question answering and so on. All these tasks are special instances of a more general task, i.e., transforming an input text sequence to some output text sequence. The sequence-to-sequence model has been designed to address this general task. The sequence-to-sequence neural network is composed of two parts: an encoder model, which receives an input sequence and builds a contextualised representation of each input element, and a decoder model, which uses these contextualised representations to generate a task-specific output sequence. Both models are composed of several stacked transformers. The transformers in the encoder employ bidirectional self-attention layers on the input sequence or the output sequence of the previous transformer. The transformers in the decoder employ causal self-attention on the previous decoder transformer's output and bidirectional cross-attention of the output of the final encoder transformer's output. In neural IR, two specific instances of the sequence-to-sequence models have been studied: encoder-only models and encoder-decoder models. Encoder-only models receive as input all the tokens of a given input sentence, and they compute an output contextualised word embedding for each token in the input sentence. Representatives of this family of models include INLINEFORM0 BIBREF21 , INLINEFORM1 BIBREF22 , and INLINEFORM2 BIBREF34 . Encoder-decoder models generate new output sentences depending on the given input sentence. The encoder model receives as input all the tokens of a given sequence and builds a contextualised representation, and the decoder model sequentially accesses these embeddings to generate new output tokens, one token at a time. Representatives of this family of models include INLINEFORM3 BIBREF35 and INLINEFORM4 BIBREF36 . Sequence-to-sequence models can be trained as language models, by projecting with a linear layer every output embedding to a given vocabulary and computing the tokens probabilities with a INLINEFORM0 operation. The INLINEFORM1 operation is a function INLINEFORM2 that takes as input INLINEFORM3 real values INLINEFORM4 and transforms each input INLINEFORM5 as follows:  DISPLAYFORM0  The INLINEFORM0 operation normalises the input values into a probability distribution. In the context of deep learning, the inputs of a INLINEFORM1 operation are usually called logits , and they represent the raw predictions generated by a multi-class classification model, turned into a probability distribution over the classes by the INLINEFORM2 operation. Depending on the training objective, a sequence-to-sequence model can be trained as a masked language model (MLM), as for INLINEFORM0 , or a casual language model (CLM), as for INLINEFORM1 . MLM training focuses on learning to predict missing tokens in a sequence given the surrounding tokens; CLM training focuses on predicting the next token in an output sequence given the preceding tokens in the input sequence. In both cases, it is commonplace to train these models using massive text data to obtain pre-trained language models . In doing so, we allow the model to learn general-purpose knowledge about a language that can be adapted afterwards to a more specific downstream task . In this transfer learning approach, a pre-trained language model is used as initial model to fine-tune it on a domain-specific, smaller training dataset for the downstream target task. In other words, fine-tuning is the procedure to update the parameters of a pre-trained language model for the domain data and target task. As illustrated in Figure FIGREF15 , pre-training typically requires a huge general-purpose training corpus, such as Wikipedia or Common Crawl web pages, expensive computation resources and long training times, spanning several days or weeks. On the other side, fine-tuning requires a small domain-specific corpus focused on the downstream task, affordable computational resources and few hours or days of additional training. Special cases of fine-tuning are few-shot learning , where the domain-specific corpus is composed of a very limited number of training data, and zero-shot learning , where a pre-trained language model is used on a downstream task that it was not fine-tuned on. In neural IR, the interaction-focused systems that use pre-trained language models are called cross-encoder models, as they receive as input a pair INLINEFORM0 of query and document texts. Depending on the type of sequence-to-sequence model, different cross-encoders are fine-tuned in different ways, but, in general, they aim at computing a relevance score INLINEFORM1 to rank documents w.r.t. a given query. In the following, we illustrate the most common cross-encoders leveraging both encoder-only models (Sec. SECREFU16 ) and encoder-decoder models (Sec. SECREFU20 ).</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Interaction-focused Systems</td>
      <td>Ranking with Encoder-only Models</td>
      <td>Interaction-focused Systems - Ranking with Encoder-only Models<br/><br/>In recent years, there has been a growing interest in developing neural information retrieval systems that focus on capturing the interaction between queries and documents. These systems aim to improve the ranking performance by leveraging the contextual information present in the query-document pairs. One approach that has gained attention is the use of encoder-only models, which learn representations of terms and capture context from the entire text [REF0].<br/><br/>Encoder-only models have shown promising results in ad-hoc document ranking tasks. By incorporating contextualized language models into existing neural ranking architectures, these models can effectively capture the relevance information between queries and documents. This is achieved by using multiple similarity matrices, one for each layer of the language model. The use of contextualized embeddings in these models has been found to significantly improve ranking performance, surpassing state-of-the-art results on benchmark datasets such as Robust 2004 and WebTrack 2012-2014 [REF0].<br/><br/>Furthermore, the combination of encoder-only models with BERT's classification mechanism has been shown to further enhance ranking performance. This approach, known as CEDR (Contextualized Embeddings for Document Ranking), leverages the power of BERT's contextualized representations and achieves state-of-the-art results in ad-hoc document ranking tasks [REF0].<br/><br/>To ensure efficient computation, various techniques have been explored in the implementation of encoder-only models. For instance, reduced precision arithmetic has been applied during decoding on TPUs, resulting in minimal loss in log perplexity and no loss in BLEU scores [REF1]. Additionally, the use of residual connections between LSTM layers in the encoder and decoder networks has been found to improve gradient flow during training, enabling the training of deeper networks [REF2].<br/><br/>The availability of large-scale datasets has played a crucial role in advancing the field of neural information retrieval. Previously, the limited availability of datasets hindered rapid progress and hindered the comparison of different models. However, the release of datasets such as MS MARCO and TREC CAR has provided researchers with valuable resources for training data-hungry neural models [REF3]. MS MARCO, for example, comprises a large collection of anonymized questions sampled from Bing's search query logs, along with a corpus of passages for the passage ranking task [REF3].<br/><br/>In the context of passage re-ranking, BERT has been widely adopted as a re-ranker due to its effectiveness in estimating the relevance of candidate passages to a given query [REF5]. By feeding the query as sentence A and the passage text as sentence B, BERT can effectively score and re-rank candidate passages based on their relevance to the query [REF5].<br/><br/>The design of interaction-focused systems often involves arranging models as stages in a pipeline, balancing the size of the candidate set with the complexity of the model [REF7]. This approach allows for the benefits of richer models while controlling the increased inference latencies. For example, monoBERT and duoBERT, which are pointwise and pairwise classification models of document relevance, respectively, have been integrated into a multistage ranking architecture to achieve improved effectiveness with only a modest increase in inference latency [REF7].<br/><br/>In conclusion, interaction-focused systems that leverage encoder-only models have shown promising results in ad-hoc document ranking tasks. By incorporating contextualized language models and utilizing large-scale datasets, these systems have achieved state-of-the-art performance. The efficient implementation of these models, along with the use of multistage ranking architectures, further enhances their effectiveness in capturing the interaction between queries and documents.</td>
      <td>The most widely adopted transformer architecture in neural IR is INLINEFORM0 , an encoder-only model. Its input text is tokenised using the WordPiece sub-word tokeniser BIBREF37 . The vocabulary INLINEFORM1 of this tokeniser is composed of INLINEFORM2 terms, where the uncommon/rare words, e.g., goldfish , are splitted up in sub-words, e.g., gold## and ##fish . The first input token of INLINEFORM3 is always the special [CLS] token, that stands for “classification”. INLINEFORM4 accepts as input other special tokens, such as [SEP] , that denotes the end of a text provided as input or to separate two different texts provided as a single input. INLINEFORM5 accepts as input at most 512 tokens, and produces an output embedding in INLINEFORM6 for each input token. The most commonly adopted INLINEFORM7 version is INLINEFORM8 base , which stacks 12 transformer layers, and whose output representation space has INLINEFORM9 dimensions.  BIBREF38 and BIBREF39 illustrated how to fine-tune INLINEFORM0 as a cross-encoder FOOTREF17 , in two slightly different ways. Given a query-document pair, both texts are tokenised into token sequences INLINEFORM1 and INLINEFORM2 . Then, the tokens are concatenated with INLINEFORM3 special tokens to form the following input configuration:  INLINEFORM0  that will be used as INLINEFORM0 input. In doing so, the self-attention layers in the INLINEFORM1 encoders are able to take into account the semantic interactions among the query tokens and the document tokens. The output embedding INLINEFORM2 , corresponding to the input [CLS] token, serves as a contextual representation of the query-document pair as a whole.  BIBREF40 fine-tune INLINEFORM0 on a binary classification task to compute the query-document relevance score, as illustrated in Figure FIGREF18 . To produce the relevance score INLINEFORM1 , the query and the document are processed by INLINEFORM2 to generate the output embedding INLINEFORM3 , that is multiplied by a learned set of classification weights INLINEFORM4 to produce two real scores INLINEFORM5 and INLINEFORM6 , and then through a INLINEFORM7 operation to transform the scores into a probability distribution INLINEFORM8 and INLINEFORM9 over the non-relevant and relevant classes. The probability corresponding to the relevant class, conventionally assigned to label 1, i.e., INLINEFORM10 , is the final relevance score.  BIBREF39 fine-tune INLINEFORM0 by projecting the output embedding INLINEFORM1 through the learned matrix INLINEFORM2 into a single real value INLINEFORM3 , that represents the final relevance score.  DISPLAYFORM0  In Section SECREFU24 we illustrate how a INLINEFORM0 -based cross-encoder is typically fine-tuned for ad-hoc ranking.</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Interaction-focused Systems</td>
      <td>Ranking with Encoder-decoder Models</td>
      <td>Interaction-focused Systems - Ranking with Encoder-decoder Models<br/><br/>Encoder-decoder models have gained significant attention in the field of neural information retrieval due to their ability to capture the interaction between queries and documents. These models, inspired by the success of transformer-based architectures like BERT and GPT, have shown promising results in various natural language processing tasks. In this section, we delve into the use of encoder-decoder models for ranking in interaction-focused systems.<br/><br/>One key aspect of encoder-decoder models is their ability to generate relevant and coherent responses by modeling the conditional probability distribution of the target sequence given the input sequence. This makes them well-suited for ranking tasks in information retrieval, where the goal is to retrieve the most relevant documents given a query.<br/><br/>To train encoder-decoder models for ranking, various objectives have been proposed. One commonly used objective is the denoising objective, also known as masked language modeling [REF1]. In this objective, the model is trained to predict missing or corrupted tokens in the input sequence. This approach has been shown to improve performance and has become a standard practice in pre-training encoder-decoder models [REF1].<br/><br/>Another important consideration in training encoder-decoder models is the choice of pre-training data. Large and diverse datasets, such as the Colossal Clean Crawled Corpus (C4), have been used to boost performance in downstream tasks [REF2]. The use of such datasets allows the model to learn generalizable knowledge that can be applied to a wide range of ranking tasks.<br/><br/>In the context of ranking with encoder-decoder models, the architecture plays a crucial role. The standard encoder-decoder architecture uses fully-visible masking in the encoder and the encoder-decoder attention, with causal masking in the decoder [REF3]. However, alternative architectures, such as decoder-only prefix language models, have also been explored [REF4]. These models leverage fully-visible self-attention over the input, which can improve the ranking performance.<br/><br/>Furthermore, the scalability of encoder-decoder models has been a topic of interest. Scaling up the model size, training steps, and ensembling techniques have been investigated to improve performance [REF8]. It has been observed that increasing the model size and training steps can lead to better ranking results, highlighting the importance of computational resources in training encoder-decoder models.<br/><br/>In addition to the technical aspects, the evaluation of encoder-decoder models for ranking in interaction-focused systems is crucial. Metrics such as precision@k and mean average precision (MAP) are commonly used to assess the effectiveness of these models in retrieving relevant documents.<br/><br/>Overall, encoder-decoder models have shown great potential in ranking tasks within interaction-focused systems. Their ability to capture the interaction between queries and documents, coupled with effective training objectives and large-scale pre-training data, make them a promising approach for improving the retrieval performance in neural information retrieval systems.<br/><br/>[REF1] Dai, Z., Le, Q. (2015). Semi-supervised sequence learning. In Advances in Neural Information Processing Systems.<br/><br/>[REF2] Radford, A., et al. (2018). Improving language understanding by generative pre-training. URL: https://s3-us-west-2.amazonaws.com/openai-assets/researchcovers/languageunsupervised/language_understanding_paper.pdf<br/><br/>[REF3] Taylor, W. (1953). Cloze procedure: A new tool for measuring readability. Journalism Quarterly.<br/><br/>[REF4] Devlin, J., et al. (2018). BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics.<br/><br/>[REF8] Lan, Z., et al. (2019). ALBERT: A lite BERT for self-supervised learning of language representations. In International Conference on Learning Representations.</td>
      <td>Instead of using an encoder-only transformer model to compute the latent representation of a query-document pair and to convert it into a relevance score, it is possible also to use an encoder-decoder model BIBREF36 with prompt learning , by converting the relevance score computation task into a cloze test, i.e., a fill-in-the-blank problem. Prompting has been successfully adopted in article summarisation tasks BIBREF41 and knowledge base completion tasks BIBREF42 . In prompt learning, the input texts are reshaped as a natural language template, and the downstream task is reshaped as a cloze-like task. For example, in topic classification, assuming we need to classify the sentence text into two classes INLINEFORM0 and INLINEFORM1 , the input template can be:  INLINEFORM0  Among the vocabulary terms, two label terms INLINEFORM0 and INLINEFORM1 are selected to correspond to the classes INLINEFORM2 and INLINEFORM3 , respectively. The probability to assign the input text to a class can be transferred into the probability that the input token [OUT] is assigned to the corresponding label token:  INLINEFORM0   BIBREF43 proposed a prompt learning approach for relevance ranking using a INLINEFORM0 model FOOTREF21 , as illustrated in Figure FIGREF22 . The query and the document texts INLINEFORM1 and INLINEFORM2 are concatenated to form the following input template:  INLINEFORM0  An encoder-decoder model is fine-tuned with a downstream task taking as input this input configuration, and generating an output sequence whose last token is equal to True or False , depending on whether the document INLINEFORM0 is relevant or non-relevant to the query INLINEFORM1 . The query-document relevance score is computed by normalising only the False and True output probabilities, computed over the whole vocabulary, with a INLINEFORM0 operation. To produce the relevance score INLINEFORM0 , the query and the document are processed by INLINEFORM1 to generate the output embedding INLINEFORM2 , that is projected by a learned set of classification weights INLINEFORM3 over the vocabulary INLINEFORM4 . The outputs INLINEFORM5 and INLINEFORM6 corresponding to the False and True terms, respectively, are transformed into a probability distribution with a INLINEFORM7 operation, to yield the required predictions INLINEFORM8 and INLINEFORM9 over the “non-relevant” and “relevant” classes. The prediction corresponding to the relevant class, i.e., INLINEFORM10 , is the final relevance score.  DISPLAYFORM0  In the next section we discuss how a INLINEFORM0 -based cross-encoder is typically fine-tuned for ad-hoc ranking.</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Interaction-focused Systems</td>
      <td>Fine-tuning Interaction-focused Systems</td>
      <td>Interaction-focused Systems - Fine-tuning Interaction-focused Systems<br/><br/>Fine-tuning interaction-focused systems is an important aspect of neural information retrieval. In this context, fine-tuning refers to the process of optimizing the performance of a system by adjusting its parameters based on user interactions and feedback. This section explores the significance of fine-tuning in interaction-focused systems and discusses relevant approaches and techniques.<br/><br/>One approach to fine-tuning interaction-focused systems is to leverage side-information. Side-information refers to additional data that can provide insights into the underlying structure and relationships within the dataset [REF0]. By incorporating more side-information, the quality of the clusterings found by the system can be improved [REF0]. For instance, in clustering problems, algorithms that learn a distance metric based on side-information have shown promising results [REF2]. These algorithms optimize the metric learning process by formulating it as a convex optimization problem, enabling the derivation of efficient and local-optima-free algorithms [REF0].<br/><br/>Another aspect of fine-tuning interaction-focused systems is the generalization of learned metrics to previously unseen data. Traditional methods such as Multidimensional Scaling (MDS) and Locally Linear Embedding (LLE) focus on finding embeddings for the points in the training set, limiting their ability to generalize to new data [REF1]. In contrast, fine-tuning approaches that learn a full metric over the input space can generalize more easily to unseen data [REF1]. This capability is crucial in scenarios where the system needs to adapt to new information and user preferences.<br/><br/>Supervised learning settings, such as nearest neighbor classification, have also explored the definition and learning of metrics for classification tasks [REF2]. While these methods often yield good metrics for classification, their effectiveness in learning general metrics for other algorithms, such as K-means, remains uncertain, especially when dealing with less structured information [REF2]. However, recent approaches have shown promise in leveraging similarity information to learn distance metrics for clustering tasks [REF2].<br/><br/>Fine-tuning interaction-focused systems can also involve the use of clustering with side information [REF4]. This approach involves learning a distance metric using similarity information and then clustering the data based on that metric [REF4]. By incorporating side information, such as indicators of whether two points belong to the same or different clusters, the clustering performance can be significantly improved [REF4]. This approach has been demonstrated to outperform traditional clustering algorithms like K-means and constrained K-means [REF4].<br/><br/>To optimize the fine-tuning process, various algorithms and techniques have been proposed. These include convex optimization methods [REF9], iterative projection methods [REF8], and the use of the Newton-Raphson method [REF7]. These algorithms enable efficient and local-minima-free optimization of the system's parameters, ensuring that the fine-tuning process is effective and reliable [REF9].<br/><br/>In summary, fine-tuning interaction-focused systems plays a crucial role in optimizing their performance. By leveraging side-information, generalizing learned metrics, and incorporating clustering with side information, these systems can adapt to user preferences and improve their retrieval capabilities. The use of efficient optimization algorithms further enhances the fine-tuning process, ensuring its effectiveness and reliability.</td>
      <td>As discussed in Section SECREFU13 , the pre-trained language models adopted in IR require a fine-tuning of the model on a specific downstream task. Given an input query-document pair INLINEFORM0 a neural IR model INLINEFORM1 , parametrised by INLINEFORM2 , computes INLINEFORM3 , the score of the document INLINEFORM4 w.r.t. the query INLINEFORM5 . We are supposed to predict INLINEFORM6 from INLINEFORM7 , where INLINEFORM8 stands for non-relevant and INLINEFORM9 for relevant. This problem can be framed as a binary classification problem. We perform the classification by assuming a joint distribution INLINEFORM0 over INLINEFORM1 from which we can sample correct pairs INLINEFORM2 and INLINEFORM3 . Using sampled correct pairs we learn a score function INLINEFORM4 as an instance of a metric learning problem BIBREF44 : the score function must assign a high score to a relevant document and a low score to a non-relevant document, as in Eq. ( EQREF19 ) and Eq. ( EQREF23 ). Then we find INLINEFORM5 that minimises the (binary) cross entropy INLINEFORM6 between the conditional probability INLINEFORM7 and the model probability INLINEFORM8 :  DISPLAYFORM0  where the expectation is computed over INLINEFORM0 , and the cross entropy is computed as:  DISPLAYFORM0  Typically, a dataset INLINEFORM0 available for fine-tuning pre-trained language models for relevance scoring is composed of a list of triples INLINEFORM1 , where INLINEFORM2 is a query, INLINEFORM3 is a relevant document for the query, and INLINEFORM4 is a non-relevant document for the query. In this case, the expected cross entropy is approximated by the sum of the cross entropies computed for each triple:  DISPLAYFORM0  In doing so, we do not take into account documents in the collection that are not explicitly labeled as relevant or non-relevant. This approach is limited to take into account positive and negative triples in a pairwise independent fashion. In Section SECREFU38 we will discuss a different fine-tuning approach commonly used for representation-focused systems, taking into account multiple non-relevant documents per relevant document.</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Interaction-focused Systems</td>
      <td>Dealing with long texts</td>
      <td>Interaction-focused Systems - Dealing with long texts<br/><br/>Dealing with long texts poses a significant challenge in neural information retrieval systems. Long documents often contain multiple passages that may be relevant to a user's query, requiring effective methods to handle and retrieve the most relevant information. In this section, we discuss various approaches and techniques employed by interaction-focused systems to address this challenge.<br/><br/>One approach to handling long texts is the use of passage-based retrieval models. These models aim to identify and rank relevant passages within a document, rather than treating the entire document as a single unit [REF4]. For instance, PARADE (Passage Ranking with Document-level Context) is an end-to-end document reranking model that incorporates diverse relevance signals from the full text into ad-hoc ranking [REF0]. By aggregating relevance signals across passages, PARADE improves the effectiveness of retrieval, particularly when the number of relevant passages per document is low [REF0]. Similarly, Birch-Passage, an improved variant of the Birch model, utilizes passages instead of sentences as input and is trained end-to-end on the target corpus [REF2]. These passage-based models demonstrate the importance of considering passage-level relevance in long document retrieval.<br/><br/>Another approach is the use of passage aggregation techniques to determine the overall relevance score of a document. ELECTRA-MaxP, for example, adopts the maximum score of passages within a document as the overall relevance score [REF2]. This approach allows for a more comprehensive representation of the document's relevance, taking into account the varying degrees of relevance across different passages. ELECTRA-KNRM, on the other hand, is a kernel-pooling neural ranking model that leverages the query-document similarity matrix to capture the relevance between passages and the query [REF3]. These techniques highlight the importance of effectively aggregating evidence from multiple passages to improve retrieval performance.<br/><br/>In addition to passage-based models and aggregation techniques, the choice of dataset also plays a crucial role in addressing the challenge of long texts. Different datasets exhibit variations in document length and the number of relevant passages per document. For instance, the TREC DL and MS MARCO datasets share similarities in query overlap, suggesting that queries in both collections can be sufficiently answered by a single highly relevant passage [REF6]. On the other hand, the Genomics dataset contains "natural" passages that can be longer, requiring consideration when drawing conclusions [REF1]. Understanding the characteristics of the dataset is essential for developing effective retrieval models tailored to specific contexts.<br/><br/>Furthermore, the evaluation of interaction-focused systems on long texts is facilitated by the availability of benchmark collections. The TREC-COVID challenge, for example, was developed to address the urgent demand for reliable retrieval of COVID-19 academic literature [REF7]. This challenge utilizes the CORD-19 dataset, which is a dynamic collection enlarged over time [REF7]. The availability of such benchmark collections enables researchers to evaluate and compare the performance of their systems in retrieving information from long texts.<br/><br/>In summary, interaction-focused systems employ various techniques to handle long texts in neural information retrieval. Passage-based models, passage aggregation techniques, dataset characteristics, and benchmark collections all contribute to addressing the challenges associated with long documents. These approaches aim to improve the effectiveness of retrieval by considering passage-level relevance and effectively aggregating evidence from multiple passages.</td>
      <td>INLINEFORM0 and INLINEFORM1 models have an input size limited to 512 tokens, including the special ones. When dealing with long documents, that cannot be fed completely into a transformer model, we need to split them into smaller texts in a procedure referred to as passaging . BIBREF45 propose to split a long document into overlapping shorter passages, to be processed independently together with the same query by a cross-encoder. During training, if a long document is relevant, all its passages are relevant, and vice-versa. Then, the relevance scores for each composing passage are aggregated back to a single score for the long document. In this scenario, the common aggregation functions are INLINEFORM2 , i.e., the document score is the score of the first passage, INLINEFORM3 , i.e., the document score is the highest score across all passages, and INLINEFORM4 , i.e., the document score is the sum of the scores of its passages. Alternatively, BIBREF46 generate the [CLS] output embedding for each passage to compute a query-passage representation for each passage. Then, the different passage embeddings are aggregated together to compute a final relevance score for the whole document using feed forward neural networks, convolutional neural networks or simple transformer architectures.</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Representation-focused Systems</td>
      <td>Single Representations</td>
      <td>Representation-focused systems in neural information retrieval aim to improve the effectiveness of retrieval models by focusing on the representation of documents and queries. In this section, we will discuss the use of single representations in these systems, highlighting various approaches and their impact on retrieval performance.<br/><br/>One approach to representation-focused systems is the use of dense retrieval models. Khattab and Zaharia [REF0] introduced a late-interaction operator on top of BERT encoders, demonstrating the feasibility of full dense retrieval in information retrieval tasks. Das et al. [REF0] proposed an iterative passage retrieval method for open-domain question answering, where relevant passages are retrieved using reformulated question vectors. Seo et al. [REF0] took a different approach by encoding candidate answer phrases as vectors and directly retrieving answers to input questions. These approaches show promising results in improving retrieval accuracy in various tasks.<br/><br/>Model distillation is another prominent approach in representation-focused systems. It involves training a smaller model based on the predictions of a larger model [REF1]. This approach can enhance the performance of a smaller model when trained on specific task data. Quantization techniques have also been explored to reduce model sizes, making neural networks suitable for embedded systems [REF1]. These approaches provide alternative methods to improve retrieval performance by leveraging distilled or quantized models.<br/><br/>Hard negative sampling is a technique that has been widely used to optimize retrieval performance. STAR [REF3] effectively minimizes top-K pairwise errors by employing hard negative sampling. BM25 Neg and ANCE [REF3] use static hard negative sampling, achieving compromised top-ranking performance. ADORE [REF3] improves top-ranking performance by training the document encoder using STAR. These results demonstrate the effectiveness of hard negative sampling in optimizing retrieval performance.<br/><br/>End-to-end training has been shown to improve ranking performance in representation-focused systems. ADORE [REF2] utilizes end-to-end training to optimize ranking performance for different compressed indexes. The results indicate that different compression techniques can benefit from end-to-end training, making ADORE suitable for improving the performance of compressed indexes. Additionally, ADORE improves retrieval performance by mapping queries closer to relevant documents, enhancing the overall retrieval effectiveness [REF2].<br/><br/>Efficiency is another important aspect of representation-focused systems. ANCE serves as an efficiency baseline due to its competitive effectiveness [REF6]. The training efficiency of proposed methods, such as STAR and ADORE, is evaluated in terms of training time and computational resources [REF6]. These evaluations provide insights into the efficiency gains achieved by the proposed methods.<br/><br/>In summary, representation-focused systems in neural information retrieval employ various techniques to enhance retrieval performance. Dense retrieval models, model distillation, hard negative sampling, end-to-end training, and efficiency considerations are among the key approaches used in these systems. These techniques contribute to improving the effectiveness and efficiency of retrieval models in neural information retrieval tasks.</td>
      <td>In interaction-focused systems discussed in Section SECREF3 , the query and document texts are concatenated together before processing with sequence-to-sequence models, yielding rich interactions between the query context and the document context, as every word in the document can attend to every word in the query, and vice-versa. At query processing time, every document must be concatenated with the query and must be processed with a forward pass of the whole sequence-to-sequence model. Even if some techniques such as the pre-computation of some internal representations have been proposed BIBREF47 , interaction-focused systems cannot scale to a large amount of documents. In fact, on a standard CPU, the processing of a query over the whole document collection exploiting an inverted index requires a few milliseconds BIBREF3 , while computing the relevance score of a single query-document pair with a transformer model may requires a few seconds BIBREF39 . Instead of leveraging sequence-to-sequence models to compute a semantically richer but computationally expensive interaction representation INLINEFORM0 , representation-focused systems employ encoder-only models to independently compute query representations INLINEFORM1 and document representations INLINEFORM2 in the same latent vector space BIBREF48 , as illustrated in Figure FIGREF30 . Next, the relevance score between the representations is computed via an aggregation function INLINEFORM3 between these representations:  DISPLAYFORM0  In neural IR, the representation functions INLINEFORM0 and INLINEFORM1 are computed through fine-tuned encoder-only sequence-to-sequence models such as INLINEFORM2 . The same neural model is used to compute both the query and the document representations, so the model is also called dual encoder or bi-encoder BIBREF49 . A bi-encoder maps queries and documents in the same vector space INLINEFORM3 , in such a way that the representations can be mathematically manipulated. Usually, the output embedding corresponding to the [CLS] token is assumed to be the representation of a given input text. Using these single representations , the score aggregation function is the dot product:  DISPLAYFORM0  Different single-representation systems have been proposed: INLINEFORM0 BIBREF50 , INLINEFORM1 BIBREF51 , and INLINEFORM2 BIBREF52 being the most widely adopted. The main difference among these systems is how the fine-tuning of the INLINEFORM3 model is carried out, as discussed in Section SECREFU38 .</td>
    </tr>
    <tr>
      <th>10</th>
      <td>Representation-focused Systems</td>
      <td>Multiple Representations</td>
      <td>Representation-focused systems in neural information retrieval often explore the use of multiple representations to enhance retrieval performance. These systems leverage different types of representations, such as down-projected representations and hybrid models, to strike a balance between fidelity and generalization [REF0].<br/><br/>One approach is to use down-projected representations, where the output representations of models like BERT are transformed using a feed-forward layer with a reduced dimensionality [REF0]. These down-projected representations, referred to as ME-BERT-k, allow for more efficient computation while still capturing important information from the original representations.<br/><br/>Another approach is the use of hybrid models, which combine the strengths of both sparse and dense representations [REF0]. These models linearly combine the scores of a sparse system and a dense system using a trainable weight, resulting in a hybrid model. For example, a hybrid model combining ME-BERT and BM25-uni is referred to as HYBRID-ME-BERT-uni [REF0]. By leveraging both sparse and dense representations, hybrid models aim to achieve a balance between fidelity and generalization.<br/><br/>The use of multiple representations in representation-focused systems is motivated by the limitations of traditional approaches. For instance, traditional bag-of-words (BOW) models rely on exact lexical match between query and document terms, which can lead to vocabulary mismatch issues [REF3]. Neural IR models, on the other hand, employ soft matching techniques to model complex matching and address vocabulary and semantic mismatch problems [REF3]. The introduction of contextualized representations from deep language models further enhances the ability to capture semantic similarity [REF3].<br/><br/>The effectiveness of multiple representations has been demonstrated in various studies. For example, ColBERT proposes a late interaction paradigm that combines contextualized embeddings of queries and documents, achieving competitive effectiveness while significantly reducing computational cost compared to models like BERT [REF4, REF6]. Dual encoders based on BERT have also shown promising results in first-stage retrieval, leveraging learned dense low-dimensional encodings to improve recall and exploit machine learning for generalization [REF1, REF5].<br/><br/>In summary, representation-focused systems in neural information retrieval often leverage multiple representations to enhance retrieval performance. These systems explore down-projected representations and hybrid models to strike a balance between fidelity and generalization. The use of multiple representations addresses limitations of traditional approaches and allows for better modeling of complex matching and semantic similarity.</td>
      <td>Up so far, we considered representation-focused systems in which queries and documents are represented through a single embedding in the latent vector space. This single representation is assumed to incorporate the meaning of an entire text within that single embedding. In contrast, multiple representation systems such as poly-encoders BIBREF53 , INLINEFORM0 BIBREF54 , INLINEFORM1 BIBREF55 and INLINEFORM2 BIBREF56 exploit more than a single embedding to represent a given text, which may allow a richer semantic representation of the content. Instead of using just the first output embedding INLINEFORM0 to encode a document INLINEFORM1 , poly-encoders BIBREF53 exploit the first INLINEFORM2 output embeddings INLINEFORM3 . A query INLINEFORM4 is still represented with the single embedding INLINEFORM5 , while we need to aggregate the INLINEFORM6 output document embeddings into a single representation INLINEFORM7 to compute the final relevance score using the dot product with the output query embedding. To do so, poly-encoders first compute the INLINEFORM8 similarities INLINEFORM9 between the query embedding and the first INLINEFORM10 document embedding using the dot product. These similarities are transformed into normalised weights INLINEFORM11 using a INLINEFORM12 operation, and the weighted output embeddings are summed up to compute the final document embedding INLINEFORM13 used to compute the relevance score:  DISPLAYFORM0  Similarly to poly-encoders, INLINEFORM0 BIBREF54 exploits the first INLINEFORM1 output embeddings to represent a document INLINEFORM2 (including the [CLS] embedding), but uses a different strategy to compute the relevance score INLINEFORM3 w.r.t. a query INLINEFORM4 . INLINEFORM5 computes the similarity between the query embedding and the first INLINEFORM6 document embedding using the dot product, and the maximum similarity, also called maximum inner product , represents the relevance score:  DISPLAYFORM0  This relevance scoring function, called max similarity or maxsim , allows us to exploit efficient implementations of maximum inner product search systems, discussed in Section SECREF5 . On the contrary, the relevance scoring function in Eq. ( EQREF34 ), based on a INLINEFORM0 operation, does not permit to decompose the relevance scoring to a maximum computation over dot products. Differently from poly-encoders and INLINEFORM0 , INLINEFORM1 BIBREF55 does not limit to INLINEFORM2 the number of embeddings used to represent a document. Instead, it uses all the INLINEFORM3 output embeddings to represent a document, i.e., one output embedding per document token, including the [CLS] special token. Moreover, also a query INLINEFORM4 is represented with multiple INLINEFORM5 output embeddings, i.e., one output embedding per query token, including the [CLS] special token. As in other representation-focused systems, query token embeddings are computed at query processing time; queries may also be augmented with additional masked tokens to provide “ a soft, differentiable mechanism for learning to expand queries with new terms or to re-weigh existing terms based on their importance for matching the query ” BIBREF55 . In current practice, queries are augmented up to 32 query token embeddings. Without loss of generality, query and documents embeddings can be projected in a smaller latent vector space through a learned weight matrix INLINEFORM6 , with INLINEFORM7 . Since there are multiple query embeddings, INLINEFORM0 exploits a modified version of the relevance scoring function in Eq ( EQREF35 ), where every query embedding contributes to the final relevance score by summing up its maximum dot product value w.r.t. every document embeddings:  DISPLAYFORM0   INLINEFORM0 's late interaction scoring in Eq. ( EQREF36 ), also called sum maxsim , performs an all-to-all computation: each query embedding, including the masked tokens' embeddings, is dot-multiplied with every document embedding, and then the maximum computed dot products for each query embedding are summed up. In doing so, a query term can contribute to the final scoring by (maximally) matching a different lexical token. A different approach is proposed by the INLINEFORM1 system BIBREF56 . In INLINEFORM2 , the query and document [CLS] embeddings are linearly projected with a learned matrix INLINEFORM3 . The embeddings corresponding to normal query and document tokens are projected into a smaller vector space with dimension INLINEFORM4 , using another learned matrix INLINEFORM5 . Typical values for INLINEFORM6 range from 8 to 32. The query-document relevance score is the sum of two components. The first component is the dot product of the projected query and document [CLS] embeddings, and the second component is the sum of sub-components, one per query token. Each sub component is the maximum inner product between a query token and the document embeddings for the same token:  DISPLAYFORM0  The INLINEFORM0 's scoring function, based on lexical matching between query and document tokens, allows us to pre-compute the projected document embeddings and, for each term in the vocabulary, to concatenate together the embeddings in the same document and in the whole collection, organising them in posting lists of embeddings, including a special posting list for the [CLS] token and its document embeddings. This organisation permits the efficient processing of posting lists at query time with optimised linear algebra libraries such as INLINEFORM1 BIBREF57 . Note that the projected query embeddings are still computed at query processing time.</td>
    </tr>
    <tr>
      <th>11</th>
      <td>Representation-focused Systems</td>
      <td>Fine-tuning Representation-focused Systems</td>
      <td>Representation-focused systems in neural information retrieval aim to improve the retrieval performance by fine-tuning the representation models. Fine-tuning involves training the models on specific tasks or datasets to enhance their ability to capture relevant information and generate accurate representations. In this section, we discuss the fine-tuning of representation-focused systems and its impact on retrieval effectiveness.<br/><br/>One common approach to fine-tuning representation-focused systems is to leverage pre-trained models and adapt them to the retrieval task at hand. For instance, Chen et al. [REF0] applied pre-processing code from DrQA to extract clean text portions from Wikipedia articles and split them into passages for retrieval. They then fine-tuned the models using question-answering datasets, such as SQuAD and Natural Questions, to improve the retrieval accuracy [REF0] [REF1]. The authors found that even with a small number of training examples, a dense passage retriever trained using a general pre-trained language model outperformed traditional retrieval methods like BM25 [REF2]. This suggests that fine-tuning representation models can significantly enhance retrieval performance.<br/><br/>In fine-tuning representation-focused systems, the choice of negative examples plays a crucial role in learning high-quality encoders. Different types of negatives, such as random passages, BM25 top passages, or gold passages from other questions, can be considered [REF4]. The selection of negatives can impact the performance of the system, and it has been observed that reusing gold passages from the same batch as negatives can improve computation efficiency while achieving excellent performance [REF4]. Additionally, in-batch negative training, which reuses negative examples within the same batch, has been shown to be an effective and memory-efficient approach to increase the number of training examples and improve model performance [REF8].<br/><br/>Dense vector representations have proven to be valuable in retrieval tasks, as they can capture semantic relevance even without exact token matching [REF9]. By training discriminatively on labeled pairs of queries and documents, dense encoders can provide high similarity scores to semantically relevant text pairs [REF9]. Furthermore, the use of pre-trained models and cross-attention mechanisms has shown effectiveness in passage or dialogue re-ranking tasks [REF9]. These approaches complement sparse vector methods and offer the flexibility to generate task-specific representations.<br/><br/>In summary, fine-tuning representation-focused systems in neural information retrieval involves adapting pre-trained models to the retrieval task at hand. This process can significantly enhance retrieval performance by leveraging the power of dense vector representations and optimizing the choice of negative examples. By fine-tuning representation models, researchers have achieved improved accuracy and surpassed traditional retrieval methods like BM25.</td>
      <td>The fine-tuning of a bi-encoder corresponds to learning an appropriate inner-product function suitable for the ad-hoc ranking task, i.e., for relevance scoring. As in Section SECREFU24 , we have a neural IR model INLINEFORM0 , parametrised by INLINEFORM1 , that computes a score INLINEFORM2 for a document INLINEFORM3 w.r.t. a query INLINEFORM4 . We now frame the learning problem as a probability estimation problem. To this end, we turn the scoring function into a proper conditional distribution by using a INLINEFORM5 operation:  DISPLAYFORM0  where INLINEFORM0 represents the posterior probability of the document being relevant given the query. We assume to have a joint distribution INLINEFORM1 over INLINEFORM2 , and we want to find the parameters INLINEFORM3 that minimise the cross entropy INLINEFORM4 between the actual probability INLINEFORM5 and the model probability INLINEFORM6 :  DISPLAYFORM0  where the expectation is computed over INLINEFORM0 . If the scoring function INLINEFORM1 is expressive enough, then, for some INLINEFORM2 , we have INLINEFORM3 . The cross entropy loss in Eq. ( EQREF40 ) is difficult to optimise, since the number of documents in INLINEFORM0 is large, and then the denominator in Eq. ( EQREF39 ), also known as partition function , is expensive to compute. In noise contrastive estimation we choose an artificial noise distribution INLINEFORM1 over INLINEFORM2 of negative samples and maximise the likelihood of INLINEFORM3 contrasting INLINEFORM4 . Given INLINEFORM5 documents INLINEFORM6 , for each of them we define the following conditional distribution:  DISPLAYFORM0  which is significantly cheaper to compute that Eq. ( EQREF39 ) if INLINEFORM0 . Now, we want to find the parameters INLINEFORM1 that minimise the noise contrastive estimation loss INLINEFORM2 , defined as:  DISPLAYFORM0  where the expectation is computed over INLINEFORM0 and INLINEFORM1 for INLINEFORM2 . The end goal of this fine-tuning is to learn a latent vector space for query and document representations where a query and its relevant document(s) are closer, w.r.t. the dot product, than the query and its non-relevant documents BIBREF50 ; this fine-tuning approach is also called contrastive learning BIBREF58 . Negative samples are drawn from the noise distribution INLINEFORM0 over INLINEFORM1 . In the following, we list some negative sampling strategies adopted in neural IR. 1.  Random sampling : any random document from the corpus is considered non-relevant, with equal probability, i.e., INLINEFORM0 . Any number of negative documents can be sampled. Intuitively, it is reasonable to expect that a randomly-sampled document will obtain a relevance score definitely smaller than the relevance score of a positive document, with a corresponding loss value close to 0. Negative documents with near zero loss contribute little to the training convergence to identify the parameters INLINEFORM1 BIBREF59 .  2.  In-batch sampling : during training, the queries to compute the loss can be randomly aggregated into batches of size INLINEFORM0 , for faster training. For each query in a given batch, the positive passages for the other INLINEFORM1 queries are considered as negative passages for the query BIBREF61 . This sampling approach suffers from the same near zero loss problem as random sampling BIBREF51 , but the sampling prodedure is faster.  3.  Hard negative sampling : negative documents can be generated exploiting a classical or trained retrieval system. Each query is given as input to the retrieval system, the top documents retrieved, and the documents not corresponding to the positive ones are treated as negatives. Note that in this case we are assuming a conditional noise distribution INLINEFORM0 , since we assume to know the relevant document INLINEFORM1 for the query. In doing so, high-ranking documents are prioritised w.r.t. low-ranking documents, that do not impact on the user experience and do not contribute to the loss. The retrieval system used to mine the negative documents can exploit BM25 relevance model, as in INLINEFORM2 BIBREF50 , the currently neural model under training, as in INLINEFORM3 BIBREF51 , or another fine-tuned neural model, as in INLINEFORM4 BIBREF52 .</td>
    </tr>
    <tr>
      <th>12</th>
      <td>Retrieval Architectures and Vector Search</td>
      <td>MIP and NN Search Problems</td>
      <td>Retrieval Architectures and Vector Search - MIP and NN Search Problems<br/><br/>In the field of neural information retrieval, retrieval architectures and vector search play a crucial role in efficiently retrieving relevant information. One popular approach is the use of Multi-Index Hashing (MIP) and Nearest Neighbor (NN) search problems. These techniques leverage the power of hashing functions to map high-dimensional data into compact binary codes, enabling fast and accurate similarity search.<br/><br/>MIP and NN search problems involve the use of hash functions to map data points into hash codes, which can then be used to efficiently retrieve similar items. In MIP, the goal is to find the most similar items to a given query point, while in NN search, the objective is to find the nearest neighbors of a query point within a dataset.<br/><br/>The L2-ALSH (Asymmetric Locality Sensitive Hashing) is a widely used hash function for MIP and NN search problems. It combines the standard L2 hash function with a spherical multi-Gaussian random vector and a uniformly distributed random variable [REF0]. The collision probabilities of L2-ALSH are related to the desired notion of similarity between objects [REF3]. This approach has been shown to be effective in approximate nearest neighbor search and is widely used in various settings [REF3].<br/><br/>The L2-ALSH hash function is defined as hL2a,b(x) = a⊤x + br [REF0]. It maps data points to hash codes based on their inner product similarity. The hash codes generated by L2-ALSH can be used to efficiently retrieve similar items by comparing the hamming distances between hash codes [REF3]. This approach has been proven to be effective in approximate nearest neighbor search and has been widely adopted in the field [REF3].<br/><br/>Another important aspect of retrieval architectures and vector search is the consideration of asymmetry in hash functions. Asymmetric hashes, such as L2-ALSH, allow for different mappings of query points and database vectors, enabling efficient retrieval of similar items [REF4]. The power of asymmetry in LSH and binary hashing has been explored in recent studies, showing that it can enable LSH when symmetric LSH is not possible and yield shorter and more accurate hashes [REF5].<br/><br/>It is worth noting that the choice between tree-based methods and LSH-based methods depends on the specific requirements and characteristics of the dataset. While tree-based methods, such as cone trees, have been proposed for inner product search, LSH-based methods are often preferred due to their independence from the dimensionality of the data [REF5]. However, the exact regimes where LSH-based methods outperform tree-based methods are still under investigation [REF5].<br/><br/>In conclusion, retrieval architectures and vector search techniques, such as MIP and NN search problems, along with the use of L2-ALSH and asymmetry in hash functions, play a crucial role in efficient and accurate information retrieval. These approaches enable fast similarity search and have been widely adopted in various domains. Further research is needed to explore the optimal use of these techniques in different settings and to compare them with other retrieval methods.</td>
      <td>The pre-computed document embeddings are stored in a special data structure called index . In its simplest form, this index must store the document embeddings and provide a search algorithm that, given a query embedding, efficiently finds the document embedding with the largest dot product, or, more in general, with the maximum inner product. Formally, let INLINEFORM0 denote a query embedding, and let INLINEFORM1 denote a set of INLINEFORM2 document embeddings, with INLINEFORM3 for INLINEFORM4 . The goal of the maximum inner product (MIP) search is to find the document embedding INLINEFORM5 such that  DISPLAYFORM0  A data structure designed to store INLINEFORM0 is called embedding index . The naïve embedding index is the flat index , which stores the document embeddings in INLINEFORM1 explicitly and performs an exhaustive search to identify INLINEFORM2 . Its complexity is INLINEFORM3 both in space and time, so it is particularly inefficient for large INLINEFORM4 or INLINEFORM5 values. A common approach to improve the space and time efficiency of the flat index is to convert the maximum inner product search into a nearest neighbour (NN) search , whose goal is to find the document embedding INLINEFORM0 such that  DISPLAYFORM0  Many efficient index data structures exist for NN search. To leverage them with embedding indexes, MIP search between embeddings must be adapted to use the Euclidean distance and NN search. This is possible by applying the following transformation from INLINEFORM0 to INLINEFORM1 BIBREF62 :  DISPLAYFORM0  where INLINEFORM0 . By using this transformation, the MIP search solution INLINEFORM1 coincides with the NN search solution INLINEFORM2 . In fact, we have:  INLINEFORM0  Hence, hereinafter we consider the MIP search for ranking with a dense retriever as a NN search based on the Euclidean distance among the transformed embeddings INLINEFORM0 and INLINEFORM1 in INLINEFORM2 . To simplify the notation, from now on we drop the hat symbol from the embeddings, i.e., INLINEFORM3 and INLINEFORM4 , and we consider INLINEFORM5 as the new dimension INLINEFORM6 , i.e., INLINEFORM7 . The index data structures for exact NN search in low dimensional spaces have been very successful, but they are not efficient with high dimensional data, as in our case, due to the curse of dimensionality. It is natural to make a compromise between search accuracy and search speed, and the most recent search methods have shifted to approximate nearest neighbor (ANN) search . The ANN search approaches commonly used in dense retrieval can be categorised into three families: locality sensitive hashing approaches, quantisation approaches, and graph approaches.</td>
    </tr>
    <tr>
      <th>13</th>
      <td>Retrieval Architectures and Vector Search</td>
      <td>Locality sensitive hashing approaches</td>
      <td>Retrieval Architectures and Vector Search - Locality sensitive hashing approaches<br/><br/>Locality sensitive hashing (LSH) is a popular technique used in information retrieval systems to efficiently search for similar objects in high-dimensional spaces. LSH achieves this by mapping similar objects to the same or nearby hash buckets, allowing for fast retrieval of nearest neighbors. In recent years, there has been a growing interest in incorporating neural networks into the LSH framework to enhance retrieval performance. This section explores retrieval architectures and vector search techniques, with a focus on locality sensitive hashing approaches.<br/><br/>One approach to improve the efficiency of LSH is through the use of perturbation vectors. Perturbation vectors are generated by adding small random perturbations to the original query vector, resulting in a set of vectors that are close to the query. The likelihood of finding points close to the query is related to the score of the perturbation vectors [REF0]. Perturbation vectors with lower scores have a higher probability of yielding points near the query. However, generating perturbed queries in a data-independent way can be challenging, and many hashed buckets by the perturbed queries may be redundant [REF1].<br/><br/>To address these challenges, the multi-probe LSH method has been proposed. This method computes a non-overlapped bucket sequence based on the probability of containing similar objects, instead of generating perturbed queries [REF1]. By carefully deriving probing sequences, the multi-probe LSH method probes multiple hash buckets in a systematic manner, achieving desired search accuracy and query time with reduced space requirements [REF1]. Experimental results have shown that the multi-probe LSH method is more space-efficient than the basic LSH and entropy-based LSH methods [REF1].<br/><br/>The query-directed probing sequence is another technique used in LSH to reduce the number of hash tables and probes required to achieve the desired recall precisions [REF2]. Compared to the step-wise probing sequence, the query-directed probing sequence requires significantly fewer hash tables and probes [REF2]. This approach has been shown to be effective in achieving high recall with fewer hash tables, making it a promising technique for efficient vector search [REF5].<br/><br/>The step-wise probing method is motivated by the property of locality sensitive hashing, where buckets that are one step away from the query bucket are more likely to contain objects close to the query than buckets that are two steps away [REF3]. This method first probes all the 1-step buckets, then the 2-step buckets, and so on [REF3]. The total number of n-step buckets in an LSH index with L hash tables and M hash functions per table is L × M^n × 2^n [REF3]. By probing buckets in a step-wise manner, the step-wise probing method aims to efficiently explore buckets that are likely to contain nearest neighbors [REF3].<br/><br/>The multi-probe LSH method has been shown to achieve similar time efficiencies as the basic LSH method while reducing the number of hash tables required [REF6]. It also reduces the space requirement compared to the entropy-based LSH method, while achieving the same search quality [REF6]. The multi-probe LSH method focuses on improving the space and time efficiency of LSH for high-dimensional similarity search [REF6]. It has been compared to other LSH methods, but a detailed comparison with other indexing techniques is beyond the scope of this work [REF6].<br/><br/>In conclusion, retrieval architectures and vector search techniques play a crucial role in enhancing the efficiency and effectiveness of information retrieval systems. Locality sensitive hashing approaches, such as the multi-probe LSH method and query-directed probing sequence, offer promising solutions for efficient vector search. These techniques reduce the number of hash tables and probes required, while achieving desired search accuracy and query time. Further research and experimentation are needed to explore the full potential of these approaches in the context of neural information retrieval.<br/><br/>[REF0] −Cx i (δ i ) 2 = e −C P i x i ((δ i ) 2 )<br/>[REF1] Multi-probe LSH method [REF1]<br/>[REF2] Query-directed probing sequence [REF2]<br/>[REF3] Step-wise probing method [REF3]<br/>[REF5] Approximation for success probabilities [REF5]<br/>[REF6] Comparison of multi-probe LSH with other methods [REF6]</td>
      <td>Locality sensitive hashing (LSH) BIBREF64 is based on the simple idea that, if two embeddings are close together, then after a “projection”, using an hash function, these two embeddings will remain close together. This requires that: 1. for any two embeddings INLINEFORM0 and INLINEFORM1 that are close to each other, there is a high probability INLINEFORM2 that they fall into the same hash bucket;  2. for any two embeddings INLINEFORM0 and INLINEFORM1 that are far apart, there is a low probability INLINEFORM2 that they fall into the same hash bucket.  The actual problem to solve is to design a family of LSH functions fulfilling these requirements. LSH functions have been designed for many distance metrics. For the euclidean distance, a popular LSH function INLINEFORM0 is the random projection BIBREF65 .A set of random projections defines a family of hash functions INLINEFORM1 that can be used to build a data structure for ANN search. First, we sample INLINEFORM2 hash functions INLINEFORM3 independently and uniformly at random from INLINEFORM4 , and we define the function family INLINEFORM5 , where INLINEFORM6 , i.e., INLINEFORM7 is the concatenation of INLINEFORM8 hash functions from INLINEFORM9 . Then, we sample INLINEFORM10 functions INLINEFORM11 independently and uniformly at random from INLINEFORM12 , and each function INLINEFORM13 is used to build a hash table INLINEFORM14 . Given the set of document embeddings INLINEFORM0 and selected the values of the parameters INLINEFORM1 and INLINEFORM2 , an LSH index is composed of INLINEFORM3 hash tables, each containing INLINEFORM4 concatenated random projections. For each INLINEFORM5 , INLINEFORM6 is inserted in the INLINEFORM7 bucket for each hash table INLINEFORM8 , for INLINEFORM9 . At query processing time, given a query embedding, we first generate a candidate set of document embeddings by taking the union of the contents of all INLINEFORM10 buckets in the INLINEFORM11 hash tables the query is hashed to. The final NN document embedding is computed performing an exhaustive exact search within the candidate set. The main drawback of the LSH index is that it may require a large number of hash tables to cover most nearest neighbors, and it requires to store the original embeddings to perform the exhaustive exact search. Although some optimisations have been proposed BIBREF66 , the space consumption may be prohibitive with very large data sets.</td>
    </tr>
    <tr>
      <th>14</th>
      <td>Retrieval Architectures and Vector Search</td>
      <td>Vector quantisation approaches</td>
      <td>Retrieval Architectures and Vector Search - Vector quantisation approaches<br/><br/>Vector quantisation is a widely used technique in retrieval architectures for efficient information retrieval. It involves the process of encoding vectors into a set of codewords or centroids, which are then used for indexing and searching purposes. In this section, we will explore the concept of vector quantisation and its applications in retrieval architectures, specifically focusing on the use of vector quantisation for efficient search and retrieval [REF8].<br/><br/>To obtain precise distances in vector quantisation, it is crucial to limit the quantisation error. This can be achieved by having a sufficiently large number of centroids or codewords [REF0]. However, this poses several challenges. Firstly, a large number of samples are required to learn the quantiser, which can be computationally expensive [REF0]. Secondly, the complexity of the algorithm itself can be prohibitive [REF0]. Lastly, the memory required to store the floating-point values representing the centroids can be a limiting factor [REF0]. <br/><br/>One approach to address these challenges is the hierarchical k-means (HKM) algorithm, which improves the efficiency of the learning stage and the corresponding assignment procedure [REF0]. However, the aforementioned limitations still apply, particularly in terms of memory usage and the size of the learning set [REF0].<br/><br/>Another important aspect of vector quantisation is the use of pre-processing techniques to enhance the efficiency of quantisation. These techniques aim to remove redundancy in the signal, reduce signal variance, or concentrate signal energy, resulting in better performance for a given bit rate and complexity [REF1]. For example, predictive quantisation involves removing linear predictions based on past reconstructed values from the signal, and quantising the resulting prediction residual [REF1]. In addition, transforming vectors or blocks of input symbols using linear and orthogonal transforms can also improve quantisation efficiency [REF1].<br/><br/>Efficient computation of distances is crucial in retrieval architectures. The use of the Hamming space allows for efficient distance computation, particularly through table lookups [REF2]. In large datasets, an exhaustive comparison of the query vector with all codes is impractical. To address this, a modified inverted file structure can be used to rapidly access the most relevant vectors [REF2]. This structure utilizes a coarse quantiser and short codes to represent vectors in the associated list, computed using a product quantiser [REF2].<br/><br/>Vector quantisation offers desirable structural properties, such as nested tree-structured codes that are optimized for progressive transmission applications [REF3]. These codes allow for progressive improvement of a signal as more bits arrive. Additionally, clustering algorithms used to design vector quantisers can enhance specific features of the original signal, such as identifying small tumors in medical images [REF3]. While scalar techniques remain dominant in many applications, vector extensions are finding increasing use in signal compression and other signal processing applications [REF3].<br/><br/>In the context of information retrieval, vector quantisation has been applied to various domains. For instance, in image retrieval, global descriptors such as GIST descriptors can be mapped to short binary codes using vector quantisation techniques [REF4]. This mapping allows for efficient search of Euclidean nearest neighbors by approximating the search using Hamming distances between codes [REF4]. Similarly, binary signatures can be used to refine quantised descriptors in bag-of-features image search frameworks [REF4].<br/><br/>In conclusion, vector quantisation plays a crucial role in retrieval architectures, enabling efficient search and retrieval of information. Despite the challenges associated with quantisation error, computational complexity, and memory usage, techniques such as hierarchical k-means and pre-processing methods have been developed to address these limitations. The use of the Hamming space and inverted file structures further enhance the efficiency of distance computation and access to relevant vectors. Vector quantisation offers desirable structural properties and has found applications in various domains, including image retrieval.</td>
      <td>Instead of random partitioning the input space INLINEFORM0 as in LSH, the input space can be partitioned according to the data distribution. By using the INLINEFORM1 -means clustering algorithm on INLINEFORM2 we can compute INLINEFORM3 centroids INLINEFORM4 , with INLINEFORM5 for INLINEFORM6 that can be used to partition the input space INLINEFORM7 . The set INLINEFORM8 is called a codebook . Given a codebook INLINEFORM9 , a vector quantiser INLINEFORM10 maps a vector INLINEFORM11 to its closest centroid:  DISPLAYFORM0  Given a codebook INLINEFORM0 , an IVF (Inverted File) index built over INLINEFORM1 and INLINEFORM2 stores the set of document embeddings INLINEFORM3 in INLINEFORM4 partitions or inverted lists INLINEFORM5 , where INLINEFORM6 . At query processing time, we specify to search for the NN document embeddings in INLINEFORM7 partitions. If INLINEFORM8 , the search is exhaustive, but if INLINEFORM9 , the search is carried out in the partitions whose centroid is closer to the query embedding. In doing so the search is not guaranteed to be exact, but the search time can be sensibly reduced. In fact, an IVF index does not improve the space consumption, since it still needs to store all document embeddings, but it can reduce the search time depending on the number of partitions processed for each query. A major limitation of IVF indexes is that they can require a large number of centroids BIBREF67 . To address this limitation, product quantisation BIBREF68 divides each vector INLINEFORM0 into INLINEFORM1 sub-vectors INLINEFORM2 . Each sub-vector INLINEFORM3 with INLINEFORM4 is quantised independently using its own sub-vector quantiser INLINEFORM5 . Each vector quantiser INLINEFORM6 has its own codebook INLINEFORM7 . Given the codebooks INLINEFORM8 , a product quantiser INLINEFORM9 maps a vector INLINEFORM10 into the concatenation of the centroids of its sub-vector quantisers:  DISPLAYFORM0  Note that a product quantiser can output any of the INLINEFORM0 centroid combinations in INLINEFORM1 . A PQ (Product Quantization) index stores, for each embedding INLINEFORM0 , its encoding INLINEFORM1 , that requires INLINEFORM2 bits of storage. At query processing time, the document embeddings are processed exhaustively. However the distance computation between a query embedding INLINEFORM3 and a document embedding INLINEFORM4 is carried out using the product quantisation of the document embedding INLINEFORM5 :  DISPLAYFORM0  To implement this computation, INLINEFORM0 lookup tables are computed, one per sub-vector quantiser: the INLINEFORM1 -th table is composed of the squared distances between the INLINEFORM2 -th sub-vector of INLINEFORM3 , and the centroids of INLINEFORM4 . These tables can be used to quickly compute the sums in Eq. ( EQREF59 ) for each document embedding. ANN search on a PQ index is fast, requiring only INLINEFORM0 additions, and memory efficient, but it is still exhaustive. To avoid it, an IVFPQ index exploits inverted files and product quantisation jointly. Firstly, a coarse quantiser partitions the input dataset into inverted lists, for a rapid access to small portions of the input data. In a given inverted list, the difference between each input data and the list centroid, i.e., the input residual , is encoded with a product quantiser. In doing so, the exhaustive ANN search can be carried out only in a limited number of the partitions computed by the coarse quantiser.</td>
    </tr>
    <tr>
      <th>15</th>
      <td>Retrieval Architectures and Vector Search</td>
      <td>Graph approaches</td>
      <td>Retrieval Architectures and Vector Search - Graph approaches<br/><br/>Graph-based approaches have gained significant attention in the field of neural information retrieval due to their ability to capture complex relationships and dependencies among data points. These approaches leverage the concept of navigable small-world networks (NSW) to construct efficient retrieval architectures [REF8]. The NSW algorithm explores the neighborhood of the closest elements in a greedy manner, continuously improving the known k closest elements [REF0]. By iteratively shrinking the search radius, the algorithm can achieve remarkable performance on various datasets [REF2].<br/><br/>Kleinberg's navigability criterion, which utilizes regular lattice graphs augmented with long-range links, has inspired the development of many K-Nearest Neighbor Search (K-NNS) and Approximate K-Nearest Neighbor Search (K-ANNS) algorithms [REF1]. However, these approaches suffer from polylogarithmic complexity scalability and require prior knowledge of the data distribution, limiting their applicability [REF1] [REF4]. In contrast, the NSW algorithm offers a decentralized graph construction mechanism that is suitable for data in arbitrary spaces [REF4]. It has been suggested that the NSW model may even explain the navigability of large-scale biological neural networks [REF4].<br/><br/>The NSW algorithm demonstrates several desirable properties for neural information retrieval. It converges to high recall rates with each point comparing only to a small fraction of the dataset on average [REF5]. Moreover, the algorithm is easy to implement and requires minimal extra data structures [REF5]. The performance advantage of the NSW algorithm can be further enhanced by optimizing the overlap between neighbors on different layers of the graph [REF6]. Simulations have shown that the proposed selection of parameters leads to significant speedup and improved performance [REF6].<br/><br/>Empirical evaluations of the NSW algorithm have demonstrated its efficiency and scalability. The algorithm exhibits polynomial time complexity, regardless of the dataset size [REF3]. The scan rate curves of different datasets form parallel straight lines, indicating consistent performance across various datasets [REF3]. Comparative studies have shown that the NSW algorithm outperforms existing methods such as Recursive Lanczos Partitioning and Locality Sensitive Hashing in terms of both speed and accuracy [REF5].<br/><br/>In conclusion, graph-based approaches, particularly the NSW algorithm, offer promising solutions for neural information retrieval. These approaches leverage the concept of navigable small-world networks to construct efficient retrieval architectures. By exploring the neighborhood of closest elements in a greedy manner, the NSW algorithm achieves high recall rates with minimal computational overhead. Furthermore, the algorithm is easy to implement and exhibits consistent performance across different datasets.</td>
      <td>The distances between vectors in a dataset can be efficiently stored in a graph-based data structure called kkNN graph . In a INLINEFORM0 NN graph INLINEFORM1 , each input data INLINEFORM2 is represented as a node INLINEFORM3 , and, for its INLINEFORM4 nearest neighbours, a corresponding edge is added in INLINEFORM5 . The computation in an exact INLINEFORM6 NN graph requires INLINEFORM7 similarity computation, but many approximate variants are available BIBREF69 . To search for an approximate nearest neighbour to an element INLINEFORM8 using a INLINEFORM9 NN graph, a greedy heuristic search is used. Starting from a predefined entry node, the graph is visited one node at a time, keeping on finding the closest node to INLINEFORM10 among the unvisited neighbour nodes. The search terminates when there is no improvement in the current NN candidate. In practice, several entry nodes are used together with a search budget to avoid local optima. For a large number of nodes, the greedy heuristic search on the INLINEFORM11 NN graph becomes inefficient, due to the long paths potentially required to connect two nodes. Instead of storing only short-range edges, i.e., edges connecting two close nodes, the INLINEFORM12 NN graph can be enriched with randomly generated long-range edges, i.e., edges connecting two randomly-selected nodes. This kind of INLINEFORM13 NN graph is a navigable small world (NSW) graph BIBREF70 , for which the greedy search heuristic is theoretically and empirically efficient BIBREF71 . A hierarchical NSW (HNSW) index stores the input data into multiple NSW graphs. The bottom layer graph contains a node for each input element, while the number of nodes in the other graphs decreases exponentially at each layer. The search procedure for approximate NN vectors starts with the top layer graph. At each layer, the greedy heuristic searches for the closest node, then the next layer is searched, starting from the node corresponding to the closest node identified in the preceding graph. At the bottom layer, the greedy heuristic searches for the INLINEFORM0 closest nodes to be returned BIBREF72 .</td>
    </tr>
    <tr>
      <th>16</th>
      <td>Retrieval Architectures and Vector Search</td>
      <td>Optimisations</td>
      <td>Retrieval Architectures and Vector Search - Optimisations<br/><br/>In the field of neural information retrieval, retrieval architectures and vector search play a crucial role in optimizing the efficiency and effectiveness of information retrieval systems. Various techniques and approaches have been proposed to enhance the retrieval process and improve the accuracy of search results. This section discusses some of the key architectures and optimizations used in retrieval systems.<br/><br/>One approach that has been explored is the use of decomposition techniques to optimize the computation of inverted lists [REF0]. By decomposing the computation of tables used during the scan of the inverted list, the computational cost can be reduced. This decomposition technique enables more efficient retrieval by reducing the number of multiply-add operations required. However, it should be noted that this decomposition may not be suitable for all scenarios, especially when memory usage is a concern.<br/><br/>Another important aspect of retrieval architectures is the use of joint learning structures. Two types of joint learning structures have been identified: tree-based approaches and product quantization (PQ) based approaches [REF1]. Tree-based approaches require special approximate training techniques, which can complicate their adoption in real-world industrial retrieval systems. On the other hand, PQ-based approaches have been designed for smaller computer vision tasks and may not be applicable to large-scale information retrieval tasks. To address this limitation, advancements have been made in product quantization based embedding indexes jointly trained with deep retrieval models.<br/><br/>In the context of dense retrieval (DR), the storage and retrieval of token-level representations have been a challenge due to the large index size [REF2]. Previous DR models have relied on brute-force search for candidate retrieval, which can be computationally expensive. However, recent studies have proposed more efficient architectures for DR models, enabling faster retrieval of candidates.<br/><br/>Dense retrieval methods often retrieve a significant number of new documents compared to sparse retrievals [REF3]. This indicates that DR methods have low overlap with traditional retrieval methods, such as BM25. The high hole rate and low recall metric suggest that DR methods may have advantages in certain retrieval tasks, such as the TREC 2020 Deep Learning Track.<br/><br/>Optimizations in vector search also play a crucial role in improving retrieval efficiency. For instance, in end-to-end ranking, the choice of similarity function can impact the performance of the retrieval system [REF4]. Different distance metrics, such as cosine similarity and L2 distance, have been used in vector search. Additionally, the configuration of the index, including the number of partitions and the number of nearest neighbors to retrieve, can also affect the retrieval performance.<br/><br/>Training strategies and techniques are also important considerations in retrieval architectures. For example, the training of the DPR model involves in-batch negative sampling and the combination of training data from multiple datasets [REF5]. The use of gradient straight-through estimators and appropriate techniques for non-differentiable operations, such as arg min, are crucial for training models with quantization steps.<br/><br/>Implementation details and hyperparameter settings are also critical in retrieval architectures. For instance, the training of the BERT-Siamese model involves the use of RoBERTa base and specific optimization techniques, such as the LAMB optimizer [REF6]. These implementation details can significantly impact the convergence and performance of retrieval models.<br/><br/>In conclusion, retrieval architectures and vector search optimizations are essential components of neural information retrieval systems. Techniques such as decomposition, joint learning structures, and efficient indexing strategies contribute to the improvement of retrieval efficiency and accuracy. Additionally, training strategies, implementation details, and hyperparameter settings play a crucial role in the performance of retrieval models.</td>
      <td>Implementations of the embedding indexes presented in the previou sections are available in many open-source production-ready search engines such as Lucene FOOTREF62 and Vespa FOOTREF63 . In the IR research community, FAISS is the most widely adopted framework for embedding indexes BIBREF73 . Among others, FAISS includes implementations of flat, LSH, IVF, PQ, IVFPQ and HNSW indexes. Single representation systems such as INLINEFORM0 BIBREF50 , INLINEFORM1 BIBREF51 , and INLINEFORM2 BIBREF52 use flat indexes. In these cases, it is unfeasible to adopt product quantisation indexes due to their negative impact on IR-specific metrics, mainly caused by the separation between the document encoding and embedding compression phases. To overcome this limitation, several recent techniques such as Poemm BIBREF74 , JPQ BIBREF75 and RepCONC BIBREF76 aim to train at the same time both phases. In doing so, during training, these techniques learn together the query and document encoders together while performing product quantisation. Multiple representation systems such as INLINEFORM0 BIBREF55 are characterised by a very large number of document embeddings. They do not use flat indexes, due to unacceptable efficiency degradation of brute-force search, and exploit IVFPQ indexes and ANN search. With these indexes, the document embeddings are stored in a quantised form, suitable for fast searching. However, the approximate similarity scores between these compressed embeddings are inaccurate, and hence are not used for computing the final top documents. Indeed, in a first stage, ANN search computes, for each query embedding, the set of the INLINEFORM1 most similar document embeddings; the retrieved document embeddings for each query embedding are mapped back to their documents. These documents are exploited to compute the final list of top INLINEFORM2 documents in a second stage. To this end, the set of documents computed in the first stage is re-ranked using the query embeddings and the documents' multiple embeddings to produce exact scores that determine the final ranking, according to the relevance function in Eq. ( EQREF36 ) (see Figure FIGREF64 . Further optimisations can reduce the number of query embeddings to be processed in the first stage BIBREF77 , or the number of documents to be processed in the second stage BIBREF78 .</td>
    </tr>
    <tr>
      <th>17</th>
      <td>Learned Sparse Retrieval</td>
      <td>Document expansion learning</td>
      <td>Learned Sparse Retrieval - Document expansion learning<br/><br/>Document expansion learning is a crucial aspect of learned sparse retrieval, aiming to improve the effectiveness of queries by expanding them with additional terms. Traditional approaches to query expansion have primarily focused on unstructured bag-of-words queries, neglecting the potential benefits of structured queries [REF2]. However, research has shown that structured queries, such as Boolean conjunctive normal form (CNF) queries, can be more effective in certain domains and provide better control over expansion [REF2] [REF3].<br/><br/>One key challenge in document expansion learning is predicting the term mismatch probability, which refers to the likelihood of a query term not appearing in relevant documents [REF4]. Prior research has demonstrated that accurately estimating this probability can significantly enhance retrieval accuracy [REF0]. To address this challenge, several methods have been proposed to predict the term recall probability (P(t|R)), which is closely related to term mismatch [REF1]. These methods leverage various features and techniques to estimate P(t|R) and incorporate it into retrieval models [REF1].<br/><br/>The use of predicted term mismatch probabilities as term weights has been explored to improve traditional retrieval models, such as Okapi BM25 and language models [REF1]. By incorporating these predicted probabilities, retrieval models can better capture the relevance between query terms and documents, leading to improved retrieval performance [REF1]. Experimental results have shown that using true or predicted term recall values can outperform standard baselines in ad hoc retrieval tasks [REF1].<br/><br/>Furthermore, document expansion learning can be guided by the diagnosis of term mismatch probabilities [REF4]. This diagnosis can be used to suggest manual query reformulation, guide interactive query expansion, or motivate other responses [REF4]. For instance, the diagnosis can guide the creation of Boolean CNF structured queries that selectively expand problematic query terms while keeping the rest of the query unchanged [REF4]. Experiments conducted on TREC Ad-hoc and Legal Track datasets have demonstrated the effectiveness of this diagnostic approach, reducing user effort and producing simple yet effective structured queries that outperform their bag-of-words counterparts [REF4] [REF9].<br/><br/>In summary, document expansion learning plays a vital role in learned sparse retrieval by leveraging predicted term mismatch probabilities and structured queries. By effectively expanding queries with additional terms, retrieval models can better capture the relevance between queries and documents, leading to improved retrieval performance. The use of structured queries, such as Boolean CNF queries, and the diagnosis of term mismatch probabilities further enhance the effectiveness of document expansion learning.</td>
      <td>Document expansion techniques address the vocabulary mismatch problem BIBREF79 : queries can use terms semantically similar but lexically different from those used in the relevant documents. Traditionally, this problem has been addressed using query expansion techniques, such as relevance feedback BIBREF80 and pseudo relevance feedback BIBREF81 . The advances in neural networks and natural language processing have paved the way to different techniques to address the vocabulary mismatch problem by expanding the documents by learning new terms.  INLINEFORM0 BIBREF82 and INLINEFORM1 BIBREF83 showed for the first time that transformer architectures can be used to expand the documents' content to include new terms or to boost the statistics of existing termw. Both approaches focus on the same task, that is, generating new queries for which a specific document will be relevant. Given a dataset of query and relevant document pairs, INLINEFORM2 fine-tunes a sequence-to-sequence transformer model BIBREF84 , while INLINEFORM3 fine-tunes the INLINEFORM4 model BIBREF36 by taking as input the relevant document and generating the corresponding query. Then, the fine-tuned model is used to predict new queries using top INLINEFORM5 random sampling BIBREF85 to enrich the document by appending these queries before indexing, as illustrated in Figure FIGREF69 . Instead of leveraging the encoder-decoder models for sentence generation and fine-tune them on document expansion, a different approach computes the importance of all terms in the vocabulary w.r.t. a given document and selects the most important new terms to enrich the document, leveraging an encoder-only architecture to compute the document embeddings. INLINEFORM0 BIBREF86 exploits the INLINEFORM1 model to compute the [CLS] output embedding of a document, and linearly projects it over the whole INLINEFORM2 vocabulary. In doing so, INLINEFORM3 computes a probability distribution over the vocabulary, i.e., a document language model, and then adds to the document a certain number of new terms, corresponding to those with the highest probabilities. As another way of expanding documents, INLINEFORM4 BIBREF87 computes a document language model for each INLINEFORM5 output token, including [CLS] , and sums them up to compute the term importance distribution over the vocabulary for the given document. Finally, a learned gating mechanism only keeps a sparse subset of those, to compute the final expanded document contents.</td>
    </tr>
    <tr>
      <th>18</th>
      <td>Learned Sparse Retrieval</td>
      <td>Impact score learning</td>
      <td>Learned Sparse Retrieval - Impact score learning<br/><br/>Impact score learning is a crucial aspect of learned sparse retrieval, which aims to assign weights to terms in order to capture their importance in the retrieval process. Several approaches have been proposed in the literature to learn impact scores and improve the effectiveness and efficiency of retrieval models.<br/><br/>One approach is the use of the MaxScore query processing algorithm, which allows for "sum of impact" scoring [REF0]. This algorithm, combined with the common index file format and the BM25 scoring method, has been shown to achieve effective retrieval results [REF0]. Another technique, called TILDEv2, integrates the best-of-breed from recent advances in neural retrieval and employs a novel use of the original TILDE as an efficient passage expansion technique [REF1] [REF2]. TILDEv2 significantly improves the effectiveness of the original TILDE while reducing its index size [REF2].<br/><br/>uniCOIL is another model that utilizes learned impact weights for sparse retrieval [REF3]. It achieves good effectiveness by adding expansion to make up for the lost expressivity of weight vectors [REF3]. Furthermore, uniCOIL represents the state of the art in sparse retrieval using learned impact weights, outperforming DeepImpact [REF3]. These findings highlight the potential of impact score learning in improving retrieval effectiveness.<br/><br/>In the context of evaluation, the effectiveness of learned sparse retrieval models is typically measured using metrics such as MRR@10, nDCG@10, and MAP [REF4]. Statistical significance tests, such as paired two-tailed t-tests with Bonferroni correction, are commonly employed to compare the performance of different methods [REF4]. Additionally, query latency is evaluated within both CPU and GPU environments to assess the efficiency of the models [REF4].<br/><br/>Both dense and sparse learned representations leverage transformers, but sparse approaches project the learned knowledge back into the sparse vocabulary space, enabling the utilization of existing techniques for efficient query evaluation [REF5]. The tradeoffs between output quality, query latency, and index size are important considerations in the design space of modern information retrieval techniques [REF5]. While learned representations for information retrieval show promise, the advantages and disadvantages of dense versus sparse approaches are still being explored [REF5].<br/><br/>In summary, impact score learning plays a crucial role in learned sparse retrieval. Various techniques, such as the MaxScore query processing algorithm, TILDEv2, and uniCOIL, have been proposed to improve retrieval effectiveness and efficiency. Evaluation metrics and statistical tests are used to assess the performance of these models. The tradeoffs between output quality, query latency, and index size are important factors to consider in the design of information retrieval techniques [REF8].</td>
      <td>Classical inverted indexes store statistical information on term occurrences in documents in posting lists, one per term in the collection. Every posting list stores a posting for each document in which the corresponding term appears in, and the posting contains a document identifier and the in-document term frequency, i.e., a positive integer counting the number of occurrences of the term in the document. When a new query arrives, the posting lists of the terms in the query are processed to compute the top scoring documents, using a classical ranking function, such as BM25, and efficient query processing algorithms BIBREF3 . The goal of impact score learning is to leverage the document embeddings generated by an encoder-only model to compute a single integer value to be stored in postings, and to be used as a proxy of the relevance of the term in the corresponding posting, i.e., its term importance . The simplest way to compute term importance in a document is to project the document embeddings of each term with a neural network into a single-value representation, filtering out negative values with INLINEFORM0 functions and discarding zeros. To save space, the real values can be further quantised into a 8-bit positive integers. A common problem in impact score learning is the vocabulary to use. Since most encoder-only models use a sub-word tokeniser, the collection vocabulary can be constructed in two different ways: by using the terms produced by the encoder-specific sub-word tokeniser, e.g., by INLINEFORM1 -like tokenisers, or by using the terms produced by a word tokeniser. These two alternatives have an impact on the final inverted index: in the former case, we have fewer terms, but longer and denser posting lists, while in the latter case, we have more terms, with shorter posting lists and with smaller query processing times BIBREF88 . The current impact score learning systems are INLINEFORM0 BIBREF89 , INLINEFORM1 BIBREF91 , INLINEFORM2 BIBREF92 , and INLINEFORM3 BIBREF93 .  INLINEFORM0 BIBREF89 represents the first example of term importance boosting. INLINEFORM1 exploits the contextualised word representations from INLINEFORM2 to learn new in-document term frequencies, to be used with classical ranking functions such as BM25. For each term INLINEFORM3 in a given document, INLINEFORM4 estimates its context-specific importance INLINEFORM5 , that is then scaled and rounded as frequency-like integer value INLINEFORM6 that can be stored in an inverted index. Formally, for each document INLINEFORM7 , INLINEFORM8 projects the INLINEFORM9 -dimensional representations INLINEFORM10 for each input INLINEFORM11 token INLINEFORM12 in the document, with INLINEFORM13 , into a scalar term importance with the learned matrix INLINEFORM14 :  DISPLAYFORM0   INLINEFORM0 is trained with a per-token regression task, trying to predict the importance of the terms. The actual term importance to predict is derived from the document containing the term, or from a training set of query, relevant document pairs. A term appearing in multiple relevant documents and in different queries has a higher importance than a term matching fewer documents, and/or fewer queries. To handle INLINEFORM1 's sub-word tokens, INLINEFORM2 uses the importance of the first sub-word token for the entire word, and when a term occurs multiple times in the document, it takes the maximum importance across the multiple occurrences.  INLINEFORM0 BIBREF91 proposes for the first time to directly compute an impact score for each unique term in a document, without resorting to classical ranking functions, but simply summing up, at query processing time, the impacts of the query terms appearing in a document to compute its relevance score. For each term INLINEFORM1 in a given document INLINEFORM2 , INLINEFORM3 estimates its context-specific impact INLINEFORM4 . INLINEFORM5 feeds the encoder-only model with the document sub-word tokens, producing an embedding for each input token. A non-learned gating layer Mask removes the embeddings of the sub-word tokens that do not correspond to the first sub-token of the whole word. Then INLINEFORM6 transforms the remaining INLINEFORM7 -dimensional representations with two feed forward networks with INLINEFORM8 activations. The first network has a weight matrix INLINEFORM9 , and the second network has a weight matrix INLINEFORM10 :  DISPLAYFORM0  The output real numbers INLINEFORM0 , with INLINEFORM1 , one per whole word in the input document, are then linearly quantised into 8-bit integers that can be stored in an inverted index. This produces a single-value score for each unique term in the document, representing its impact. Given a query INLINEFORM2 , the score of the document INLINEFORM3 is simply the sum of impacts for the intersection of terms in INLINEFORM4 and INLINEFORM5 . INLINEFORM6 is trained with query, relevant document, non-relevant document triples, and, for each triple, two scores for the corresponding two documents are computed. The model is optimized via pairwise cross-entropy loss over the document scores. Moreover, INLINEFORM7 has been the first sparse learned model leveraging at the same time documents expansion learning and impact score learning. In fact, INLINEFORM8 leverages INLINEFORM9 to enrich the document collection before learning the term impact.  INLINEFORM0 BIBREF86 computes the terms' impact with an approach similar to INLINEFORM1 . The main differences are (i) the use of a single layer feed forward network with INLINEFORM2 activations, instead of a two-layer network, to project the document embeddings into a single positive scalar value using a learned matrix INLINEFORM3 , (ii) the use of its own document expansion technique, as discussed in Section SECREFU68 , (iii) the use of an index with sub-word terms instead of whole word terms, and (iv) the selection of the highest-valued impact score for a token if that token appears multiple times in a document:  DISPLAYFORM0  The INLINEFORM0 scores are then summed up, obtaining an accumulated query-document score.  INLINEFORM0 BIBREF93 exploits the INLINEFORM1 approach (see Sec. SECREF4 ), but instead of projecting the query and document embeddings on 8-32 dimensions, it projects them to single-dimension query weights and document weights. In INLINEFORM2 the query and document [CLS] embeddings are not used, and the embeddings corresponding to normal query and document tokens are projected into single scalar values INLINEFORM3 using a learned matrix INLINEFORM4 , with INLINEFORM5 activations on the output term weights of the base INLINEFORM6 model to force the model to generate non-negative weights.  DISPLAYFORM0  The document weights INLINEFORM0 are then linearly quantised into 8-bit integers, and the final query-document score is computed by summing up the highest valued document impact scores times its query weight INLINEFORM1 , computed at query processing time, as in Eq. ( EQREF74 ).</td>
    </tr>
    <tr>
      <th>19</th>
      <td>Learned Sparse Retrieval</td>
      <td>Sparse representation learning</td>
      <td>Learned Sparse Retrieval - Sparse representation learning<br/><br/>Sparse representation learning is a fundamental aspect of neural information retrieval systems. It involves the development of models and techniques that enable efficient and effective retrieval of relevant information from large collections of documents. In this section, we discuss the concept of learned sparse retrieval and its significance in the field of information retrieval.<br/><br/>One approach to learned sparse retrieval is the use of sparse representation models such as BOW (Bag-of-Words) and SPLADE (Sparse Lexical Adaptive Document Embeddings) [REF0]. These models aim to represent documents and queries in a sparse manner, where only a subset of the features or terms are considered relevant for retrieval. This sparsity enables efficient computation and storage, making it suitable for large-scale retrieval tasks.<br/><br/>EPIC (Expansion via Prediction of Importance with Contextualization) is another approach that combines term importance modeling and expansion using contextualized language models [REF1]. EPIC builds query and document representations by incorporating salience and expansion techniques. This approach has shown promising results in narrowing the effectiveness gap between practical and computationally expensive retrieval methods.<br/><br/>The effectiveness of learned sparse retrieval models is not solely dependent on their efficiency but also on their interpretability. Sparse models, such as SPLADE, provide interpretable representations where the dimensions directly correspond to the terms in the lexicon [REF1]. This interpretability allows for better understanding and analysis of the retrieval process.<br/><br/>To improve the efficiency of sparse representation learning, regularization techniques are often employed. Regularization helps in reducing the computational cost by promoting sparsity in the learned representations. For example, FLOPS regularization has been shown to be advantageous over ℓ1 regularization in terms of decreasing computing cost [REF4]. Additionally, regularization schedules, such as gradually increasing the regularization weight, have been proposed to prevent the model from getting stuck in local minima during training [REF6].<br/><br/>The trade-off between effectiveness and efficiency is a crucial consideration in learned sparse retrieval. Models like SPLADE have demonstrated efficiency levels equivalent to sparse BOW models while outperforming other dense retrieval models [REF0]. This trade-off is often visualized by varying the efficiency (FLOPS) and observing the corresponding effectiveness (MRR@10) [REF4].<br/><br/>In conclusion, learned sparse retrieval techniques play a vital role in neural information retrieval systems. These techniques enable efficient and effective retrieval of relevant information from large document collections. Sparse representation models, such as BOW and SPLADE, along with regularization and interpretability considerations, contribute to the advancement of learned sparse retrieval in the field of information retrieval.</td>
      <td>Instead of independently learning to expand the documents and then learning the impact score of the terms in the expanded documents, sparse representation learning aims at learning both at the same time. At its core, sparse representation learning projects the output embeddings of an encoder-only model into the input vocabulary, compute, for each input term in the document, a language model, i.e., a probability distribution over the whole vocabulary. These term-based language models capture the semantic correlations between the input term and all other terms in the collection, and they can be used to (i) expand the input text with highly correlated terms, and (ii) compress the input text by removing terms with low probabilities w.r.t. the other terms. Encoder-only models such as INLINEFORM0 already compute term-based language models, as part of their training as masked language models. Formally, given a document INLINEFORM1 , together with the output embeddings INLINEFORM2 , an encoder-only model also returns the masked language heads INLINEFORM3 , one for each token in the document, where INLINEFORM4 for INLINEFORM5 is an estimation of the importance of each word in the vocabulary implied by the INLINEFORM6 -th token in the document INLINEFORM7 . INLINEFORM8 BIBREF94 and INLINEFORM9 BIBREF87 have been the first systems focusing on vocabulary-based expansion and importance estimation, and inspired the INLINEFORM10 BIBREF95 system, on which we focus. For a given document INLINEFORM0 , INLINEFORM1 computes its per-token masked language heads INLINEFORM2 using INLINEFORM3 , filters and sums up these vocabulary-sized vectors into a single vector INLINEFORM4 representing the whole document, and then uses this vector to represent the document itself, together with the term importance scores:  DISPLAYFORM0  The logarithm and INLINEFORM0 functions in Eq. ( EQREF76 ) are computed element-wise; the logarithm prevents some terms with large values from dominating, and the INLINEFORM1 function deals with the negative components of INLINEFORM2 . The document representation INLINEFORM0 potentially contains all terms in the vocabulary, even if the logarithm and INLINEFORM1 functions in Eq. ( EQREF76 ) can zero out some of its components. To learn to “sparsify” the document representations, BIBREF95 leverage the INLINEFORM2 regulariser INLINEFORM3 BIBREF96 . As part of the INLINEFORM4 loss function used during training, the FLOPS loss is computed as the sum, across the terms in the vocabulary, of the squared probability INLINEFORM5 that a term INLINEFORM6 has a non-zero weight in a document. Minimising the INLINEFORM7 loss coincides with minimising the non-zero weights in a document, i.e., maximising the number of zero weights in a document. The square operation helps in reducing high term weights more than low term weights. The probability that a term INLINEFORM8 has a non-zero weight in a document INLINEFORM9 is proportional to the average weight of that term INLINEFORM10 estimated through the whole collection. To make the computation feasible, the average is computed on a batch INLINEFORM11 of documents during training, considered as a representative sample of the whole collection:  DISPLAYFORM0   INLINEFORM0 does not limit expansion to documents only. Indeed, Eq. ( EQREF76 ) can be applied to a query INLINEFORM1 as well, to compute the corresponding vector INLINEFORM2 . However, this query expansion must be carried out at query processing time; to reduce the latency, the expanded query should be far more sparse than a document. To enforce this different behaviour, BIBREF95 adopt two distinct INLINEFORM3 regularisers for documents and queries, both as in Eq. EQREF77 .</td>
    </tr>
  </tbody>
</table>