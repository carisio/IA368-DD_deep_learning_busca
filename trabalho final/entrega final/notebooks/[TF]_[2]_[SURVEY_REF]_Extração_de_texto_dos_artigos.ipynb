{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Cria manualmente as estruturas de seção do survey de referência para servir de entrada para a próxima etapa do pipeline. A estrutura tem o seguinte tipo:\n",
        "\n",
        "    sections = [\n",
        "      {\n",
        "        \"sections_title\": TITULO,\n",
        "        \"papers\": {\"paperId\": ID, \"abstract\": ABSTRACT, \"title\": TITULO}[],\n",
        "        \"subsections\": {ESTURUTRA DE SEÇÕES}[],\n",
        "      }\n",
        "    ]\n",
        "    \n",
        "    papers = {\n",
        "      \"paperId\": {\n",
        "        \"metadata\": {\n",
        "          \"paperId\": ID,\n",
        "          \"abstract\": ABSTRACT,\n",
        "          \"title\": TITLE\n",
        "        },\n",
        "        \"title\": TÍTULO,\n",
        "        \"abstract\": ABSTRACT,\n",
        "        \"text\": TEXTO\n",
        "      }\n",
        "    }"
      ],
      "metadata": {
        "id": "QQGIPeLofIxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_papers_zip = '/content/drive/MyDrive/IA368-DD_deep_learning_busca/papers_neural_information_retrieval.zip'\n",
        "folder_papers = './papers_pdf/'\n",
        "\n",
        "# Output file with the structure of the sections\n",
        "file_sections_structure = 'sections-survey-ref.pkl'\n",
        "# Output file with the contents of each paper\n",
        "file_papers_contents = 'papers_contents-survey-ref.pkl'"
      ],
      "metadata": {
        "id": "CanHtgtVUlXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNKcfxLFUydd",
        "outputId": "5a654db2-509b-4835-d70d-951defd3bdd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdfium2 -q"
      ],
      "metadata": {
        "id": "l4z7a3DtUk6z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69c5ee6b-7f76-4a23-cb70-c09080a034d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/2.9 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p {folder_papers}\n",
        "!cp {all_papers_zip} {folder_papers}all_papers.zip\n",
        "!unzip {folder_papers}all_papers.zip -d {folder_papers}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9o7nbEsDU59P",
        "outputId": "0a810b5d-fff8-4ffd-b18f-f7ec83983d41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ./papers_pdf/all_papers.zip\n",
            "   creating: ./papers_pdf/paper_section/Conclusions/\n",
            "  inflating: ./papers_pdf/paper_section/Conclusions/BIBREF6_2c953a3c378b40dadf2e3fb486713c8608b8e282.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Conclusions/BIBREF97_b97a33933541c276778c3fe63baad6964f4bdf44.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Conclusions/BIBREF98_3df1aa7f82e150662ab8e87b78e64c8238997f1c.pdf  \n",
            "   creating: ./papers_pdf/paper_section/Interaction-focused_Systems/\n",
            "   creating: ./papers_pdf/paper_section/Interaction-focused_Systems-Convolutional_Neural_Networks/\n",
            "  inflating: ./papers_pdf/paper_section/Interaction-focused_Systems-Convolutional_Neural_Networks/BIBREF24_563e821bb5ea825efb56b77484f5287f08cf3753.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Interaction-focused_Systems-Convolutional_Neural_Networks/BIBREF25_d51ed05fd05b9d222427a05a87ed88217447b44f.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Interaction-focused_Systems-Convolutional_Neural_Networks/BIBREF26_ea738439b880ad033ff01602ea52d04b366d0d37.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Interaction-focused_Systems-Convolutional_Neural_Networks/BIBREF27_fc3384d631f5e2b2a9d66623d4d3e1d28b96dee7.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Interaction-focused_Systems-Convolutional_Neural_Networks/BIBREF28_32e7f0863e7c56cfced89abedaee46e2288bc127.pdf  \n",
            "   creating: ./papers_pdf/paper_section/Interaction-focused_Systems-Dealing_with_long_texts/\n",
            "  inflating: ./papers_pdf/paper_section/Interaction-focused_Systems-Dealing_with_long_texts/BIBREF45_7a31e2dcbaa1cf6e9f76084793a02a2a4e4c2d15.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Interaction-focused_Systems-Dealing_with_long_texts/BIBREF46_afed54533ecc624cb5e0241172268c6188ded20c.pdf  \n",
            "   creating: ./papers_pdf/paper_section/Interaction-focused_Systems-Fine-tuning_Interaction-focused_Systems/\n",
            "  inflating: ./papers_pdf/paper_section/Interaction-focused_Systems-Fine-tuning_Interaction-focused_Systems/BIBREF44_d1a2d203733208deda7427c8e20318334193d9d7.pdf  \n",
            "   creating: ./papers_pdf/paper_section/Interaction-focused_Systems-Pre-trained_Language_Models/\n",
            "  inflating: ./papers_pdf/paper_section/Interaction-focused_Systems-Pre-trained_Language_Models/BIBREF21_df2b0e26d0599ce3e70df8a9da02e51594e0e992.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Interaction-focused_Systems-Pre-trained_Language_Models/BIBREF22_077f8329a7b6fa3b7c877a57b81eb6c18b5f87de.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Interaction-focused_Systems-Pre-trained_Language_Models/BIBREF34_a54b56af24bb4873ed0163b77df63b92bd018ddc.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Interaction-focused_Systems-Pre-trained_Language_Models/BIBREF35_395de0bd3837fdf4b4b5e5f04835bcc69c279481.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Interaction-focused_Systems-Pre-trained_Language_Models/BIBREF36_3cfb319689f06bf04c2e28399361f414ca32c4b3.pdf  \n",
            "   creating: ./papers_pdf/paper_section/Interaction-focused_Systems-Ranking_with_Encoder-decoder_Models/\n",
            "  inflating: ./papers_pdf/paper_section/Interaction-focused_Systems-Ranking_with_Encoder-decoder_Models/BIBREF36_3cfb319689f06bf04c2e28399361f414ca32c4b3.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Interaction-focused_Systems-Ranking_with_Encoder-decoder_Models/BIBREF41_9405cc0d6169988371b2755e573cc28650d14dfe.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Interaction-focused_Systems-Ranking_with_Encoder-decoder_Models/BIBREF42_d0086b86103a620a86bc918746df0aa642e2a8a3.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Interaction-focused_Systems-Ranking_with_Encoder-decoder_Models/BIBREF43_f6e0164466e827112fd415afdc28ddf8e0eb1ba3.pdf  \n",
            "   creating: ./papers_pdf/paper_section/Interaction-focused_Systems-Ranking_with_Encoder-only_Models/\n",
            "  inflating: ./papers_pdf/paper_section/Interaction-focused_Systems-Ranking_with_Encoder-only_Models/BIBREF37_dbde7dfa6cae81df8ac19ef500c42db96c3d1edd.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Interaction-focused_Systems-Ranking_with_Encoder-only_Models/BIBREF38_85e07116316e686bf787114ba10ca60f4ea7c5b2.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Interaction-focused_Systems-Ranking_with_Encoder-only_Models/BIBREF39_1ec78c0ec945572673fabd50bf263870fe9d3601.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Interaction-focused_Systems-Ranking_with_Encoder-only_Models/BIBREF40_63a2fabbe4b1615a84d5f4d90987733cf09e3ff8.pdf  \n",
            "   creating: ./papers_pdf/paper_section/Learned_Sparse_Retrieval/\n",
            "   creating: ./papers_pdf/paper_section/Learned_Sparse_Retrieval-Document_expansion_learning/\n",
            "  inflating: ./papers_pdf/paper_section/Learned_Sparse_Retrieval-Document_expansion_learning/BIBREF36_3cfb319689f06bf04c2e28399361f414ca32c4b3.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Learned_Sparse_Retrieval-Document_expansion_learning/BIBREF79_1225eb6570ce8d45067329fafcc8ff7636a65923.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Learned_Sparse_Retrieval-Document_expansion_learning/BIBREF81_7f8cbf4453062af8d5c8cf1a94ff01867d122121.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Learned_Sparse_Retrieval-Document_expansion_learning/BIBREF82_b092b6b843e9421bf42bf96f57ed4658a3e0bdf7.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Learned_Sparse_Retrieval-Document_expansion_learning/BIBREF83_54fa64b74ec020699fad989f85e74e50c7a34445.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Learned_Sparse_Retrieval-Document_expansion_learning/BIBREF84_204e3073870fae3d05bcbc2f6a8e263d9b72e776.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Learned_Sparse_Retrieval-Document_expansion_learning/BIBREF85_29de7c0fb3c09eaf55b20619bceaeafe72fd87a6.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Learned_Sparse_Retrieval-Document_expansion_learning/BIBREF86_ea6838915acc4e7764caa8f2fdae8dc4629f924b.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Learned_Sparse_Retrieval-Document_expansion_learning/BIBREF87_57a07372e2a620d6ae920f74877eee5f61753a96.pdf  \n",
            "   creating: ./papers_pdf/paper_section/Learned_Sparse_Retrieval-Impact_score_learning/\n",
            "  inflating: ./papers_pdf/paper_section/Learned_Sparse_Retrieval-Impact_score_learning/BIBREF3_629f50daebbb9003f645f671f76cc6b33088c17d.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Learned_Sparse_Retrieval-Impact_score_learning/BIBREF86_ea6838915acc4e7764caa8f2fdae8dc4629f924b.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Learned_Sparse_Retrieval-Impact_score_learning/BIBREF88_1c68eefcceb042fb79712aed347370d6ce7190c1.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Learned_Sparse_Retrieval-Impact_score_learning/BIBREF89_3de1752cd0854e220fc41f0ccf7db913f846284c.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Learned_Sparse_Retrieval-Impact_score_learning/BIBREF91_4aa1d28944856ebe1950a27f633c6667ead3cbf8.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Learned_Sparse_Retrieval-Impact_score_learning/BIBREF93_89d373d61c68465fd49da1257aa959e5abefd155.pdf  \n",
            "   creating: ./papers_pdf/paper_section/Learned_Sparse_Retrieval-Sparse_representation_learning/\n",
            "  inflating: ./papers_pdf/paper_section/Learned_Sparse_Retrieval-Sparse_representation_learning/BIBREF87_57a07372e2a620d6ae920f74877eee5f61753a96.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Learned_Sparse_Retrieval-Sparse_representation_learning/BIBREF94_0c57dcf959ead9530f9ec3ebe0dd58de42a3e8af.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Learned_Sparse_Retrieval-Sparse_representation_learning/BIBREF95_1e8a6de5561f557ff9abf43d538d8d5e9347efa0.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Learned_Sparse_Retrieval-Sparse_representation_learning/BIBREF96_9f753f67da834e59f9a5c8cdf9a88ee84c496b2d.pdf  \n",
            "   creating: ./papers_pdf/paper_section/List_of_Symbols/\n",
            "   creating: ./papers_pdf/paper_section/Neural_Information_Retrieval/\n",
            "   creating: ./papers_pdf/paper_section/Neural_Information_Retrieval-Notes/\n",
            "   creating: ./papers_pdf/paper_section/Representation-focused_Systems/\n",
            "   creating: ./papers_pdf/paper_section/Representation-focused_Systems-Fine-tuning_Representation-focused_Systems/\n",
            "  inflating: ./papers_pdf/paper_section/Representation-focused_Systems-Fine-tuning_Representation-focused_Systems/BIBREF50_79cd9f77e5258f62c0e15d11534aea6393ef73fe.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Representation-focused_Systems-Fine-tuning_Representation-focused_Systems/BIBREF58_fdb813d8b927bdd21ae1858cafa6c34b66a36268.pdf  \n",
            "   creating: ./papers_pdf/paper_section/Representation-focused_Systems-Multiple_Representations/\n",
            "  inflating: ./papers_pdf/paper_section/Representation-focused_Systems-Multiple_Representations/BIBREF53_bb2afd8172469fef7276e9789b306e085ed6e650.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Representation-focused_Systems-Multiple_Representations/BIBREF54_050050e30d0f162c4dd87c1aac8d37df266e4c93.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Representation-focused_Systems-Multiple_Representations/BIBREF55_60b8ad6177230ad5402af409a6edb5af441baeb4.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Representation-focused_Systems-Multiple_Representations/BIBREF56_2d7a784a093615d00d4ac0a7b5763a15d86d4996.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Representation-focused_Systems-Multiple_Representations/BIBREF57_4de571b44a7612189dded764f7a2988ac7529158.pdf  \n",
            "   creating: ./papers_pdf/paper_section/Representation-focused_Systems-Single_Representations/\n",
            "  inflating: ./papers_pdf/paper_section/Representation-focused_Systems-Single_Representations/BIBREF3_629f50daebbb9003f645f671f76cc6b33088c17d.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Representation-focused_Systems-Single_Representations/BIBREF39_1ec78c0ec945572673fabd50bf263870fe9d3601.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Representation-focused_Systems-Single_Representations/BIBREF47_0c3bdbad193ec8a5b1f4005dc1496e341a2025b4.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Representation-focused_Systems-Single_Representations/BIBREF48_f7c455cc5a40d2a31b63ac2657c9d2d6c53b1be5.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Representation-focused_Systems-Single_Representations/BIBREF49_997dc5d9a058753f034422afe7bd0cc0b8ad808b.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Representation-focused_Systems-Single_Representations/BIBREF50_79cd9f77e5258f62c0e15d11534aea6393ef73fe.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Representation-focused_Systems-Single_Representations/BIBREF51_c9b8593db099869fe7254aa1fa53f3c9073b0176.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Representation-focused_Systems-Single_Representations/BIBREF52_7b577ba0e4230b2ac58d297b3d2cfc3d2f1aaace.pdf  \n",
            "   creating: ./papers_pdf/paper_section/Retrieval_Architectures_and_Vector_Search/\n",
            "   creating: ./papers_pdf/paper_section/Retrieval_Architectures_and_Vector_Search-Graph_approaches/\n",
            "  inflating: ./papers_pdf/paper_section/Retrieval_Architectures_and_Vector_Search-Graph_approaches/BIBREF69_f17c6e164ccc7ec1ad91b3fbbafe8f84664e9803.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Retrieval_Architectures_and_Vector_Search-Graph_approaches/BIBREF70_c197ecb6a6987667cadcb498136989af1827cce0.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Retrieval_Architectures_and_Vector_Search-Graph_approaches/BIBREF71_e2e073433931c4d1a739f548b7d17b6e9b2fa13e.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Retrieval_Architectures_and_Vector_Search-Graph_approaches/BIBREF72_699a2e3b653c69aff5cf7a9923793b974f8ca164.pdf  \n",
            "   creating: ./papers_pdf/paper_section/Retrieval_Architectures_and_Vector_Search-Locality_sensitive_hashing_approaches/\n",
            "  inflating: ./papers_pdf/paper_section/Retrieval_Architectures_and_Vector_Search-Locality_sensitive_hashing_approaches/BIBREF64_1955266a8a58d94e41ad0efe20d707c92a069e95.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Retrieval_Architectures_and_Vector_Search-Locality_sensitive_hashing_approaches/BIBREF65_3f1e54ed3bd801766e1897d53a9fc962524dd3c2.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Retrieval_Architectures_and_Vector_Search-Locality_sensitive_hashing_approaches/BIBREF66_9ed960374381062d85d3944182a539c1d00f7703.pdf  \n",
            "   creating: ./papers_pdf/paper_section/Retrieval_Architectures_and_Vector_Search-MIP_and_NN_Search_Problems/\n",
            "  inflating: ./papers_pdf/paper_section/Retrieval_Architectures_and_Vector_Search-MIP_and_NN_Search_Problems/BIBREF62_5b0a88bdec473552c6a386cd94fdac53c74b79a8.pdf  \n",
            "   creating: ./papers_pdf/paper_section/Retrieval_Architectures_and_Vector_Search-Optimisations/\n",
            "  inflating: ./papers_pdf/paper_section/Retrieval_Architectures_and_Vector_Search-Optimisations/BIBREF50_79cd9f77e5258f62c0e15d11534aea6393ef73fe.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Retrieval_Architectures_and_Vector_Search-Optimisations/BIBREF51_c9b8593db099869fe7254aa1fa53f3c9073b0176.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Retrieval_Architectures_and_Vector_Search-Optimisations/BIBREF52_7b577ba0e4230b2ac58d297b3d2cfc3d2f1aaace.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Retrieval_Architectures_and_Vector_Search-Optimisations/BIBREF55_60b8ad6177230ad5402af409a6edb5af441baeb4.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Retrieval_Architectures_and_Vector_Search-Optimisations/BIBREF73_2cbb8de53759e75411bc528518947a3094fbce3a.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Retrieval_Architectures_and_Vector_Search-Optimisations/BIBREF74_28336cbf2ee3e8fca6b173c91c5ca9628ba1fa4a.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Retrieval_Architectures_and_Vector_Search-Optimisations/BIBREF75_d6f83c915565f575e55fdce0424f65fe192af218.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Retrieval_Architectures_and_Vector_Search-Optimisations/BIBREF76_91429255eefe48ad140ccfaf6aa1e6be11a72a53.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Retrieval_Architectures_and_Vector_Search-Optimisations/BIBREF77_c3cf35677834fb535d3bc7cf8d375366df4b1397.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Retrieval_Architectures_and_Vector_Search-Optimisations/BIBREF78_2408c965a3855a6b66d128195c783d76e2e939da.pdf  \n",
            "   creating: ./papers_pdf/paper_section/Retrieval_Architectures_and_Vector_Search-Retrieval_architectures/\n",
            "   creating: ./papers_pdf/paper_section/Retrieval_Architectures_and_Vector_Search-Vector_quantisation_approaches/\n",
            "  inflating: ./papers_pdf/paper_section/Retrieval_Architectures_and_Vector_Search-Vector_quantisation_approaches/BIBREF67_c564aa7639a08c280423489e52b6e32055c9aa7f.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Retrieval_Architectures_and_Vector_Search-Vector_quantisation_approaches/BIBREF68_4748d22348e72e6e06c2476486afddbc76e5eca7.pdf  \n",
            "   creating: ./papers_pdf/paper_section/Text_Representations_for_Ranking/\n",
            "  inflating: ./papers_pdf/paper_section/Text_Representations_for_Ranking/BIBREF7_514bd50d54908ff49cc6ec41386e4b7a5d606e0f.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Text_Representations_for_Ranking/BIBREF8_47354d4d1915ae3d286d401005ba8a44af7d1fa5.pdf  \n",
            "   creating: ./papers_pdf/paper_section/Text_Representations_for_Ranking-BOW_Encodings/\n",
            "  inflating: ./papers_pdf/paper_section/Text_Representations_for_Ranking-BOW_Encodings/BIBREF10_47ced790a563344efae66588b5fb7fe6cca29ed3.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Text_Representations_for_Ranking-BOW_Encodings/BIBREF11_73a76dd71abfbd29dbba4ea034ab52284626aa71.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Text_Representations_for_Ranking-BOW_Encodings/BIBREF5_3cf0822f63e51be5343028bad7ee72a5882ef7de.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Text_Representations_for_Ranking-BOW_Encodings/BIBREF9_d5f169880e30e1f76827d72f862555d00b01bed9.pdf  \n",
            "   creating: ./papers_pdf/paper_section/Text_Representations_for_Ranking-LTR_Features/\n",
            "  inflating: ./papers_pdf/paper_section/Text_Representations_for_Ranking-LTR_Features/BIBREF12_008f1d2741ebef51e6400686b050e046455b52fb.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Text_Representations_for_Ranking-LTR_Features/BIBREF14_6adb7a21e59d962c6420471d151602cdfbec48a8.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Text_Representations_for_Ranking-LTR_Features/BIBREF15_0df9c70875783a73ce1e933079f328e8cf5e9ea2.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Text_Representations_for_Ranking-LTR_Features/BIBREF16_63aaf12163fe9735dfe9a69114937c4fa34f303a.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Text_Representations_for_Ranking-LTR_Features/BIBREF4_5fc5c5a4e489e781de434567d946e6eb65c44f60.pdf  \n",
            "   creating: ./papers_pdf/paper_section/Text_Representations_for_Ranking-Word_Embeddings/\n",
            "  inflating: ./papers_pdf/paper_section/Text_Representations_for_Ranking-Word_Embeddings/BIBREF17_5303f288c0de1fc717c3389773a2a684589ee46b.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Text_Representations_for_Ranking-Word_Embeddings/BIBREF18_87f40e6f3022adbc1f1905e3e506abad05a9964f.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Text_Representations_for_Ranking-Word_Embeddings/BIBREF19_892e53fe5cd39f037cb2a961499f42f3002595dd.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Text_Representations_for_Ranking-Word_Embeddings/BIBREF20_f37e1b62a767a307c046404ca96bc140b3e68cb5.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Text_Representations_for_Ranking-Word_Embeddings/BIBREF21_df2b0e26d0599ce3e70df8a9da02e51594e0e992.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Text_Representations_for_Ranking-Word_Embeddings/BIBREF22_077f8329a7b6fa3b7c877a57b81eb6c18b5f87de.pdf  \n",
            "  inflating: ./papers_pdf/paper_section/Text_Representations_for_Ranking-Word_Embeddings/BIBREF23_cd18800a0fe0b668a1cc19f2ec95b5003d0a5035.pdf  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lista de títulos e abstract por artigo\n",
        "titles_and_abstracts_by_id = {\n",
        "  \"BIBREF6_2c953a3c378b40dadf2e3fb486713c8608b8e282\": (\"Pretrained Transformers for Text Ranking: BERT and Beyond\", \"The goal of text ranking is to generate an ordered list of texts retrieved from a corpus in response to a query for a particular task. Although the most common formulation of text ranking is search, instances of the task can also be found in many text processing applications. This tutorial provides an overview of text ranking with neural network architectures known as transformers, of which BERT (Bidirectional Encoder Representations from Transformers) is the best-known example. These models produce high quality results across many domains, tasks, and settings. This tutorial, which is based on the preprint of a forthcoming book to be published by Morgan and & Claypool under the Synthesis Lectures on Human Language Technologies series, provides an overview of existing work as a single point of entry for practitioners who wish to deploy transformers for text ranking in real-world applications and researchers who wish to pursue work in this area. We cover a wide range of techniques, grouped into two categories: transformer models that perform reranking in multi-stage ranking architectures and learned dense representations that perform ranking directly.\"),\n",
        "  \"BIBREF97_b97a33933541c276778c3fe63baad6964f4bdf44\": (\"Neural Approaches to Conversional Information Retrieval\", \"A conversational information retrieval (CIR) system is an information retrieval (IR) system with a conversational interface which allows users to interact with the system to seek information via multi-turn conversations of natural language, in spoken or written form. Recent progress in deep learning has brought tremendous improvements in natural language processing (NLP) and conversational AI, leading to a plethora of commercial conversational services that allow naturally spoken and typed interaction, increasing the need for more human-centric interactions in IR. As a result, we have witnessed a resurgent interest in developing modern CIR systems in both research communities and industry. This book surveys recent advances in CIR, focusing on neural approaches that have been developed in the last few years. This book is based on the authors’ tutorial at SIGIR’2020 [Gao et al., 2020b], with IR and NLP communities as the primary target audience. However, audiences with other background, such as machine learning and human-computer interaction, will also find it an accessible introduction to CIR. We hope that this book will prove a valuable resource for students, researchers, and software developers.\"),\n",
        "  \"BIBREF98_3df1aa7f82e150662ab8e87b78e64c8238997f1c\": (\"Conversational Information Seeking - An Introduction to Conversational Search, Recommendation, and Question Answering\", \"\"),\n",
        "  \"BIBREF24_563e821bb5ea825efb56b77484f5287f08cf3753\": (\"Convolutional Networks for Images, Speech, and Time-Series\", \"\"),\n",
        "  \"BIBREF25_d51ed05fd05b9d222427a05a87ed88217447b44f\": (\"A Deep Relevance Matching Model for Ad-hoc Retrieval\", \"In recent years, deep neural networks have led to exciting breakthroughs in speech recognition, computer vision, and natural language processing (NLP) tasks. However, there have been few positive results of deep models on ad-hoc retrieval tasks. This is partially due to the fact that many important characteristics of the ad-hoc retrieval task have not been well addressed in deep models yet. Typically, the ad-hoc retrieval task is formalized as a matching problem between two pieces of text in existing work using deep models, and treated equivalent to many NLP tasks such as paraphrase identification, question answering and automatic conversation. However, we argue that the ad-hoc retrieval task is mainly about relevance matching while most NLP matching tasks concern semantic matching, and there are some fundamental differences between these two matching tasks. Successful relevance matching requires proper handling of the exact matching signals, query term importance, and diverse matching requirements. In this paper, we propose a novel deep relevance matching model (DRMM) for ad-hoc retrieval. Specifically, our model employs a joint deep architecture at the query term level for relevance matching. By using matching histogram mapping, a feed forward matching network, and a term gating network, we can effectively deal with the three relevance matching factors mentioned above. Experimental results on two representative benchmark collections show that our model can significantly outperform some well-known retrieval models as well as state-of-the-art deep matching models.\"),\n",
        "  \"BIBREF26_ea738439b880ad033ff01602ea52d04b366d0d37\": (\"End-to-End Neural Ad-hod Ranking with Kernel Pooling\", \"This paper proposes K-NRM, a kernel based neural model for document ranking. Given a query and a set of documents, K-NRM uses a translation matrix that models word-level similarities via word embeddings, a new kernel-pooling technique that uses kernels to extract multi-level soft match features, and a learning-to-rank layer that combines those features into the final ranking score. The whole model is trained end-to-end. The ranking layer learns desired feature patterns from the pairwise ranking loss. The kernels transfer the feature patterns into soft-match targets at each similarity level and enforce them on the translation matrix. The word embeddings are tuned accordingly so that they can produce the desired soft matches. Experiments on a commercial search engine's query log demonstrate the improvements of K-NRM over prior feature-based and neural-based states-of-the-art, and explain the source of K-NRM's advantage: Its kernel-guided embedding encodes a similarity metric tailored for matching query words to document words, and provides effective multi-level soft matches.\"),\n",
        "  \"BIBREF27_fc3384d631f5e2b2a9d66623d4d3e1d28b96dee7\": (\"Convolutional Neural Networks for Soft-Machting N-Grams in Ad-hoc Search\", \"This paper presents Conv-KNRM, a Convolutional Kernel-based Neural Ranking Model that models n-gram soft matches for ad-hoc search. Instead of exact matching query and document n-grams, Conv-KNRM uses Convolutional Neural Networks to represent ngrams of various lengths and soft matches them in a unified embedding space. The n-gram soft matches are then utilized by the kernel pooling and learning-to-rank layers to generate the final ranking score. Conv-KNRM can be learned end-to-end and fully optimized from user feedback. The learned model’s generalizability is investigated by testing how well it performs in a related domain with small amounts of training data. Experiments on English search logs, Chinese search logs, and TREC Web track tasks demonstrated consistent advantages of Conv-KNRM over prior neural IR methods and feature-based methods.\"),\n",
        "  \"BIBREF28_32e7f0863e7c56cfced89abedaee46e2288bc127\": (\"PACRR: A Position-Aware Neural IR Model for Relevance Matching\", \"In order to adopt deep learning for information retrieval, models are needed that can capture all relevant information required to assess the relevance of a document to a given user query. While previous works have successfully captured unigram term matches, how to fully employ position-dependent information such as proximity and term dependencies has been insufficiently explored. In this work, we propose a novel neural IR model named PACRR aiming at better modeling position-dependent interactions between a query and a document. Extensive experiments on six years' TREC Web Track data confirm that the proposed model yields better results under multiple benchmarks.\"),\n",
        "  \"BIBREF45_7a31e2dcbaa1cf6e9f76084793a02a2a4e4c2d15\": (\"Deeper Text Understanding for IR with Contextual Neural Language Modeling\", \"Neural networks provide new possibilities to automatically learn complex language patterns and query-document relations. Neural IR models have achieved promising results in learning query-document relevance patterns, but few explorations have been done on understanding the text content of a query or a document. This paper studies leveraging a recently-proposed contextual neural language model, BERT, to provide deeper text understanding for IR. Experimental results demonstrate that the contextual text representations from BERT are more effective than traditional word embeddings. Compared to bag-of-words retrieval models, the contextual language model can better leverage language structures, bringing large improvements on queries written in natural languages. Combining the text understanding ability with search knowledge leads to an enhanced pre-trained BERT model that can benefit related search tasks where training data are limited.\"),\n",
        "  \"BIBREF46_afed54533ecc624cb5e0241172268c6188ded20c\": (\"PARADE: Passage Representation Aggregation for Document Reranking\", \"Pre-trained transformer models, such as BERT and T5, have shown to be highly effective at ad-hoc passage and document ranking. Due to the inherent sequence length limits of these models, they need to process document passages one at a time rather than processing the entire document sequence at once. Although several approaches for aggregating passage-level signals into a document-level relevance score have been proposed, there has yet to be an extensive comparison of these techniques. In this work, we explore strategies for aggregating relevance signals from a document’s passages into a final ranking score. We find that passage representation aggregation techniques can significantly improve over score aggregation techniques proposed in prior work, such as taking the maximum passage score. We call this new approach PARADE. In particular, PARADE can significantly improve results on collections with broad information needs where relevance signals can be spread throughout the document (such as TREC Robust04 and GOV2). Meanwhile, less complex aggregation techniques may work better on collections with an information need that can often be pinpointed to a single passage (such as TREC DL and TREC Genomics). We also conduct efficiency analyses and highlight several strategies for improving transformer-based aggregation.\"),\n",
        "  \"BIBREF44_d1a2d203733208deda7427c8e20318334193d9d7\": (\"Distance metric learning, with application to clustering with side-information\", \"Many algorithms rely critically on being given a good metric over their inputs. For instance, data can often be clustered in many “plausible” ways, and if a clustering algorithm such as K-means initially fails to find one that is meaningful to a user, the only recourse may be for the user to manually tweak the metric until sufficiently good clusters are found. For these and other applications requiring good metrics, it is desirable that we provide a more systematic way for users to indicate what they consider “similar.” For instance, we may ask them to provide examples. In this paper, we present an algorithm that, given examples of similar (and, if desired, dissimilar) pairs of points in R, learns a distance metric over R^n that respects these relationships. Our method is based on posing metric learning as a convex optimization problem, which allows us to give efficient, local-optima-free algorithms. We also demonstrate empirically that the learned metrics can be used to significantly improve clustering performance.\"),\n",
        "  \"BIBREF21_df2b0e26d0599ce3e70df8a9da02e51594e0e992\": (\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\", \"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\"),\n",
        "  \"BIBREF22_077f8329a7b6fa3b7c877a57b81eb6c18b5f87de\": (\"RoBERTa: A Robustly Optimized BERT Pretraining Approach\", \"Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.\"),\n",
        "  \"BIBREF34_a54b56af24bb4873ed0163b77df63b92bd018ddc\": (\"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter\", \"As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.\"),\n",
        "  \"BIBREF35_395de0bd3837fdf4b4b5e5f04835bcc69c279481\": (\"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\", \"We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.\"),\n",
        "  \"BIBREF36_3cfb319689f06bf04c2e28399361f414ca32c4b3\": (\"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\", \"Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.\"),\n",
        "  \"BIBREF41_9405cc0d6169988371b2755e573cc28650d14dfe\": (\"Language Models are Unsupervised Multask Learners\", \"Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.\"),\n",
        "  \"BIBREF42_d0086b86103a620a86bc918746df0aa642e2a8a3\": (\"Language Models as Knowledge Bases?\", \"Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as “fill-in-the-blank” cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at https://github.com/facebookresearch/LAMA.\"),\n",
        "  \"BIBREF43_f6e0164466e827112fd415afdc28ddf8e0eb1ba3\": (\"Document Ranking with a Pretrained Sequence-to-Sequence Model\", \"This work proposes the use of a pretrained sequence-to-sequence model for document ranking. Our approach is fundamentally different from a commonly adopted classification-based formulation based on encoder-only pretrained transformer architectures such as BERT. We show how a sequence-to-sequence model can be trained to generate relevance labels as “target tokens”, and how the underlying logits of these target tokens can be interpreted as relevance probabilities for ranking. Experimental results on the MS MARCO passage ranking task show that our ranking approach is superior to strong encoder-only models. On three other document retrieval test collections, we demonstrate a zero-shot transfer-based approach that outperforms previous state-of-the-art models requiring in-domain cross-validation. Furthermore, we find that our approach significantly outperforms an encoder-only architecture in a data-poor setting. We investigate this observation in more detail by varying target tokens to probe the model’s use of latent knowledge. Surprisingly, we find that the choice of target tokens impacts effectiveness, even for words that are closely related semantically. This finding sheds some light on why our sequence-to-sequence formulation for document ranking is effective. Code and models are available at pygaggle.ai.\"),\n",
        "  \"BIBREF37_dbde7dfa6cae81df8ac19ef500c42db96c3d1edd\": (\"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation\", \"Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (\\\"wordpieces\\\") for both input and output. This method provides a good balance between the flexibility of \\\"character\\\"-delimited models and the efficiency of \\\"word\\\"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system.\"),\n",
        "  \"BIBREF38_85e07116316e686bf787114ba10ca60f4ea7c5b2\": (\"Passage Re-Ranking with BERT\", \"Recently, neural models pretrained on a language modeling task, such as ELMo (Peters et al., 2017), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2018), have achieved impressive results on various natural language processing tasks such as question-answering and natural language inference. In this paper, we describe a simple re-implementation of BERT for query-based passage re-ranking. Our system is the state of the art on the TREC-CAR dataset and the top entry in the leaderboard of the MS MARCO passage retrieval task, outperforming the previous state of the art by 27% (relative) in MRR@10. The code to reproduce our results is available at this https URL\"),\n",
        "  \"BIBREF39_1ec78c0ec945572673fabd50bf263870fe9d3601\": (\"CEDR: Contextualized Embeddings for Document Reranking\", \"Although considerable attention has been given to neural ranking architectures recently, far less attention has been paid to the term representations that are used as input to these models. In this work, we investigate how two pretrained contextualized language models (ELMo and BERT) can be utilized for ad-hoc document ranking. Through experiments on TREC benchmarks, we find that several ex-sting neural ranking architectures can benefit from the additional context provided by contextualized language models. Furthermore, we propose a joint approach that incorporates BERT's classification vector into existing neural models and show that it outperforms state-of-the-art ad-hoc ranking baselines. We call this joint approach CEDR (Contextualized Embeddings for Document Ranking). We also address practical challenges in using these models for ranking, including the maximum input length imposed by BERT and runtime performance impacts of contextualized language models.\"),\n",
        "  \"BIBREF40_63a2fabbe4b1615a84d5f4d90987733cf09e3ff8\": (\"Multi-Stage Document Ranking with BERT\", \"The advent of deep neural networks pre-trained via language modeling tasks has spurred a number of successful applications in natural language processing. This work explores one such popular model, BERT, in the context of document ranking. We propose two variants, called monoBERT and duoBERT, that formulate the ranking problem as pointwise and pairwise classification, respectively. These two models are arranged in a multi-stage ranking architecture to form an end-to-end search system. One major advantage of this design is the ability to trade off quality against latency by controlling the admission of candidates into each pipeline stage, and by doing so, we are able to find operating points that offer a good balance between these two competing metrics. On two large-scale datasets, MS MARCO and TREC CAR, experiments show that our model produces results that are either at or comparable to the state of the art. Ablation studies show the contributions of each component and characterize the latency/quality tradeoff space.\"),\n",
        "  \"BIBREF79_1225eb6570ce8d45067329fafcc8ff7636a65923\": (\"Modeling and Solving Term Mismatch for Full-Text Retrieval\", \"Even though modern retrieval systems typically use a multitude of features to rank documents, the backbone for search ranking is usually the standard tf.idf retrieval models. This thesis addresses a limitation of the fundamental retrieval models, the term mismatch problem, which happens when query terms fail to appear in the documents that are relevant to the query. The term mismatch problem is a long standing problem in information retrieval. However, it was not well understood how often term mismatch happens in retrieval, how important it is for retrieval, or how it affects retrieval performance. This thesis answers the above questions, and proposes principled solutions to address this limitation. The new understandings of the retrieval models will benefit its users, as well as inform the development of software applications built on top of them. This new direction of research is enabled by the formal definition of the probability of term mismatch, and quantitative data analyses around it. In this thesis, term mismatch is defined as the probability of a term not appearing in a document that is relevant to the query. The complement of term mismatch is the term recall, the probability of a term appearing in relevant documents. Even though the term recall probability is known to be a fundamental quantity in the theory of probabilistic information retrieval, prior research in ad hoc retrieval provided few clues about how to estimate term recall reliably. This dissertation research designs two term mismatch prediction methods. With exploratory data analyses, this research first identifies common reasons that user-specified query terms fail to appear in documents relevant to the query, develops features correlated with each reason, and integrates them into a predictive model that can be trained from data. This prediction model uses training queries with relevance judgments to predict term mismatch for test queries without known relevance, and can be viewed as a type of transfer learning where training queries represent related ranking tasks that are used by the learning algorithm to facilitate the ranking for new test tasks. Further data analyses focus on the variation of the term mismatch probability for the same term across different queries, and demonstrate that query dependent features are needed for effective term mismatch prediction. At the same time, because the cross-query variation of term mismatch is small for most of the repeating term occurrences, a second mismatch prediction method is designed to use historic occurrences of the same term to predict the mismatch probability for its test occurrences. This provides an alternative and more efficient procedure to predict term mismatch. Effective term mismatch predictions can be used in several different ways to improve retrieval. The probabilistic retrieval theory suggests to use the term recall probabilities as term weights in the retrieval models. Experiments on 6 different TREC Ad hoc track and Web track datasets show that this automatic intervention improves both retrieval recall and precision substantially for long queries. Even though term weighting does not substantially improve retrieval accuracy for short queries which typically have a higher baseline performance, much larger gains are possible by solving mismatch using user expanded Conjunctive Normal Form queries. These queries try to fix the mismatch problem by expanding every query term individually. Our method uses the automatic term mismatch predictions as a diagnostic tool to guide interactive interventions, so that the users can expand the query terms that need expansion most. Simulated expansion interactions based on real user-expanded queries on TREC Ad hoc and Legal track datasets show that expanding the terms that have the highest predicted mismatch probabilities effectively improves retrieval performance. The resulting Boolean Conjunctive Normal Form expansion queries are both compact and effective, substantially outperforming the short keyword queries as well as the traditional bag of word expansion that may use the same set of high quality manual expansion terms. Promising problems for future research are identified, together with research areas where the term mismatch research may make an impact.\"),\n",
        "  \"BIBREF81_7f8cbf4453062af8d5c8cf1a94ff01867d122121\": (\"Time-Based Language Models\", \"We explore the relationship between time and relevance using TREC ad-hoc queries. Two types of queries are classified as timebased: one favors very recent documents and the other has more relevant documents within a specific period in the past. We propose a time-based language model approach to retrieval for these queries. We show how time can be incorporated into both query-likelihood models and relevance models. The experiments on TREC title queries show time-based language models outperforming baseline language model approaches on both types of time queries\"),\n",
        "  \"BIBREF82_b092b6b843e9421bf42bf96f57ed4658a3e0bdf7\": (\"Document Expansion by Query Prediction\", \"One technique to improve the retrieval effectiveness of a search engine is to expand documents with terms that are related or representative of the documents' content.From the perspective of a question answering system, this might comprise questions the document can potentially answer. Following this observation, we propose a simple method that predicts which queries will be issued for a given document and then expands it with those predictions with a vanilla sequence-to-sequence model, trained using datasets consisting of pairs of query and relevant documents. By combining our method with a highly-effective re-ranking component, we achieve the state of the art in two retrieval tasks. In a latency-critical regime, retrieval results alone (without re-ranking) approach the effectiveness of more computationally expensive neural re-rankers but are much faster.\"),\n",
        "  \"BIBREF83_54fa64b74ec020699fad989f85e74e50c7a34445\": (\"From doc2query to docTTTTTquery\", \"The setup in this work follows doc2query, but with T5 [8] as the expansion model. T5 is a sequence-tosequence model that uses a similar pretraining objective as BERT [3] to pretrain its encoder-decoder architecture. In this model, all target tasks are cast as sequence-to-sequence tasks. In our case, we feed as input the passage and train the model to generate the question. We train the model with a constant learning rate of 10−4 for 4k iterations with batches of 256, which corresponds to 2 epochs with the MS MARCO training set. We use a maximum of 512 input tokens and 64 output tokens. In the MS MARCO dataset, none of the inputs or outputs have to be truncated when using these lengths. Similar to Nogueira et al. [7], we find that the top-k sampling decoder [4] produces more effective queries than beam search. We use k = 10. In all experiments, we use T5-base as we did not notice any improvement in retrieval effectiveness with the large model. We did not experiment with T5-3B and T5-11B due to their computational cost.\"),\n",
        "  \"BIBREF84_204e3073870fae3d05bcbc2f6a8e263d9b72e776\": (\"Attention is All You Need\", \"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\"),\n",
        "  \"BIBREF85_29de7c0fb3c09eaf55b20619bceaeafe72fd87a6\": (\"Hierarchical Neural Story Generation\", \"We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.\"),\n",
        "  \"BIBREF86_ea6838915acc4e7764caa8f2fdae8dc4629f924b\": (\"Fast Passage Re-ranking with Contextualized Exact Term Matching and Efficient Passage Expansion\", \"BERT-based information retrieval models are expensive, in both time (query latency) and computational resources (energy, hardware cost), making many of these models impractical especially under resource constraints. The reliance on a query encoder that only performs tokenization and on the pre-processing of passage representations at indexing, has allowed the recently proposed TILDE method to overcome the high query latency issue typical of BERT-based models. This however is at the expense of a lower effectiveness compared to other BERT-based re-rankers and dense retrievers. In addition, the original TILDE method is characterised by indexes with a very high memory footprint, as it expands each passage into the size of the BERT vocabulary. In this paper, we propose TILDEv2, a new model that stems from the original TILDE but that addresses its limitations. TILDEv2 relies on contextualized exact term matching with expanded passages. This requires to only store in the index the score of tokens that appear in the expanded passages (rather than all the vocabulary), thus producing indexes that are 99% smaller than those of TILDE. This matching mechanism also improves ranking effectiveness by 24%, without adding to the query latency. This makes TILDEv2 the state-of-the-art passage re-ranking method for CPU-only environments, capable of maintaining query latency below 100ms on commodity hardware.\"),\n",
        "  \"BIBREF87_57a07372e2a620d6ae920f74877eee5f61753a96\": (\"SparTerm: Learning Term-based Sparse Representation for Fast Text Retrieval\", \"Term-based sparse representations dominate the first-stage text retrieval in industrial applications, due to its advantage in efficiency, interpretability, and exact term matching. In this paper, we study the problem of transferring the deep knowledge of the pre-trained language model (PLM) to Term-based Sparse representations, aiming to improve the representation capacity of bag-of-words(BoW) method for semantic-level matching, while still keeping its advantages. Specifically, we propose a novel framework SparTerm to directly learn sparse text representations in the full vocabulary space. The proposed SparTerm comprises an importance predictor to predict the importance for each term in the vocabulary, and a gating controller to control the term activation. These two modules cooperatively ensure the sparsity and flexibility of the final text representation, which unifies the term-weighting and expansion in the same framework. Evaluated on MSMARCO dataset, SparTerm significantly outperforms traditional sparse methods and achieves state of the art ranking performance among all the PLM-based sparse models.\"),\n",
        "  \"BIBREF3_629f50daebbb9003f645f671f76cc6b33088c17d\": (\"Efficient Query Processing for Scalable Web Search\", \"Search engines are exceptionally important tools for accessing information in today’s world. In satisfying the information needs of millions of users, the effectiveness (the quality of the search results) and the efficiency (the speed at which the results are returned to the users) of a search engine are two goals that form a natural trade-off, as techniques that improve the effectiveness of the search engine can also make it less efficient. Meanwhile, search engines continue to rapidly evolve, with larger indexes, more complex retrieval strategies and growing query volumes. Hence, there is a need for the development of efficient query processing infrastructures that make appropriate sacrifices in effectiveness in order to make gains in efficiency. This survey comprehensively reviews the foundations of search engines, from index layouts to basic term-at-a-time (TAAT) and document-at-a-time (DAAT) query processing strategies, while also providing the latest trends in the literature in efficient query processing, including the coherent and systematic reviews of techniques such as dynamic pruning and impact-sorted posting lists as well as their variants and optimisations. Our explanations of query processing strategies, for instance the WAND and BMW dynamic pruning algorithms, are presented with illustrative figures showing how the processing state changes as the algorithms progress. Moreover, acknowledging the recent trends in applying a cascading infrastructure within search systems, this survey describes techniques for efficiently integrating effective learned models, such as those obtained from learning-torank techniques. The survey also covers the selective application of query processing techniques, often achieved by predicting the response times of the search engine (known as query efficiency prediction), and making per-query tradeoffs between efficiency and effectiveness to ensure that the required retrieval speed targets can be met. Finally, the survey concludes with a summary of open directions in efficient search infrastructures, namely the use of signatures, real-time, energy-efficient and modern hardware & software architectures.\"),\n",
        "  \"BIBREF88_1c68eefcceb042fb79712aed347370d6ce7190c1\": (\"Fast Learned Sparse Retrieval with Guided Traversal\", \"Neural information retrieval architectures based on transformers such as BERT are able to significantly improve system effectiveness over traditional sparse models such as BM25. Though highly effective, these neural approaches are very expensive to run, making them difficult to deploy under strict latency constraints. To address this limitation, recent studies have proposed new families of learned sparse models that try to match the effectiveness of learned dense models, while leveraging the traditional inverted index data structure for efficiency. Current learned sparse models learn the weights of terms in documents and, sometimes, queries; however, they exploit different vocabulary structures, document expansion techniques, and query expansion strategies, which can make them slower than traditional sparse models such as BM25. In this work, we propose a novel indexing and query processing technique that exploits a traditional sparse model's \\\"guidance\\\" to efficiently traverse the index, allowing the more effective learned model to execute fewer scoring operations. Our experiments show that our guided processing heuristic is able to boost the efficiency of the underlying learned sparse model by a factor of four without any measurable loss of effectiveness.\"),\n",
        "  \"BIBREF89_3de1752cd0854e220fc41f0ccf7db913f846284c\": (\"Context-Aware Sentence/Passage Term Importance Estimation for First Stage Retrieval\", \"Term frequency is a common method for identifying the importance of a term in a query or document. But it is a weak signal, especially when the frequency distribution is flat, such as in long queries or short documents where the text is of sentence/passage-length. This paper proposes a Deep Contextualized Term Weighting framework that learns to map BERT's contextualized text representations to context-aware term weights for sentences and passages. When applied to passages, DeepCT-Index produces term weights that can be stored in an ordinary inverted index for passage retrieval. When applied to query text, DeepCT-Query generates a weighted bag-of-words query. Both types of term weight can be used directly by typical first-stage retrieval algorithms. This is novel because most deep neural network based ranking models have higher computational costs, and thus are restricted to later-stage rankers. Experiments on four datasets demonstrate that DeepCT's deep contextualized text understanding greatly improves the accuracy of first-stage retrieval algorithms.\"),\n",
        "  \"BIBREF91_4aa1d28944856ebe1950a27f633c6667ead3cbf8\": (\"Learning Passage Impacts for Inverted Indexes\", \"Neural information retrieval systems typically use a cascading pipeline, in which a first-stage model retrieves a candidate set of documents and one or more subsequent stages re-rank this set using contextualized language models such as BERT. In this paper, we propose DeepImpact, a new document term-weighting scheme suitable for efficient retrieval using a standard inverted index. Compared to existing methods, DeepImpact improves impact-score modeling and tackles the vocabulary-mismatch problem. In particular, DeepImpact leverages DocT5Query to enrich the document collection and, using a contextualized language model, directly estimates the semantic importance of tokens in a document, producing a single-value representation for each token in each document. Our experiments show that DeepImpact significantly outperforms prior first-stage retrieval approaches by up to 17% on effectiveness metrics w.r.t. DocT5Query, and, when deployed in a re-ranking scenario, can reach the same effectiveness of state-of-the-art approaches with up to 5.1x speedup in efficiency.\"),\n",
        "  \"BIBREF93_89d373d61c68465fd49da1257aa959e5abefd155\": (\"A Few Brief Notes on DeepImpact, COIL, and a Conceptual Framework for Information Retrieval Techniques\", \"Recent developments in representational learning for information retrieval can be organized in a conceptual framework that establishes two pairs of contrasts: sparse vs. dense representations and unsupervised vs. learned representations. Sparse learned representations can further be decomposed into expansion and term weighting components. This framework allows us to understand the relationship between recently proposed techniques such as DPR, ANCE, DeepCT, DeepImpact, and COIL, and furthermore, gaps revealed by our analysis point to \\\"low hanging fruit\\\" in terms of techniques that have yet to be explored. We present a novel technique dubbed \\\"uniCOIL\\\", a simple extension of COIL that achieves to our knowledge the current state-of-the-art in sparse retrieval on the popular MS MARCO passage ranking dataset. Our implementation using the Anserini IR toolkit is built on the Lucene search library and thus fully compatible with standard inverted indexes.\"),\n",
        "  \"BIBREF94_0c57dcf959ead9530f9ec3ebe0dd58de42a3e8af\": (\"Expansion via Prediction of Importance with Contextualization\", \"The identification of relevance with little textual context is a primary challenge in passage retrieval. We address this problem with a representation-based ranking approach that: (1) explicitly models the importance of each term using a contextualized language model; (2) performs passage expansion by propagating the importance to similar terms; and (3) grounds the representations in the lexicon, making them interpretable. Passage representations can be pre-computed at index time to reduce query-time latency. We call our approach EPIC (Expansion via Prediction of Importance with Contextualization). We show that EPIC significantly outperforms prior importance-modeling and document expansion approaches. We also observe that the performance is additive with the current leading first-stage retrieval methods, further narrowing the gap between inexpensive and cost-prohibitive passage ranking approaches. Specifically, EPIC achieves a MRR@10 of 0.304 on the MS-MARCO passage ranking dataset with 78ms average query latency on commodity hardware. We also find that the latency is further reduced to 68ms by pruning document representations, with virtually no difference in effectiveness.\"),\n",
        "  \"BIBREF95_1e8a6de5561f557ff9abf43d538d8d5e9347efa0\": (\"SPLADE: Sparse Lexical and Expansion Model for First Stage Ranking\", \"In neural Information Retrieval, ongoing research is directed towards improving the first retriever in ranking pipelines. Learning dense embeddings to conduct retrieval using efficient approximate nearest neighbors methods has proven to work well. Meanwhile, there has been a growing interest in learning sparse representations for documents and queries, that could inherit from the desirable properties of bag-of-words models such as the exact matching of terms and the efficiency of inverted indexes. In this work, we present a new first-stage ranker based on explicit sparsity regularization and a log-saturation effect on term weights, leading to highly sparse representations and competitive results with respect to state-of-the-art dense and sparse methods. Our approach is simple, trained end-to-end in a single stage. We also explore the trade-off between effectiveness and efficiency, by controlling the contribution of the sparsity regularization.\"),\n",
        "  \"BIBREF96_9f753f67da834e59f9a5c8cdf9a88ee84c496b2d\": (\"Minimizing FLOPS to Learn Efficient Sparse Representations\", \"Deep representation learning has become one of the most widely adopted approaches for visual search, recommendation, and identification. Retrieval of such representations from a large database is however computationally challenging. Approximate methods based on learning compact representations, have been widely explored for this problem, such as locality sensitive hashing, product quantization, and PCA. In this work, in contrast to learning compact representations, we propose to learn high dimensional and sparse representations that have similar representational capacity as dense embeddings while being more efficient due to sparse matrix multiplication operations which can be much faster than dense multiplication. Following the key insight that the number of operations decreases quadratically with the sparsity of embeddings provided the non-zero entries are distributed uniformly across dimensions, we propose a novel approach to learn such distributed sparse embeddings via the use of a carefully constructed regularization function that directly minimizes a continuous relaxation of the number of floating-point operations (FLOPs) incurred during retrieval. Our experiments show that our approach is competitive to the other baselines and yields a similar or better speed-vs-accuracy tradeoff on practical datasets.\"),\n",
        "  \"BIBREF50_79cd9f77e5258f62c0e15d11534aea6393ef73fe\": (\"Dense Passage Retrieval for Open-Domain Question Answering\", \"Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.\"),\n",
        "  \"BIBREF58_fdb813d8b927bdd21ae1858cafa6c34b66a36268\": (\"Learning Deep Structured Semantic Models for Web Search using Clickthrough Data\", \"Latent semantic models, such as LSA, intend to map a query to its relevant documents at the semantic level where keyword-based matching often fails. In this study we strive to develop a series of new latent semantic models with a deep structure that project queries and documents into a common low-dimensional space where the relevance of a document given a query is readily computed as the distance between them. The proposed deep structured semantic models are discriminatively trained by maximizing the conditional likelihood of the clicked documents given a query using the clickthrough data. To make our models applicable to large-scale Web search applications, we also use a technique called word hashing, which is shown to effectively scale up our semantic models to handle large vocabularies which are common in such tasks. The new models are evaluated on a Web document ranking task using a real-world data set. Results show that our best model significantly outperforms other latent semantic models, which were considered state-of-the-art in the performance prior to the work presented in this paper.\"),\n",
        "  \"BIBREF53_bb2afd8172469fef7276e9789b306e085ed6e650\": (\"Real-time Inference in Multi-sentence with Deep Pretrained Transformers\", \"The use of deep pretrained bidirectional transformers has led to remarkable progress in learning multi-sentence representations for downstream language understanding tasks (Devlin et al., 2018). For tasks that make pairwise comparisons, e.g. matching a given context with a corresponding response, two approaches have permeated the literature. A Cross-encoder performs full self-attention over the pair; a Bi-encoder performs selfattention for each sequence separately, and the final representation is a function of the pair. While Cross-encoders nearly always outperform Bi-encoders on various tasks, both in our work and others’ (Urbanek et al., 2019), they are orders of magnitude slower, which hampers their ability to perform real-time inference. In this work, we develop a new architecture, the Poly-encoder, that is designed to approach the performance of the Crossencoder while maintaining reasonable computation time. Additionally, we explore two pretraining schemes with different datasets to determine how these affect the performance on our chosen dialogue tasks: ConvAI2 and DSTC7 Track 1. We show that our models achieve state-of-the-art results on both tasks; that the Poly-encoder is a suitable replacement for Bi-encoders and Cross-encoders; and that even better results can be obtained by pretraining on a large dialogue dataset.\"),\n",
        "  \"BIBREF54_050050e30d0f162c4dd87c1aac8d37df266e4c93\": (\"Sparse, Dense, and Attentional Representations for Text Retrieval\", \"Abstract Dual encoders perform retrieval by encoding documents and queries into dense low-dimensional vectors, scoring each document by its inner product with the query. We investigate the capacity of this architecture relative to sparse bag-of-words models and attentional neural networks. Using both theoretical and empirical analysis, we establish connections between the encoding dimension, the margin between gold and lower-ranked documents, and the document length, suggesting limitations in the capacity of fixed-length encodings to support precise retrieval of long documents. Building on these insights, we propose a simple neural model that combines the efficiency of dual encoders with some of the expressiveness of more costly attentional architectures, and explore sparse-dense hybrids to capitalize on the precision of sparse retrieval. These models outperform strong alternatives in large-scale retrieval.\"),\n",
        "  \"BIBREF55_60b8ad6177230ad5402af409a6edb5af441baeb4\": (\"ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT\", \"Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Crucially, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from millions of documents. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring up to four orders-of-magnitude fewer FLOPs per query.\"),\n",
        "  \"BIBREF56_2d7a784a093615d00d4ac0a7b5763a15d86d4996\": (\"COIL: Revisit Exact Lexical Match in Information Retrieval with Contextualized Inverted List\", \"Classical information retrieval systems such as BM25 rely on exact lexical match and can carry out search efficiently with inverted list index. Recent neural IR models shifts towards soft matching all query document terms, but they lose the computation efficiency of exact match systems. This paper presents COIL, a contextualized exact match retrieval architecture, where scoring is based on overlapping query document tokens’ contextualized representations. The new architecture stores contextualized token representations in inverted lists, bringing together the efficiency of exact match and the representation power of deep language models. Our experimental results show COIL outperforms classical lexical retrievers and state-of-the-art deep LM retrievers with similar or smaller latency.\"),\n",
        "  \"BIBREF57_4de571b44a7612189dded764f7a2988ac7529158\": (\"An Updated Set of Basic Linear Algebra\", \"\"),\n",
        "  \"BIBREF47_0c3bdbad193ec8a5b1f4005dc1496e341a2025b4\": (\"Efficient Document Re-Ranking for Transformers by Precomputing Term Representations\", \"Deep pretrained transformer networks are effective at various ranking tasks, such as question answering and ad-hoc document ranking. However, their computational expenses deem them cost-prohibitive in practice. Our proposed approach, called PreTTR (Precomputing Transformer Term Representations), considerably reduces the query-time latency of deep transformer networks (up to a 42x speedup on web document ranking) making these networks more practical to use in a real-time ranking scenario. Specifically, we precompute part of the document term representations at indexing time (without a query), and merge them with the query representation at query time to compute the final ranking score. Due to the large size of the token representations, we also propose an effective approach to reduce the storage requirement by training a compression layer to match attention scores. Our compression technique reduces the storage required up to 95% and it can be applied without a substantial degradation in ranking performance.\"),\n",
        "  \"BIBREF48_f7c455cc5a40d2a31b63ac2657c9d2d6c53b1be5\": (\"Learning to Speak and Act in a Fantasy Text Adventure Game\", \"We introduce a large-scale crowdsourced text adventure game as a research platform for studying grounded dialogue. In it, agents can perceive, emote, and act whilst conducting dialogue with other agents. Models and humans can both act as characters within the game. We describe the results of training state-of-the-art generative and retrieval models in this setting. We show that in addition to using past dialogue, these models are able to effectively use the state of the underlying world to condition their predictions. In particular, we show that grounding on the details of the local environment, including location descriptions, and the objects (and their affordances) and characters (and their previous actions) present within it allows better predictions of agent behavior and dialogue. We analyze the ingredients necessary for successful grounding in this setting, and how each of these factors relate to agents that can talk and act successfully.\"),\n",
        "  \"BIBREF49_997dc5d9a058753f034422afe7bd0cc0b8ad808b\": (\"Signature Verification using a \\\"Siamese\\\" Time Delay Neural Network\", \"This paper describes an algorithm for verification of signatures written on a pen-input tablet. The algorithm is based on a novel, artificial neural network, called a \\\"Siamese\\\" neural network. This network consists of two identical sub-networks joined at their outputs. During training the two sub-networks extract features from two signatures, while the joining neuron measures the distance between the two feature vectors. Verification consists of comparing an extracted feature vector with a stored feature vector for the signer. Signatures closer to this stored representation than a chosen threshold are accepted, all other signatures are rejected as forgeries.\"),\n",
        "  \"BIBREF51_c9b8593db099869fe7254aa1fa53f3c9073b0176\": (\"Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval\", \"Conducting text retrieval in a dense learned representation space has many intriguing advantages over sparse retrieval. Yet the effectiveness of dense retrieval (DR) often requires combination with sparse retrieval. In this paper, we identify that the main bottleneck is in the training mechanisms, where the negative instances used in training are not representative of the irrelevant documents in testing. This paper presents Approximate nearest neighbor Negative Contrastive Estimation (ANCE), a training mechanism that constructs negatives from an Approximate Nearest Neighbor (ANN) index of the corpus, which is parallelly updated with the learning process to select more realistic negative training instances. This fundamentally resolves the discrepancy between the data distribution used in the training and testing of DR. In our experiments, ANCE boosts the BERT-Siamese DR model to outperform all competitive dense and sparse retrieval baselines. It nearly matches the accuracy of sparse-retrieval-and-BERT-reranking using dot-product in the ANCE-learned representation space and provides almost 100x speed-up.\"),\n",
        "  \"BIBREF52_7b577ba0e4230b2ac58d297b3d2cfc3d2f1aaace\": (\"Optimizing Dense Retrieval Model Training with Hard Negatives\", \"Ranking has always been one of the top concerns in information retrieval researches. For decades, the lexical matching signal has dominated the ad-hoc retrieval process, but solely using this signal in retrieval may cause the vocabulary mismatch problem. In recent years, with the development of representation learning techniques, many researchers turn to Dense Retrieval (DR) models for better ranking performance. Although several existing DR models have already obtained promising results, their performance improvement heavily relies on the sampling of training examples. Many effective sampling strategies are not efficient enough for practical usage, and for most of them, there still lacks theoretical analysis in how and why performance improvement happens. To shed light on these research questions, we theoretically investigate different training strategies for DR models and try to explain why hard negative sampling performs better than random sampling. Through the analysis, we also find that there are many potential risks in static hard negative sampling, which is employed by many existing training methods. Therefore, we propose two training strategies named a Stable Training Algorithm for dense Retrieval (STAR) and a query-side training Algorithm for Directly Optimizing Ranking pErformance (ADORE), respectively. STAR improves the stability of DR training process by introducing random negatives. ADORE replaces the widely-adopted static hard negative sampling method with a dynamic one to directly optimize the ranking performance. Experimental results on two publicly available retrieval benchmark datasets show that either strategy gains significant improvements over existing competitive baselines and a combination of them leads to the best performance.\"),\n",
        "  \"BIBREF69_f17c6e164ccc7ec1ad91b3fbbafe8f84664e9803\": (\"Efficient K-Nearest Neighbor Graph Construction for Generic Similarity Measures\", \"K-Nearest Neighbor Graph (K-NNG) construction is an important operation with many web related applications, including collaborative filtering, similarity search, and many others in data mining and machine learning. Existing methods for K-NNG construction either do not scale, or are specific to certain similarity measures. We present NN-Descent, a simple yet efficient algorithm for approximate K-NNG construction with arbitrary similarity measures. Our method is based on local search, has minimal space overhead and does not rely on any shared global index. Hence, it is especially suitable for large-scale applications where data structures need to be distributed over the network. We have shown with a variety of datasets and similarity measures that the proposed method typically converges to above 90% recall with each point comparing only to several percent of the whole dataset on average.\"),\n",
        "  \"BIBREF70_c197ecb6a6987667cadcb498136989af1827cce0\": (\"Approximate Nearest Neighbor Algorithm based on Navigable Small World Graphs\", \"We propose a novel approach to solving the approximate k-nearest neighbor search problem in metric spaces. The search structure is based on a navigable small world graph with vertices corresponding to the stored elements, edges to links between them, and a variation of greedy algorithm for searching. The navigable small world is created simply by keeping old Delaunay graph approximation links produced at the start of construction. The approach is very universal, defined in terms of arbitrary metric spaces and at the same time it is very simple. The algorithm handles insertions in the same way as queries: by finding approximate neighbors for the inserted element and connecting it to them. Both search and insertion can be done in parallel requiring only local information from the structure. The structure can be made distributed. The accuracy of the probabilistic k-nearest neighbor queries can be adjusted without rebuilding the structure. The performed simulation for data in the Euclidean spaces shows that the structure built using the proposed algorithm has small world navigation properties with insertion and search complexity at fixed accuracy, and performs well at high dimensionality. Simulation on a CoPHiR dataset revealed its high efficiency in case of large datasets (more than an order of magnitude less metric computations at fixed recall) compared to permutation indexes. Only 0.03% of the 10 million 208-dimensional vector dataset is needed to be evaluated to achieve 0.999 recall (virtually exact search). For recall 0.93 processing speed 2800 queries/s can be achieved on a dual Intel X5675 Xenon server node with Java implementation.\"),\n",
        "  \"BIBREF71_e2e073433931c4d1a739f548b7d17b6e9b2fa13e\": (\"The Small-World Phenomenon - An Algorithmic Perspective\", \"Long a matter of folklore, the “small-world phenomenon” — the principle that we are all linked by short chains of acquaintances — was inaugurated as an area of experimental study in the social sciences through the pioneering work of Stanley Milgram in the 1960’s. This work was among the first to make the phenomenon quantitative, allowing people to speak of the “six degrees of separation” between any two people in the United States. Since then, a number of network models have been proposed as frameworks in which to study the problem analytically. One of the most refined of these models was formulated in recent work of Watts and Strogatz; their framework provided compelling evidence that the small-world phenomenon is pervasive in a range of networks arising in nature and technology, and a fundamental ingredient in the evolution of the World Wide Web. But existing models are insufficient to explain the striking algorithmic component of Milgram’s original findings: that individuals using local information are collectively very effective at actually constructing short paths between two points in a social network. Although recently proposed network models are rich in short paths, we prove that no decentralized algorithm, operating with local information only, can construct short paths in these networks with non-negligible probability. We then define an infinite family of network models that naturally generalizes the Watts-Strogatz model, and show that for one of these models, there is a decentralized algorithm capable of finding short paths with high probability. More generally, we provide a strong characterization of this family of network models, showing that there is in fact a unique model within the family for which decentralized algorithms are effective.\"),\n",
        "  \"BIBREF72_699a2e3b653c69aff5cf7a9923793b974f8ca164\": (\"Efficient and Robust Approximate Nearest Neighbor Search using Hierarchical Navigable Small World Graphs\", \"We present a new approach for the approximate K-nearest neighbor search based on navigable small world graphs with controllable hierarchy (Hierarchical NSW, HNSW). The proposed solution is fully graph-based, without any need for additional search structures (typically used at the coarse search stage of the most proximity graph techniques). Hierarchical NSW incrementally builds a multi-layer structure consisting of a hierarchical set of proximity graphs (layers) for nested subsets of the stored elements. The maximum layer in which an element is present is selected randomly with an exponentially decaying probability distribution. This allows producing graphs similar to the previously studied Navigable Small World (NSW) structures while additionally having the links separated by their characteristic distance scales. Starting the search from the upper layer together with utilizing the scale separation boosts the performance compared to NSW and allows a logarithmic complexity scaling. Additional employment of a heuristic for selecting proximity graph neighbors significantly increases performance at high recall and in case of highly clustered data. Performance evaluation has demonstrated that the proposed general metric space search index is able to strongly outperform previous opensource state-of-the-art vector-only approaches. Similarity of the algorithm to the skip list structure allows straightforward balanced distributed implementation.\"),\n",
        "  \"BIBREF64_1955266a8a58d94e41ad0efe20d707c92a069e95\": (\"Approximate Nearest Neighbors: Towards Removing the Curse of Dimensionality\", \"We present two algorithms for the approximate nearest neighbor problem in high-dimensional spaces. For data sets of size n living in R d , the algorithms require space that is only polynomial in n and d, while achieving query times that are sub-linear in n and polynomial in d. We also show applications to other high-dimensional geometric problems, such as the approximate minimum spanning tree. The article is based on the material from the authors' STOC'98 and FOCS'01 papers. It unifies, generalizes and simplifies the results from those papers.\"),\n",
        "  \"BIBREF65_3f1e54ed3bd801766e1897d53a9fc962524dd3c2\": (\"Locality-Sensitive Hashing Scheme Based on p-Stable Distributions\", \"We present a novel Locality-Sensitive Hashing scheme for the Approximate Nearest Neighbor Problem under lp norm, based on pstable distributions. Our scheme improves the running time of the earlier algorithm for the case of the l2 norm. It also yields the first known provably efficient approximate NN algorithm for the case p < 1. We also show that the algorithm finds the exact near neigbhor in O(log n) time for data satisfying certain “bounded growth” condition. Unlike earlier schemes, our LSH scheme works directly on points in the Euclidean space without embeddings. Consequently, the resulting query time bound is free of large factors and is simple and easy to implement. Our experiments (on synthetic data sets) show that the our data structure is up to 40 times faster than kd-tree.\"),\n",
        "  \"BIBREF66_9ed960374381062d85d3944182a539c1d00f7703\": (\"Multi-Probe LSH: Efficient Indexing for High-Dimensional Similarity Search\", \"Similarity indices for high-dimensional data are very desirable for building content-based search systems for feature-rich data such as audio, images, videos, and other sensor data. Recently, locality sensitive hashing (LSH) and its variations have been proposed as indexing techniques for approximate similarity search. A significant drawback of these approaches is the requirement for a large number of hash tables in order to achieve good search quality. This paper proposes a new indexing scheme called multi-probe LSH that overcomes this drawback. Multi-probe LSH is built on the well-known LSH technique, but it intelligently probes multiple buckets that are likely to contain query results in a hash table. Our method is inspired by and improves upon recent theoretical work on entropy-based LSH designed to reduce the space requirement of the basic LSH method. We have implemented the multi-probe LSH method and evaluated the implementation with two different high-dimensional datasets. Our evaluation shows that the multi-probe LSH method substantially improves upon previously proposed methods in both space and time efficiency. To achieve the same search quality, multi-probe LSH has a similar time-efficiency as the basic LSH method while reducing the number of hash tables by an order of magnitude. In comparison with the entropy-based LSH method, to achieve the same search quality, multi-probe LSH uses less query time and 5 to 8 times fewer number of hash tables.\"),\n",
        "  \"BIBREF62_5b0a88bdec473552c6a386cd94fdac53c74b79a8\": (\"On Symmetric and Asymmetric LSHs for Inner Product Search\", \"We consider the problem of designing locality sensitive hashes (LSH) for inner product similarity, and of the power of asymmetric hashes in this context. Shrivastava and Li argue that there is no symmetric LSH for the problem and propose an asymmetric LSH based on different mappings for query and database points. However, we show there does exist a simple symmetric LSH that enjoys stronger guarantees and better empirical performance than the asymmetric LSH they suggest. We also show a variant of the settings where asymmetry is in-fact needed, but there a different asymmetric LSH is required.\"),\n",
        "  \"BIBREF73_2cbb8de53759e75411bc528518947a3094fbce3a\": (\"Billion-Scale Similarity Search with GPUs\", \"Similarity search finds application in specialized database systems handling complex data such as images or videos, which are typically represented by high-dimensional features and require specific indexing structures. This paper tackles the problem of better utilizing GPUs for this task. While GPUs excel at data-parallel tasks, prior approaches are bottlenecked by algorithms that expose less parallelism, such as k-min selection, or make poor use of the memory hierarchy. We propose a design for k-selection that operates at up to 55% of theoretical peak performance, enabling a nearest neighbor implementation that is 8.5x faster than prior GPU state of the art. We apply it in different similarity search scenarios, by proposing optimized design for brute-force, approximate and compressed-domain search based on product quantization. In all these setups, we outperform the state of the art by large margins. Our implementation enables the construction of a high accuracy k-NN graph on 95 million images from the Yfcc100M dataset in 35 minutes, and of a graph connecting 1 billion vectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced our approach for the sake of comparison and reproducibility.\"),\n",
        "  \"BIBREF74_28336cbf2ee3e8fca6b173c91c5ca9628ba1fa4a\": (\"Joint Learning Deep Retrieval Model and Product Quantization based Embedding Index\", \"Embedding index that enables fast approximate nearest neighbor(ANN) search, serves as an indispensable component for state-of-the-art deep retrieval systems. Traditional approaches, often separating the two steps of embedding learning and index building, incur additional indexing time and decayed retrieval accuracy. In this paper, we propose a novel method called Poeem, which stands for product quantization based embedding index jointly trained with deep retrieval model, to unify the two separate steps within an end-to-end training, by utilizing a few techniques including the gradient straight-through estimator, warm start strategy, optimal space decomposition and Givens rotation. Extensive experimental results show that the proposed method not only improves retrieval accuracy significantly but also reduces the indexing time to almost none. We have open sourced our approach for the sake of comparison and reproducibility.\"),\n",
        "  \"BIBREF75_d6f83c915565f575e55fdce0424f65fe192af218\": (\"Jointly Optimizing Query Encoder and Product Quantization to Improve Retrieval Performance\", \"Recently, Information Retrieval community has witnessed fast-paced advances in Dense Retrieval (DR), which performs first-stage retrieval with embedding-based search. Despite the impressive ranking performance, previous studies usually adopt brute-force search to acquire candidates, which is prohibitive in practical Web search scenarios due to its tremendous memory usage and time cost. To overcome these problems, vector compression methods have been adopted in many practical embedding-based retrieval applications. One of the most popular methods is Product Quantization (PQ). However, although existing vector compression methods including PQ can help improve the efficiency of DR, they incur severely decayed retrieval performance due to the separation between encoding and compression. To tackle this problem, we present JPQ, which stands for Joint optimization of query encoding and Product Quantization. It trains the query encoder and PQ index jointly in an end-to-end manner based on three optimization strategies, namely ranking-oriented loss, PQ centroid optimization, and end-to-end negative sampling. We evaluate JPQ on two publicly available retrieval benchmarks. Experimental results show that JPQ significantly outperforms popular vector compression methods. Compared with previous DR models that use brute-force search, JPQ almost matches the best retrieval performance with 30x compression on index size. The compressed index further brings 10x speedup on CPU and 2x speedup on GPU in query latency.\"),\n",
        "  \"BIBREF76_91429255eefe48ad140ccfaf6aa1e6be11a72a53\": (\"Learning Discrete Representations via Constrained Clustering for Effective and Efficient Dense Retrieval\", \"Dense Retrieval (DR) has achieved state-of-the-art first-stage ranking effectiveness. However, the efficiency of most existing DR models is limited by the large memory cost of storing dense vectors and the time-consuming nearest neighbor search (NNS) in vector space. Therefore, we present RepCONC, a novel retrieval model that learns discrete Representations via CONstrained Clustering. RepCONC jointly trains dual-encoders and the Product Quantization (PQ) method to learn discrete document representations and enables fast approximate NNS with compact indexes. It models quantization as a constrained clustering process, which requires the document embeddings to be uniformly clustered around the quantization centroids and supports end-to-end optimization of the quantization method and dual-encoders. We theoretically demonstrate the importance of the uniform clustering constraint in RepCONC and derive an efficient approximate solution for constrained clustering by reducing it to an instance of the optimal transport problem. Besides constrained clustering, RepCONC further adopts a vector-based inverted file system (IVF) to support highly efficient vector search on CPUs. Extensive experiments on two popular ad-hoc retrieval benchmarks show that RepCONC achieves better ranking effectiveness than competitive vector quantization baselines under different compression ratio settings. It also substantially outperforms a wide range of existing retrieval models in terms of retrieval effectiveness, memory efficiency, and time efficiency.\"),\n",
        "  \"BIBREF77_c3cf35677834fb535d3bc7cf8d375366df4b1397\": (\"Query Embedding Pruning for Dense Retrieval\", \"Recent advances in dense retrieval techniques have offered the promise of being able not just to re-rank documents using contextualised language models such as BERT, but also to use such models to identify documents from the collection in the first place. However, when using dense retrieval approaches that use multiple embedded representations for each query, a large number of documents can be retrieved for each query, hindering the efficiency of the method. Hence, this work is the first to consider efficiency improvements in the context of a dense retrieval approach (namely ColBERT), by pruning query term embeddings that are estimated not to be useful for retrieving relevant documents. Our proposed query embeddings pruning reduces the cost of the dense retrieval operation, as well as reducing the number of documents that are retrieved and hence require to be fully scored. Experiments conducted on the MSMARCO passage ranking corpus demonstrate that, when reducing the number of query embeddings used from 32 to 3 based on the collection frequency of the corresponding tokens, query embedding pruning results in no statistically significant differences in effectiveness, while reducing the number of documents retrieved by 70%. In terms of mean response time for the end-to-end to end system, this results in a 2.65x speedup.\"),\n",
        "  \"BIBREF78_2408c965a3855a6b66d128195c783d76e2e939da\": (\"On Approximate Nearest Neighbour Selection for Multi-Stage Dense Retrieval\", \"Dense retrieval, which describes the use of contextualised language models such as BERT to identify documents from a collection by leveraging approximate nearest neighbour (ANN) techniques, has been increasing in popularity. Two families of approaches have emerged, depending on whether documents and queries are represented by single or multiple embeddings. ColBERT, the exemplar of the latter, uses an ANN index and approximate scores to identify a set of candidate documents for each query embedding, which are then re-ranked using accurate document representations. In this manner, a large number of documents can be retrieved for each query, hindering the efficiency of the approach. In this work, we investigate the use of ANN scores for ranking the candidate documents, in order to decrease the number of candidate documents being fully scored. Experiments conducted on the MSMARCO passage ranking corpus demonstrate that, by cutting of the candidate set by using the approximate scores to only 200 documents, we can still obtain an effective ranking without statistically significant differences in effectiveness, and resulting in a 2x speedup in efficiency.\"),\n",
        "  \"BIBREF67_c564aa7639a08c280423489e52b6e32055c9aa7f\": (\"Vector Quantization and Signal Compression\", \"\"),\n",
        "  \"BIBREF68_4748d22348e72e6e06c2476486afddbc76e5eca7\": (\"Product Quantization for Nearest Neighbor Search\", \"This paper introduces a product quantization-based approach for approximate nearest neighbor search. The idea is to decompose the space into a Cartesian product of low-dimensional subspaces and to quantize each subspace separately. A vector is represented by a short code composed of its subspace quantization indices. The euclidean distance between two vectors can be efficiently estimated from their codes. An asymmetric version increases precision, as it computes the approximate distance between a vector and a code. Experimental results show that our approach searches for nearest neighbors efficiently, in particular in combination with an inverted file system. Results for SIFT and GIST image descriptors show excellent search accuracy, outperforming three state-of-the-art approaches. The scalability of our approach is validated on a data set of two billion vectors.\"),\n",
        "  \"BIBREF7_514bd50d54908ff49cc6ec41386e4b7a5d606e0f\": (\"The Probability Ranking Principal in IR\", \"The principle that, for optimal retrieval, documents should be ranked in order of the probability of relevance or usefulness has been brought into question by Cooper. It is shown that the principle can be justified under certain assumptions, but that in cases where these assumptions do not hold, the principle is not valid. The major problem appears to lie in the way the principle considers each document independently of the rest. The nature of the information on the basis of which the system decides whether or not to retrieve the documents determines whether the document-by-document approach is valid.\"),\n",
        "  \"BIBREF8_47354d4d1915ae3d286d401005ba8a44af7d1fa5\": (\"A Deep Look into Neural Ranking Models for Information Retrieval\", \"Ranking models lie at the heart of research on information retrieval (IR). During the past decades, different techniques have been proposed for constructing ranking models, from traditional heuristic methods, probabilistic methods, to modern machine learning methods. Recently, with the advance of deep learning technology, we have witnessed a growing body of work in applying shallow or deep neural networks to the ranking problem in IR, referred to as neural ranking models in this paper. The power of neural ranking models lies in the ability to learn from the raw text inputs for the ranking problem to avoid many limitations of hand-crafted features. Neural networks have sufficient capacity to model complicated tasks, which is needed to handle the complexity of relevance estimation in ranking. Since there have been a large variety of neural ranking models proposed, we believe it is the right time to summarize the current status, learn from existing methodologies, and gain some insights for future development. In contrast to existing reviews, in this survey, we will take a deep look into the neural ranking models from different dimensions to analyze their underlying assumptions, major design principles, and learning strategies. We compare these models through benchmark tasks to obtain a comprehensive empirical understanding of the existing techniques. We will also discuss what is missing in the current literature and what are the promising and desired future directions.\"),\n",
        "  \"BIBREF0_5f3b50c6c826ad105163b09d53e1eb498a4b3994\": (\"Books Reviews - Introduction to Information Retrieval\", \"\"),\n",
        "  \"BIBREF5_3cf0822f63e51be5343028bad7ee72a5882ef7de\": (\"Scalability Challenges in Web Search Engines\", \"Continuous growth of the Web and user bases forces web search engine companies to make costly investments on very large compute infrastructures. The scalability of these infrastructures requires careful performance optimizations in every major component of the search engine. Herein, we try to provide a fairly comprehensive coverage of the literature on scalability challenges in large-scale web search engines. We present the identified challenges through an architectural classification, starting from a simple single-node search system and moving towards a hypothetical multi-site web search architecture. We also discuss a number of open research problems and provide recommendations to researchers in the field.\"),\n",
        "  \"BIBREF9_d5f169880e30e1f76827d72f862555d00b01bed9\": (\"A Vector Space Model for Automatic Indexing\", \"In a document retrieval, or other pattern matching environment where stored entities (documents) are compared with each other or with incoming patterns (search requests), it appears that the best indexing (property) space is one where each entity lies as far away from the others as possible; in these circumstances the value of an indexing system may be expressible as a function of the density of the object space; in particular, retrieval performance may correlate inversely with space density. An approach based on space density computations is used to choose an optimum indexing vocabulary for a collection of documents. Typical evaluation results are shown, demonstating the usefulness of the model.\"),\n",
        "  \"BIBREF10_47ced790a563344efae66588b5fb7fe6cca29ed3\": (\"The Probabilistic Relevance Framework: BM25 and Beyond\", \"The Probabilistic Relevance Framework (PRF) is a formal framework for document retrieval, grounded in work done in the 1970—1980s, which led to the development of one of the most successful text-retrieval algorithms, BM25. In recent years, research in the PRF has yielded new retrieval models capable of taking into account document meta-data (especially structure and link-graph information). Again, this has led to one of the most successful Web-search and corporate-search algorithms, BM25F. This work presents the PRF from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the different ranking algorithms that result from its application: the binary independence model, relevance feedback models, BM25 and BM25F. It also discusses the relation between the PRF and other statistical models for IR, and covers some related topics, such as the use of non-textual features, and parameter optimisation for models with free parameters.\"),\n",
        "  \"BIBREF11_73a76dd71abfbd29dbba4ea034ab52284626aa71\": (\"A Language Modeling Approach to Information Retrieval\", \"Models of document indexing and document retrieval have been extensively studied. The integration of these two classes of models has been the goal of several researchers but it is a very difficult problem. We argue that much of the reason for this is the lack of an adequate indexing model. This suggests that perhaps a better indexing model would help solve the problem. However, we feel that making unwarranted parametric assumptions will not lead to better retrieval performance. Furthermore, making prior assumptions about the similarity of documents is not warranted either. Instead, we propose an approach to retrieval based on probabilistic language modeling. We estimate models for each document individually. Our approach to modeling is non-parametric and integrates document indexing and document retrieval into a single model. One advantage of our approach is that collection statistics which are used heuristically in many other retrieval models are an integral part of our model. We have implemented our model and tested it empirically. Our approach significantly outperforms standard tf.idf weighting on two different collections and query sets.\"),\n",
        "  \"BIBREF4_5fc5c5a4e489e781de434567d946e6eb65c44f60\": (\"Learning to Rank for Information Retrieval\", \"\"),\n",
        "  \"BIBREF12_008f1d2741ebef51e6400686b050e046455b52fb\": (\"The Whens and Hows of Learning to Rank for Web Search\", \"Web search engines are increasingly deploying many features, combined using learning to rank techniques. However, various practical questions remain concerning the manner in which learning to rank should be deployed. For instance, a sample of documents with sufficient recall is used, such that re-ranking of the sample by the learned model brings the relevant documents to the top. However, the properties of the document sample such as when to stop ranking—i.e. its minimum effective size—remain unstudied. Similarly, effective listwise learning to rank techniques minimise a loss function corresponding to a standard information retrieval evaluation measure. However, the appropriate choice of how to calculate the loss function—i.e. the choice of the learning evaluation measure and the rank depth at which this measure should be calculated—are as yet unclear. In this paper, we address all of these issues by formulating various hypotheses and research questions, before performing exhaustive experiments using multiple learning to rank techniques and different types of information needs on the ClueWeb09 and LETOR corpora. Among many conclusions, we find, for instance, that the smallest effective sample for a given query set is dependent on the type of information need of the queries, the document representation used during sampling and the test evaluation measure. As the sample size is varied, the selected features markedly change—for instance, we find that the link analysis features are favoured for smaller document samples. Moreover, despite reflecting a more realistic user model, the recently proposed ERR measure is not as effective as the traditional NDCG as a learning loss function. Overall, our comprehensive experiments provide the first empirical derivation of best practices for learning to rank deployments.\"),\n",
        "  \"BIBREF14_6adb7a21e59d962c6420471d151602cdfbec48a8\": (\"\", \"\"),\n",
        "  \"BIBREF15_0df9c70875783a73ce1e933079f328e8cf5e9ea2\": (\"From RankNet to LambdaRank to LabdaMART: An Overview\", \"LambdaMART is the boosted tree versin of LambdaRank, which is based on RankNet. RankNet, LambdaRank, and LambdaMART have proven to be very successful algorithms for solving real world ranking problems: for example an ensemble of LambdaMART rankers won Track 1 of the 2010 Yahoo! Learning To Rank Challenge. The details of these algorithms are spread across several papers and reports, and so here we give a self-contained, detailed and complete description of them.\"),\n",
        "  \"BIBREF16_63aaf12163fe9735dfe9a69114937c4fa34f303a\": (\"Learning to Rank using Gradient Descent\", \"We investigate using gradient descent methods for learning ranking functions; we propose a simple probabilistic cost function, and we introduce RankNet, an implementation of these ideas using a neural network to model the underlying ranking function. We present test results on toy data and on data from a commercial internet search engine.\"),\n",
        "  \"BIBREF17_5303f288c0de1fc717c3389773a2a684589ee46b\": (\"Semantic Memory Search and Retrieval in a Novel Cooperative Word Game: A Comparison of Associative and Distributional Semantic Models\", \"Considerable work during the past two decades has focused on modeling the structure of semantic memory, although the performance of these models in complex and unconstrained semantic tasks remains relatively understudied. We introduce a two-player cooperative word game, Connector (based on the boardgame Codenames), and investigate whether similarity metrics derived from two large databases of human free association norms, the University of South Florida norms and the Small World of Words norms, and two distributional semantic models based on large language corpora (word2vec and GloVe) predict performance in this game. Participant dyads were presented with 20-item word boards with word pairs of varying relatedness. The speaker received a word pair from the board (e.g., exam-algebra) and generated a one-word semantic clue (e.g., math), which was used by the guesser to identify the word pair on the board across three attempts. Response times to generate the clue, as well as accuracy and latencies for the guessed word pair, were strongly predicted by the cosine similarity between word pairs and clues in random walk-based associative models, and to a lesser degree by the distributional models, suggesting that conceptual representations activated during free association were better able to capture search and retrieval processes in the game. Further, the speaker adjusted subsequent clues based on the first attempt by the guesser, who in turn benefited from the adjustment in clues, suggesting a cooperative influence in the game that was effectively captured by both associative and distributional models. These results indicate that both associative and distributional models can capture relatively unconstrained search processes in a cooperative game setting, and Connector is particularly suited to examine communication and semantic search processes.\"),\n",
        "  \"BIBREF18_87f40e6f3022adbc1f1905e3e506abad05a9964f\": (\"Distributed Representations of Words and Phrases and their Compositionality\", \"The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of \\\"Canada\\\" and \\\"Air\\\" cannot be easily combined to obtain \\\"Air Canada\\\". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.\"),\n",
        "  \"BIBREF19_892e53fe5cd39f037cb2a961499f42f3002595dd\": (\"Bag of Tricks for Efficient Text Classification\", \"This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore CPU, and classify half a million sentences among 312K classes in less than a minute.\"),\n",
        "  \"BIBREF20_f37e1b62a767a307c046404ca96bc140b3e68cb5\": (\"GloVe: Global Vectors for Word Representation\", \"Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.\"),\n",
        "  \"BIBREF23_cd18800a0fe0b668a1cc19f2ec95b5003d0a5035\": (\"Improving Language Understanding by Generative Pre-Training\", \"Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test), 5.7% on question answering (RACE), and 1.5% on textual entailment (MultiNLI).\"),\n",
        "}"
      ],
      "metadata": {
        "id": "TLzsfO2wSihp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def find(name, path):\n",
        "  for root, dirs, files in os.walk(path):\n",
        "    if name in files:\n",
        "      return os.path.join(root, name)\n",
        "\n",
        "find('BIBREF23_cd18800a0fe0b668a1cc19f2ec95b5003d0a5035.pdf', folder_papers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "_pFWLkk-VVF_",
        "outputId": "7a737078-8609-498b-f11f-73df3c18e54b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./papers_pdf/paper_section/Text_Representations_for_Ranking-Word_Embeddings/BIBREF23_cd18800a0fe0b668a1cc19f2ec95b5003d0a5035.pdf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pypdfium2 as pdfium\n",
        "\n",
        "def extract_text(paperId):\n",
        "  pdf_file = find(f'{paperId}.pdf', folder_papers)\n",
        "  txt_contents = ''\n",
        "\n",
        "  print(f\"Extracting {paperId}\")\n",
        "  try:\n",
        "    pdf = pdfium.PdfDocument(pdf_file)\n",
        "    for i in range(len(pdf)):\n",
        "      txt_contents += pdf[i].get_textpage().get_text_range()\n",
        "\n",
        "    # Remove the break lines and considers only one big string of text:\n",
        "    txt_contents = txt_contents.replace('\\r\\n', ' ')\n",
        "    txt_contents = txt_contents.replace('\\n', ' ')\n",
        "    # Remove everything before introduction and reference section:\n",
        "    txt_contents_lower = txt_contents.lower()\n",
        "    idx_introduction = max(txt_contents_lower.find('introduction'), 0)\n",
        "    idx_references = max(txt_contents_lower.rfind('reference'), 0)\n",
        "    txt_contents = txt_contents[idx_introduction:idx_references]\n",
        "\n",
        "    return txt_contents\n",
        "  except:\n",
        "    print(f'***** Problems with {pdf_file}. Ignoring...')\n",
        "    return ''"
      ],
      "metadata": {
        "id": "mq1byGkQht-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "papers = {}\n",
        "for paperId, title_abstract in titles_and_abstracts_by_id.items():\n",
        "  title = title_abstract[0]\n",
        "  abstract = title_abstract[1]\n",
        "  text = extract_text(paperId)\n",
        "  if text == '':\n",
        "    continue\n",
        "  papers[paperId] = {\n",
        "      \"metadata\": {\n",
        "          \"paperId\": paperId,\n",
        "          \"abstract\": abstract,\n",
        "          \"title\": title\n",
        "      },\n",
        "      \"title\": title,\n",
        "      \"abstract\": abstract,\n",
        "      \"text\": text\n",
        "  }"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYrwgI1JTBSX",
        "outputId": "a0a22ef8-8f94-41df-ee51-a5d03f10d729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting BIBREF6_2c953a3c378b40dadf2e3fb486713c8608b8e282\n",
            "Extracting BIBREF97_b97a33933541c276778c3fe63baad6964f4bdf44\n",
            "Extracting BIBREF98_3df1aa7f82e150662ab8e87b78e64c8238997f1c\n",
            "Extracting BIBREF24_563e821bb5ea825efb56b77484f5287f08cf3753\n",
            "Extracting BIBREF25_d51ed05fd05b9d222427a05a87ed88217447b44f\n",
            "Extracting BIBREF26_ea738439b880ad033ff01602ea52d04b366d0d37\n",
            "Extracting BIBREF27_fc3384d631f5e2b2a9d66623d4d3e1d28b96dee7\n",
            "Extracting BIBREF28_32e7f0863e7c56cfced89abedaee46e2288bc127\n",
            "Extracting BIBREF45_7a31e2dcbaa1cf6e9f76084793a02a2a4e4c2d15\n",
            "Extracting BIBREF46_afed54533ecc624cb5e0241172268c6188ded20c\n",
            "Extracting BIBREF44_d1a2d203733208deda7427c8e20318334193d9d7\n",
            "Extracting BIBREF21_df2b0e26d0599ce3e70df8a9da02e51594e0e992\n",
            "Extracting BIBREF22_077f8329a7b6fa3b7c877a57b81eb6c18b5f87de\n",
            "Extracting BIBREF34_a54b56af24bb4873ed0163b77df63b92bd018ddc\n",
            "Extracting BIBREF35_395de0bd3837fdf4b4b5e5f04835bcc69c279481\n",
            "Extracting BIBREF36_3cfb319689f06bf04c2e28399361f414ca32c4b3\n",
            "Extracting BIBREF41_9405cc0d6169988371b2755e573cc28650d14dfe\n",
            "Extracting BIBREF42_d0086b86103a620a86bc918746df0aa642e2a8a3\n",
            "Extracting BIBREF43_f6e0164466e827112fd415afdc28ddf8e0eb1ba3\n",
            "Extracting BIBREF37_dbde7dfa6cae81df8ac19ef500c42db96c3d1edd\n",
            "Extracting BIBREF38_85e07116316e686bf787114ba10ca60f4ea7c5b2\n",
            "Extracting BIBREF39_1ec78c0ec945572673fabd50bf263870fe9d3601\n",
            "Extracting BIBREF40_63a2fabbe4b1615a84d5f4d90987733cf09e3ff8\n",
            "Extracting BIBREF79_1225eb6570ce8d45067329fafcc8ff7636a65923\n",
            "Extracting BIBREF81_7f8cbf4453062af8d5c8cf1a94ff01867d122121\n",
            "Extracting BIBREF82_b092b6b843e9421bf42bf96f57ed4658a3e0bdf7\n",
            "Extracting BIBREF83_54fa64b74ec020699fad989f85e74e50c7a34445\n",
            "Extracting BIBREF84_204e3073870fae3d05bcbc2f6a8e263d9b72e776\n",
            "Extracting BIBREF85_29de7c0fb3c09eaf55b20619bceaeafe72fd87a6\n",
            "Extracting BIBREF86_ea6838915acc4e7764caa8f2fdae8dc4629f924b\n",
            "Extracting BIBREF87_57a07372e2a620d6ae920f74877eee5f61753a96\n",
            "Extracting BIBREF3_629f50daebbb9003f645f671f76cc6b33088c17d\n",
            "Extracting BIBREF88_1c68eefcceb042fb79712aed347370d6ce7190c1\n",
            "Extracting BIBREF89_3de1752cd0854e220fc41f0ccf7db913f846284c\n",
            "Extracting BIBREF91_4aa1d28944856ebe1950a27f633c6667ead3cbf8\n",
            "Extracting BIBREF93_89d373d61c68465fd49da1257aa959e5abefd155\n",
            "Extracting BIBREF94_0c57dcf959ead9530f9ec3ebe0dd58de42a3e8af\n",
            "Extracting BIBREF95_1e8a6de5561f557ff9abf43d538d8d5e9347efa0\n",
            "Extracting BIBREF96_9f753f67da834e59f9a5c8cdf9a88ee84c496b2d\n",
            "Extracting BIBREF50_79cd9f77e5258f62c0e15d11534aea6393ef73fe\n",
            "Extracting BIBREF58_fdb813d8b927bdd21ae1858cafa6c34b66a36268\n",
            "Extracting BIBREF53_bb2afd8172469fef7276e9789b306e085ed6e650\n",
            "Extracting BIBREF54_050050e30d0f162c4dd87c1aac8d37df266e4c93\n",
            "Extracting BIBREF55_60b8ad6177230ad5402af409a6edb5af441baeb4\n",
            "Extracting BIBREF56_2d7a784a093615d00d4ac0a7b5763a15d86d4996\n",
            "Extracting BIBREF57_4de571b44a7612189dded764f7a2988ac7529158\n",
            "Extracting BIBREF47_0c3bdbad193ec8a5b1f4005dc1496e341a2025b4\n",
            "Extracting BIBREF48_f7c455cc5a40d2a31b63ac2657c9d2d6c53b1be5\n",
            "Extracting BIBREF49_997dc5d9a058753f034422afe7bd0cc0b8ad808b\n",
            "Extracting BIBREF51_c9b8593db099869fe7254aa1fa53f3c9073b0176\n",
            "Extracting BIBREF52_7b577ba0e4230b2ac58d297b3d2cfc3d2f1aaace\n",
            "Extracting BIBREF69_f17c6e164ccc7ec1ad91b3fbbafe8f84664e9803\n",
            "Extracting BIBREF70_c197ecb6a6987667cadcb498136989af1827cce0\n",
            "Extracting BIBREF71_e2e073433931c4d1a739f548b7d17b6e9b2fa13e\n",
            "Extracting BIBREF72_699a2e3b653c69aff5cf7a9923793b974f8ca164\n",
            "Extracting BIBREF64_1955266a8a58d94e41ad0efe20d707c92a069e95\n",
            "Extracting BIBREF65_3f1e54ed3bd801766e1897d53a9fc962524dd3c2\n",
            "Extracting BIBREF66_9ed960374381062d85d3944182a539c1d00f7703\n",
            "Extracting BIBREF62_5b0a88bdec473552c6a386cd94fdac53c74b79a8\n",
            "Extracting BIBREF73_2cbb8de53759e75411bc528518947a3094fbce3a\n",
            "Extracting BIBREF74_28336cbf2ee3e8fca6b173c91c5ca9628ba1fa4a\n",
            "Extracting BIBREF75_d6f83c915565f575e55fdce0424f65fe192af218\n",
            "Extracting BIBREF76_91429255eefe48ad140ccfaf6aa1e6be11a72a53\n",
            "Extracting BIBREF77_c3cf35677834fb535d3bc7cf8d375366df4b1397\n",
            "Extracting BIBREF78_2408c965a3855a6b66d128195c783d76e2e939da\n",
            "Extracting BIBREF67_c564aa7639a08c280423489e52b6e32055c9aa7f\n",
            "Extracting BIBREF68_4748d22348e72e6e06c2476486afddbc76e5eca7\n",
            "Extracting BIBREF7_514bd50d54908ff49cc6ec41386e4b7a5d606e0f\n",
            "Extracting BIBREF8_47354d4d1915ae3d286d401005ba8a44af7d1fa5\n",
            "Extracting BIBREF0_5f3b50c6c826ad105163b09d53e1eb498a4b3994\n",
            "***** Problems with None. Ignoring...\n",
            "Extracting BIBREF5_3cf0822f63e51be5343028bad7ee72a5882ef7de\n",
            "Extracting BIBREF9_d5f169880e30e1f76827d72f862555d00b01bed9\n",
            "Extracting BIBREF10_47ced790a563344efae66588b5fb7fe6cca29ed3\n",
            "Extracting BIBREF11_73a76dd71abfbd29dbba4ea034ab52284626aa71\n",
            "Extracting BIBREF4_5fc5c5a4e489e781de434567d946e6eb65c44f60\n",
            "Extracting BIBREF12_008f1d2741ebef51e6400686b050e046455b52fb\n",
            "Extracting BIBREF14_6adb7a21e59d962c6420471d151602cdfbec48a8\n",
            "***** Problems with ./papers_pdf/paper_section/Text_Representations_for_Ranking-LTR_Features/BIBREF14_6adb7a21e59d962c6420471d151602cdfbec48a8.pdf. Ignoring...\n",
            "Extracting BIBREF15_0df9c70875783a73ce1e933079f328e8cf5e9ea2\n",
            "Extracting BIBREF16_63aaf12163fe9735dfe9a69114937c4fa34f303a\n",
            "Extracting BIBREF17_5303f288c0de1fc717c3389773a2a684589ee46b\n",
            "Extracting BIBREF18_87f40e6f3022adbc1f1905e3e506abad05a9964f\n",
            "Extracting BIBREF19_892e53fe5cd39f037cb2a961499f42f3002595dd\n",
            "Extracting BIBREF20_f37e1b62a767a307c046404ca96bc140b3e68cb5\n",
            "Extracting BIBREF23_cd18800a0fe0b668a1cc19f2ec95b5003d0a5035\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open(file_papers_contents, 'wb') as f:\n",
        "  pickle.dump(papers, f)"
      ],
      "metadata": {
        "id": "l13GPFt_WEpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'BIBREF10_47ced790a563344efae66588b5fb7fe6cca29ed3' in papers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOZBjOo3ge8v",
        "outputId": "32709f1e-0932-447b-9f92-fe1b22450183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_papers(ids):\n",
        "  result = []\n",
        "  for id in ids:\n",
        "    if id in papers:\n",
        "      result.append(papers[id]['metadata'])\n",
        "\n",
        "  return result\n",
        "\n",
        "sections = [\n",
        "  {\n",
        "    \"section_title\": \"Text Representations for Ranking\",\n",
        "    \"papers\": [],\n",
        "    \"subsections\": [\n",
        "      {\n",
        "          \"section_title\": \"BOW Encodings\",\n",
        "          \"papers\": get_papers([\"BIBREF10_47ced790a563344efae66588b5fb7fe6cca29ed3\", \"BIBREF11_73a76dd71abfbd29dbba4ea034ab52284626aa71\", \"BIBREF5_3cf0822f63e51be5343028bad7ee72a5882ef7de\", \"BIBREF5_3cf0822f63e51be5343028bad7ee72a5882ef7de\"]),\n",
        "          \"subsections\": []\n",
        "      },\n",
        "      {\n",
        "          \"section_title\": \"LTR Features\",\n",
        "          \"papers\": get_papers([\"BIBREF4_5fc5c5a4e489e781de434567d946e6eb65c44f60\", \"BIBREF16_63aaf12163fe9735dfe9a69114937c4fa34f303a\", \"BIBREF15_0df9c70875783a73ce1e933079f328e8cf5e9ea2\", \"BIBREF14_6adb7a21e59d962c6420471d151602cdfbec48a8\", \"BIBREF12_008f1d2741ebef51e6400686b050e046455b52fb\"]),\n",
        "          \"subsections\": []\n",
        "      },\n",
        "      {\n",
        "          \"section_title\": \"Word Embeddings\",\n",
        "          \"papers\": get_papers([\"BIBREF23_cd18800a0fe0b668a1cc19f2ec95b5003d0a5035\", \"BIBREF22_077f8329a7b6fa3b7c877a57b81eb6c18b5f87de\", \"BIBREF21_df2b0e26d0599ce3e70df8a9da02e51594e0e992\", \"BIBREF20_f37e1b62a767a307c046404ca96bc140b3e68cb5\", \"BIBREF19_892e53fe5cd39f037cb2a961499f42f3002595dd\", \"BIBREF18_87f40e6f3022adbc1f1905e3e506abad05a9964f\", \"BIBREF17_5303f288c0de1fc717c3389773a2a684589ee46b\"]),\n",
        "          \"subsections\": []\n",
        "      }\n",
        "    ]\n",
        "  },\n",
        "  {\n",
        "    \"section_title\": \"Interaction-focused Systems\",\n",
        "    \"papers\": [],\n",
        "    \"subsections\": [\n",
        "      {\n",
        "          \"section_title\": \"Convolutional Neural Networks\",\n",
        "          \"papers\": get_papers([\"BIBREF28_32e7f0863e7c56cfced89abedaee46e2288bc127\", \"BIBREF27_fc3384d631f5e2b2a9d66623d4d3e1d28b96dee7\", \"BIBREF26_ea738439b880ad033ff01602ea52d04b366d0d37\", \"BIBREF25_d51ed05fd05b9d222427a05a87ed88217447b44f\", \"BIBREF24_563e821bb5ea825efb56b77484f5287f08cf3753\"]),\n",
        "          \"subsections\": []\n",
        "      },\n",
        "      {\n",
        "          \"section_title\": \"Pre-trained Language Models\",\n",
        "          \"papers\": get_papers([\"BIBREF21_df2b0e26d0599ce3e70df8a9da02e51594e0e992\", \"BIBREF22_077f8329a7b6fa3b7c877a57b81eb6c18b5f87de\", \"BIBREF34_a54b56af24bb4873ed0163b77df63b92bd018ddc\", \"BIBREF35_395de0bd3837fdf4b4b5e5f04835bcc69c279481\", \"BIBREF36_3cfb319689f06bf04c2e28399361f414ca32c4b3\"]),\n",
        "          \"subsections\": []\n",
        "      },\n",
        "      {\n",
        "          \"section_title\": \"Ranking with Encoder-only Models\",\n",
        "          \"papers\": get_papers([\"BIBREF37_dbde7dfa6cae81df8ac19ef500c42db96c3d1edd\", \"BIBREF38_85e07116316e686bf787114ba10ca60f4ea7c5b2\", \"BIBREF39_1ec78c0ec945572673fabd50bf263870fe9d3601\", \"BIBREF40_63a2fabbe4b1615a84d5f4d90987733cf09e3ff8\"]),\n",
        "          \"subsections\": []\n",
        "      },\n",
        "      {\n",
        "          \"section_title\": \"Ranking with Encoder-decoder Models\",\n",
        "          \"papers\": get_papers([\"BIBREF43_f6e0164466e827112fd415afdc28ddf8e0eb1ba3\", \"BIBREF42_d0086b86103a620a86bc918746df0aa642e2a8a3\", \"BIBREF41_9405cc0d6169988371b2755e573cc28650d14dfe\", \"BIBREF36_3cfb319689f06bf04c2e28399361f414ca32c4b3\"]),\n",
        "          \"subsections\": []\n",
        "      },\n",
        "      {\n",
        "          \"section_title\": \"Fine-tuning Interaction-focused Systems\",\n",
        "          \"papers\": get_papers([\"BIBREF44_d1a2d203733208deda7427c8e20318334193d9d7\"]),\n",
        "          \"subsections\": []\n",
        "      },\n",
        "      {\n",
        "          \"section_title\": \"Dealing with long texts\",\n",
        "          \"papers\": get_papers([\"BIBREF45_7a31e2dcbaa1cf6e9f76084793a02a2a4e4c2d15\", \"BIBREF46_afed54533ecc624cb5e0241172268c6188ded20c\"]),\n",
        "          \"subsections\": []\n",
        "      },\n",
        "    ]\n",
        "  },\n",
        "  {\n",
        "    \"section_title\": \"Representation-focused Systems\",\n",
        "    \"papers\": [],\n",
        "    \"subsections\": [\n",
        "      {\n",
        "          \"section_title\": \"Single Representations\",\n",
        "          \"papers\": get_papers([\"BIBREF3_629f50daebbb9003f645f671f76cc6b33088c17d\", \"BIBREF39_1ec78c0ec945572673fabd50bf263870fe9d3601\", \"BIBREF47_0c3bdbad193ec8a5b1f4005dc1496e341a2025b4\", \"BIBREF48_f7c455cc5a40d2a31b63ac2657c9d2d6c53b1be5\", \"BIBREF49_997dc5d9a058753f034422afe7bd0cc0b8ad808b\", \"BIBREF50_79cd9f77e5258f62c0e15d11534aea6393ef73fe\", \"BIBREF51_c9b8593db099869fe7254aa1fa53f3c9073b0176\", \"BIBREF52_7b577ba0e4230b2ac58d297b3d2cfc3d2f1aaace\"]),\n",
        "          \"subsections\": []\n",
        "      },\n",
        "      {\n",
        "          \"section_title\": \"Multiple Representations\",\n",
        "          \"papers\": get_papers([\"BIBREF53_bb2afd8172469fef7276e9789b306e085ed6e650\", \"BIBREF54_050050e30d0f162c4dd87c1aac8d37df266e4c93\", \"BIBREF55_60b8ad6177230ad5402af409a6edb5af441baeb4\", \"BIBREF56_2d7a784a093615d00d4ac0a7b5763a15d86d4996\", \"BIBREF57_4de571b44a7612189dded764f7a2988ac7529158\"]),\n",
        "          \"subsections\": []\n",
        "      },\n",
        "      {\n",
        "          \"section_title\": \"Fine-tuning Representation-focused Systems\",\n",
        "          \"papers\": get_papers([\"BIBREF50_79cd9f77e5258f62c0e15d11534aea6393ef73fe\", \"BIBREF58_fdb813d8b927bdd21ae1858cafa6c34b66a36268\"]),\n",
        "          \"subsections\": []\n",
        "      }\n",
        "    ]\n",
        "  },\n",
        "  {\n",
        "    \"section_title\": \"Retrieval Architectures and Vector Search\",\n",
        "    \"papers\": [],\n",
        "    \"subsections\": [\n",
        "      {\n",
        "          \"section_title\": \"MIP and NN Search Problems\",\n",
        "          \"papers\": get_papers([\"BIBREF62_5b0a88bdec473552c6a386cd94fdac53c74b79a8\"]),\n",
        "          \"subsections\": []\n",
        "      },\n",
        "      {\n",
        "          \"section_title\": \"Locality sensitive hashing approaches\",\n",
        "          \"papers\": get_papers([\"BIBREF64_1955266a8a58d94e41ad0efe20d707c92a069e95\", \"BIBREF65_3f1e54ed3bd801766e1897d53a9fc962524dd3c2\", \"BIBREF66_9ed960374381062d85d3944182a539c1d00f7703\"]),\n",
        "          \"subsections\": []\n",
        "      },\n",
        "      {\n",
        "          \"section_title\": \"Vector quantisation approaches\",\n",
        "          \"papers\": get_papers([\"BIBREF67_c564aa7639a08c280423489e52b6e32055c9aa7f\", \"BIBREF68_4748d22348e72e6e06c2476486afddbc76e5eca7\"]),\n",
        "          \"subsections\": []\n",
        "      },\n",
        "      {\n",
        "          \"section_title\": \"Graph approaches\",\n",
        "          \"papers\": get_papers([\"BIBREF69_f17c6e164ccc7ec1ad91b3fbbafe8f84664e9803\", \"BIBREF70_c197ecb6a6987667cadcb498136989af1827cce0\", \"BIBREF71_e2e073433931c4d1a739f548b7d17b6e9b2fa13e\", \"BIBREF72_699a2e3b653c69aff5cf7a9923793b974f8ca164\"]),\n",
        "          \"subsections\": []\n",
        "      },\n",
        "      {\n",
        "          \"section_title\": \"Optimisations\",\n",
        "          \"papers\": get_papers([\"BIBREF50_79cd9f77e5258f62c0e15d11534aea6393ef73fe\", \"BIBREF78_2408c965a3855a6b66d128195c783d76e2e939da\", \"BIBREF77_c3cf35677834fb535d3bc7cf8d375366df4b1397\", \"BIBREF76_91429255eefe48ad140ccfaf6aa1e6be11a72a53\", \"BIBREF75_d6f83c915565f575e55fdce0424f65fe192af218\", \"BIBREF74_28336cbf2ee3e8fca6b173c91c5ca9628ba1fa4a\", \"BIBREF55_60b8ad6177230ad5402af409a6edb5af441baeb4\", \"BIBREF73_2cbb8de53759e75411bc528518947a3094fbce3a\", \"BIBREF52_7b577ba0e4230b2ac58d297b3d2cfc3d2f1aaace\", \"BIBREF51_c9b8593db099869fe7254aa1fa53f3c9073b0176\"]),\n",
        "          \"subsections\": []\n",
        "      }\n",
        "    ]\n",
        "  },\n",
        "  {\n",
        "    \"section_title\": \"Learned Sparse Retrieval\",\n",
        "    \"papers\": [],\n",
        "    \"subsections\": [\n",
        "      {\n",
        "          \"section_title\": \"Document expansion learning\",\n",
        "          \"papers\": get_papers([\"BIBREF36_3cfb319689f06bf04c2e28399361f414ca32c4b3\", \"BIBREF79_1225eb6570ce8d45067329fafcc8ff7636a65923\", \"BIBREF81_7f8cbf4453062af8d5c8cf1a94ff01867d122121\", \"BIBREF82_b092b6b843e9421bf42bf96f57ed4658a3e0bdf7\", \"BIBREF83_54fa64b74ec020699fad989f85e74e50c7a34445\", \"BIBREF84_204e3073870fae3d05bcbc2f6a8e263d9b72e776\", \"BIBREF85_29de7c0fb3c09eaf55b20619bceaeafe72fd87a6\", \"BIBREF86_ea6838915acc4e7764caa8f2fdae8dc4629f924b\", \"BIBREF87_57a07372e2a620d6ae920f74877eee5f61753a96\"]),\n",
        "          \"subsections\": []\n",
        "      },\n",
        "      {\n",
        "          \"section_title\": \"Impact score learning\",\n",
        "          \"papers\": get_papers([\"BIBREF3_629f50daebbb9003f645f671f76cc6b33088c17d\", \"BIBREF86_ea6838915acc4e7764caa8f2fdae8dc4629f924b\", \"BIBREF88_1c68eefcceb042fb79712aed347370d6ce7190c1\", \"BIBREF89_3de1752cd0854e220fc41f0ccf7db913f846284c\", \"BIBREF91_4aa1d28944856ebe1950a27f633c6667ead3cbf8\", \"BIBREF93_89d373d61c68465fd49da1257aa959e5abefd155\"]),\n",
        "          \"subsections\": []\n",
        "      },\n",
        "      {\n",
        "          \"section_title\": \"Sparse representation learning\",\n",
        "          \"papers\": get_papers([\"BIBREF87_57a07372e2a620d6ae920f74877eee5f61753a96\", \"BIBREF94_0c57dcf959ead9530f9ec3ebe0dd58de42a3e8af\", \"BIBREF95_1e8a6de5561f557ff9abf43d538d8d5e9347efa0\", \"BIBREF96_9f753f67da834e59f9a5c8cdf9a88ee84c496b2d\"]),\n",
        "          \"subsections\": []\n",
        "      },\n",
        "    ]\n",
        "  }\n",
        "]"
      ],
      "metadata": {
        "id": "vK4ppTsYWgVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(file_sections_structure, 'wb') as f:\n",
        "  pickle.dump(sections, f)"
      ],
      "metadata": {
        "id": "qLTmUUQ3btVB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}