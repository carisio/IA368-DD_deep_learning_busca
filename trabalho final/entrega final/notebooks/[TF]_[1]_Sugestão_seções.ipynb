{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3583f13ec4d949c08c56fa01f5859c9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bfd89e1eba2d423b8224efeb05c63d15",
              "IPY_MODEL_74649c6d23b94f60b0c979bc0179abbd",
              "IPY_MODEL_ef90a087008e4577a2d714caa3d7790f"
            ],
            "layout": "IPY_MODEL_77e68db54b064e09bb10966b2acd8366"
          }
        },
        "bfd89e1eba2d423b8224efeb05c63d15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0c3e8dedaec463c9549cd282db4daf8",
            "placeholder": "​",
            "style": "IPY_MODEL_c27431dfe5634bc08ec9b70fabe53e5f",
            "value": "Fetching 4 files: 100%"
          }
        },
        "74649c6d23b94f60b0c979bc0179abbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55547d8cdc5841eebfac1a38651daf9d",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2bd4227089d44d48a946c22f0a0348c2",
            "value": 4
          }
        },
        "ef90a087008e4577a2d714caa3d7790f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b006f7b7e8743a69e82127d5c1a7eb5",
            "placeholder": "​",
            "style": "IPY_MODEL_f63206b02dc74ba8a15d834b5c1e7291",
            "value": " 4/4 [00:00&lt;00:00, 109.09it/s]"
          }
        },
        "77e68db54b064e09bb10966b2acd8366": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0c3e8dedaec463c9549cd282db4daf8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c27431dfe5634bc08ec9b70fabe53e5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55547d8cdc5841eebfac1a38651daf9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bd4227089d44d48a946c22f0a0348c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3b006f7b7e8743a69e82127d5c1a7eb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f63206b02dc74ba8a15d834b5c1e7291": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68abe864bcdb474dbc5b4621141b1c47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0ef2475d179b4cb68dc97c5d0819afe4",
              "IPY_MODEL_fcaee5efc5f0487483eda658faf1feb1",
              "IPY_MODEL_d3cf82f28f5240f58ba8616106bdf9bb"
            ],
            "layout": "IPY_MODEL_5862c25cda594078b5ae87ad61b25580"
          }
        },
        "0ef2475d179b4cb68dc97c5d0819afe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02df5b2e56904f96bfe32d7c84ec29fe",
            "placeholder": "​",
            "style": "IPY_MODEL_5505aa16711445f8bc010bdee7fbc7b8",
            "value": "Fetching 4 files: 100%"
          }
        },
        "fcaee5efc5f0487483eda658faf1feb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c5b5fd032474e61abe68439ea58bd04",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a3cf1051d16b47a290188646fff2f9d1",
            "value": 4
          }
        },
        "d3cf82f28f5240f58ba8616106bdf9bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ab7186eccbd410ea03677eb816d38ab",
            "placeholder": "​",
            "style": "IPY_MODEL_f56ec00989444745ba4921ff4f758c81",
            "value": " 4/4 [00:00&lt;00:00, 231.09it/s]"
          }
        },
        "5862c25cda594078b5ae87ad61b25580": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02df5b2e56904f96bfe32d7c84ec29fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5505aa16711445f8bc010bdee7fbc7b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c5b5fd032474e61abe68439ea58bd04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3cf1051d16b47a290188646fff2f9d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2ab7186eccbd410ea03677eb816d38ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f56ec00989444745ba4921ff4f758c81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2200d9cb98784f75b1128ad4f69e3ae0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b5407b545a2c4752928ead55e41a8b38",
              "IPY_MODEL_730d69cb56c14d7ab72f914463a31678",
              "IPY_MODEL_ec2696171050498ea8b83f4d9bbd786c"
            ],
            "layout": "IPY_MODEL_6d6d8660911b47ed98c9cc74c6e1c699"
          }
        },
        "b5407b545a2c4752928ead55e41a8b38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71edc248a097478faf7dd1a3898ec4e1",
            "placeholder": "​",
            "style": "IPY_MODEL_d3df3521876040ab8fc0ba241683f03b",
            "value": "Batches: 100%"
          }
        },
        "730d69cb56c14d7ab72f914463a31678": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f648fd9f9d8d438e87a70b0a994663b6",
            "max": 101,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f7b754f0563e48f49aec626699b8f476",
            "value": 101
          }
        },
        "ec2696171050498ea8b83f4d9bbd786c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3f6502fb983404aa71ec9ee146a2b31",
            "placeholder": "​",
            "style": "IPY_MODEL_9dbff6fcf8a84c379234073a5e465cc0",
            "value": " 101/101 [01:01&lt;00:00, 15.72it/s]"
          }
        },
        "6d6d8660911b47ed98c9cc74c6e1c699": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71edc248a097478faf7dd1a3898ec4e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3df3521876040ab8fc0ba241683f03b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f648fd9f9d8d438e87a70b0a994663b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7b754f0563e48f49aec626699b8f476": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e3f6502fb983404aa71ec9ee146a2b31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9dbff6fcf8a84c379234073a5e465cc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a194be1efb134d41a959fe78ba170fa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_175465a2ed514f3485bf4aa2a89363e1",
              "IPY_MODEL_7e11e8f338594a77bb79b3c4560d8797",
              "IPY_MODEL_2d6f86aeca34416d83cbbd438d0a9181"
            ],
            "layout": "IPY_MODEL_cf9c5f2ca0414a628c3f8f33d615d507"
          }
        },
        "175465a2ed514f3485bf4aa2a89363e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93a14110625f4883a37ac8a5c7fbcc49",
            "placeholder": "​",
            "style": "IPY_MODEL_38fb1de5f2ab4cdfb78a8428139999a0",
            "value": "Batches: 100%"
          }
        },
        "7e11e8f338594a77bb79b3c4560d8797": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_082d7e1fa49d4674ba9728b067af5c7c",
            "max": 101,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_23c808b4d9fa40ea9b5ea64ed2120d5e",
            "value": 101
          }
        },
        "2d6f86aeca34416d83cbbd438d0a9181": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22f13f50dc4a4fef9c9e20160522d20c",
            "placeholder": "​",
            "style": "IPY_MODEL_69cac5931b3545edb922afe197838c29",
            "value": " 101/101 [01:00&lt;00:00, 15.34it/s]"
          }
        },
        "cf9c5f2ca0414a628c3f8f33d615d507": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93a14110625f4883a37ac8a5c7fbcc49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38fb1de5f2ab4cdfb78a8428139999a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "082d7e1fa49d4674ba9728b067af5c7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23c808b4d9fa40ea9b5ea64ed2120d5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "22f13f50dc4a4fef9c9e20160522d20c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69cac5931b3545edb922afe197838c29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa57f5794886470289b4239b810e9049": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f587605bd10480599ee7cfaeb0d56cb",
              "IPY_MODEL_4572efbc1f904425a02153f8658a1336",
              "IPY_MODEL_a6119bd0242443da804bbaabf1e585bf"
            ],
            "layout": "IPY_MODEL_7bb1aa7efd60473a9f3b0fc12aca7a0d"
          }
        },
        "7f587605bd10480599ee7cfaeb0d56cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a6073202e9745fa89e56be0418198c5",
            "placeholder": "​",
            "style": "IPY_MODEL_52897c8aaf97456397e4d6575a9ee5bf",
            "value": "Batches: 100%"
          }
        },
        "4572efbc1f904425a02153f8658a1336": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc55db25702947368d10987828162cd8",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ffa5964f347a43d6b2d83664b510b32b",
            "value": 2
          }
        },
        "a6119bd0242443da804bbaabf1e585bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dfa9b96bdc9c431388f4be23c1314962",
            "placeholder": "​",
            "style": "IPY_MODEL_d5596691c2904783b0fdb96cf022ff97",
            "value": " 2/2 [00:01&lt;00:00,  1.58it/s]"
          }
        },
        "7bb1aa7efd60473a9f3b0fc12aca7a0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a6073202e9745fa89e56be0418198c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52897c8aaf97456397e4d6575a9ee5bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc55db25702947368d10987828162cd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffa5964f347a43d6b2d83664b510b32b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dfa9b96bdc9c431388f4be23c1314962": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5596691c2904783b0fdb96cf022ff97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68fde3edfa624e9cafa9c46abedd9b9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_471fa00c991f455fbcf954962e98232e",
              "IPY_MODEL_351c1e8753d749a6ac6b36cdf0867bfb",
              "IPY_MODEL_cbf297e3587147aba77d8132ff50147d"
            ],
            "layout": "IPY_MODEL_13f40cfb243249948cc6eeac7d87de0f"
          }
        },
        "471fa00c991f455fbcf954962e98232e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76f56b47bbef411d8991ab31c3495cff",
            "placeholder": "​",
            "style": "IPY_MODEL_9acdd1c32d484f4fbbbbdc7fadf97b24",
            "value": "Batches: 100%"
          }
        },
        "351c1e8753d749a6ac6b36cdf0867bfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f029aa5fb36451ea28c183e81d97a59",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_41e05c3756aa40c7b26c12bc12e76631",
            "value": 2
          }
        },
        "cbf297e3587147aba77d8132ff50147d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3224b12fbd943a78328055bd8b4fdb1",
            "placeholder": "​",
            "style": "IPY_MODEL_f664f4fc8ffd4201abe45362b7a6d810",
            "value": " 2/2 [00:01&lt;00:00,  1.60it/s]"
          }
        },
        "13f40cfb243249948cc6eeac7d87de0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76f56b47bbef411d8991ab31c3495cff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9acdd1c32d484f4fbbbbdc7fadf97b24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f029aa5fb36451ea28c183e81d97a59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41e05c3756aa40c7b26c12bc12e76631": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b3224b12fbd943a78328055bd8b4fdb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f664f4fc8ffd4201abe45362b7a6d810": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27ea8e1b05264095a468e705fdfaefa2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0b4bdb98c8da473ab87daeea44c06bf4",
              "IPY_MODEL_22e237ac8f784615a2b8dd13e3f2cbc7",
              "IPY_MODEL_1f28da6a7ce143d1bfd067077d3dc0fb"
            ],
            "layout": "IPY_MODEL_b10eac84f9f34147a274e2c8b1486fd9"
          }
        },
        "0b4bdb98c8da473ab87daeea44c06bf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16034ba8d65d4ef488a63499fe49c7a1",
            "placeholder": "​",
            "style": "IPY_MODEL_57ec6862a1b34dab923e2d52a20c422a",
            "value": "Batches: 100%"
          }
        },
        "22e237ac8f784615a2b8dd13e3f2cbc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a52665149d6f4595b39ad29125d6a466",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8b297fcde23548b88c9d1f306209a7de",
            "value": 2
          }
        },
        "1f28da6a7ce143d1bfd067077d3dc0fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e7c2d62ed2e48d4a7ca84889e8c8eea",
            "placeholder": "​",
            "style": "IPY_MODEL_2f9a1f078b7c499098b6c3bc529128a7",
            "value": " 2/2 [00:01&lt;00:00,  1.68it/s]"
          }
        },
        "b10eac84f9f34147a274e2c8b1486fd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16034ba8d65d4ef488a63499fe49c7a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57ec6862a1b34dab923e2d52a20c422a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a52665149d6f4595b39ad29125d6a466": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b297fcde23548b88c9d1f306209a7de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0e7c2d62ed2e48d4a7ca84889e8c8eea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f9a1f078b7c499098b6c3bc529128a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe0ba0cd32174fc5a6faba6b5e981060": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_33a213706c5146a3a6629afe7d19c6c7",
              "IPY_MODEL_2da500ad4e194e98bcd933bffe5c76f5",
              "IPY_MODEL_43e85765543b4d139c57e866bcefda82"
            ],
            "layout": "IPY_MODEL_8141f965239347f69763c6be11508cec"
          }
        },
        "33a213706c5146a3a6629afe7d19c6c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c54e74bde624f8a95fd5cd145bdc159",
            "placeholder": "​",
            "style": "IPY_MODEL_6cba0dc98de843cc9d5772cf5dc08daa",
            "value": "Batches: 100%"
          }
        },
        "2da500ad4e194e98bcd933bffe5c76f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d37b9777d1448c99fc6bde45d8e525b",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d8a71a01121243f9bc609b7fb8eb87ad",
            "value": 2
          }
        },
        "43e85765543b4d139c57e866bcefda82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed80f436acdb4d3091b6d47c75f34881",
            "placeholder": "​",
            "style": "IPY_MODEL_0a7c3dd5973d4209843bae3916965c4d",
            "value": " 2/2 [00:01&lt;00:00,  1.64it/s]"
          }
        },
        "8141f965239347f69763c6be11508cec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c54e74bde624f8a95fd5cd145bdc159": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cba0dc98de843cc9d5772cf5dc08daa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d37b9777d1448c99fc6bde45d8e525b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8a71a01121243f9bc609b7fb8eb87ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ed80f436acdb4d3091b6d47c75f34881": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a7c3dd5973d4209843bae3916965c4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51a2b4396f4d40189890f3f454035d0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2727cd985ac84240ac2e49c091f3e7aa",
              "IPY_MODEL_5f9dcb8f8b624a3c91907bbd3695a2da",
              "IPY_MODEL_d165be1aa4ec407486384ca395ba3460"
            ],
            "layout": "IPY_MODEL_7bab703985ee472eb183dd5dfe1ebd1c"
          }
        },
        "2727cd985ac84240ac2e49c091f3e7aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_245536790dc34fc0b16252aa72a4116f",
            "placeholder": "​",
            "style": "IPY_MODEL_a63d4f061f4e468cb1e9340753fca7e8",
            "value": "Batches: 100%"
          }
        },
        "5f9dcb8f8b624a3c91907bbd3695a2da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f592edab70aa428488125c5cffd501b2",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_794d46c1873b436cb552139fcc5e2f83",
            "value": 2
          }
        },
        "d165be1aa4ec407486384ca395ba3460": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a23292148264ee3a96b68831119309a",
            "placeholder": "​",
            "style": "IPY_MODEL_715fc585ed044c2d99075bc4996660b2",
            "value": " 2/2 [00:01&lt;00:00,  1.73it/s]"
          }
        },
        "7bab703985ee472eb183dd5dfe1ebd1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "245536790dc34fc0b16252aa72a4116f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a63d4f061f4e468cb1e9340753fca7e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f592edab70aa428488125c5cffd501b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "794d46c1873b436cb552139fcc5e2f83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8a23292148264ee3a96b68831119309a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "715fc585ed044c2d99075bc4996660b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain -q\n",
        "!pip install faiss-gpu -q\n",
        "!pip install adapter-transformers -q\n",
        "!pip install openai -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2X0-PsaEz_GW",
        "outputId": "6a5f5880-7305-4ca9-b306-46ab25fa2048"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Params"
      ],
      "metadata": {
        "id": "lWzvhUhr01iu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The researcher will write a survey about some topic. In this pipeline, we consider this topic as the query for a database of scientific papers. In this implementation, the Semantic Scholar API is used to access this database.\n",
        "\n",
        "He/she must also provide search criteria that will be used to narrow their search. For example, he/she migth be interested only in papers from a certain date."
      ],
      "metadata": {
        "id": "FMaeqD7pIIBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####################################\n",
        "# INPUT PARAMS DEFINED BY THE USER #\n",
        "####################################\n",
        "DEBUG = True\n",
        "SURVEY_TOPIC_QUERY = 'text neural information retrieval'\n",
        "\n",
        "SURVEY_FILTERS = {\n",
        "  \"fields_of_study\": 'Computer Science',\n",
        "  \"year\": '2020-2023',\n",
        "  \"only_open_access\": True,\n",
        "  \"max_number_of_papers\": 9999, # should be less than 10000\n",
        "}\n",
        "\n",
        "SURVEY_STRUCTURE = {\n",
        "  \"n_sections\": [5, 3], # Recursive -> first level is 5 sections, second level is 3 sections. Arbitrary levels are allows\n",
        "  \"n_papers_to_suggest_title_section\": 10, # top_k1 in paper\n",
        "  \"max_papers_per_section_as_ref\": 20,     # top_k2 in paper\n",
        "  \"n_papers_to_cluster_in_subsections\": 50, # top_k3 in paper, should be less than the total number of papers in the first level\n",
        "}"
      ],
      "metadata": {
        "id": "CfpBVCODHxHi"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "##########################################\n",
        "# INPUT PARAMS DEFINED BY THE PROGRAMMER #\n",
        "##########################################\n",
        "# Batch size to extract the embeddings of the papers\n",
        "PARAMS = {\n",
        "  'batch_size': 32,\n",
        "  'OPENAI_API_KEY': '',\n",
        "  'SEMANTIC_SCHOLAR_API': '',\n",
        "  'gpt_model_name': 'gpt-3.5-turbo-0613'\n",
        "}\n",
        "\n",
        "PARAMS['OPENAI_API_KEY'] = getpass('OpenAI API key: ')\n",
        "PARAMS['SEMANTIC_SCHOLAR_API'] = getpass('Semantic Scholar API key: ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZh5j0vp2VR5",
        "outputId": "62dc3587-c905-4bf1-82c3-edd1f41ffff0"
      },
      "execution_count": 44,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API key: ··········\n",
            "Semantic Scholar API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Structure of a survey"
      ],
      "metadata": {
        "id": "v_-PeaOLHw1N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "EUaVrutxHKtp"
      },
      "outputs": [],
      "source": [
        "# The sections list has the following structure:\n",
        "#sections = [\n",
        "#    {\n",
        "#        'section_title': 'TITILE 1',\n",
        "#        'papers': [],\n",
        "#        'subsections': [\n",
        "#            {\n",
        "#                'section_title': 'SUBTITLE 1.1',\n",
        "#                'papers': []\n",
        "#            },\n",
        "#            ...\n",
        "#        ]\n",
        "#    },\n",
        "#    ...\n",
        "#]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create empty sections structure:\n",
        "def create_empty_sections(number_of_sections):\n",
        "  secs = []\n",
        "  for i in range(number_of_sections):\n",
        "    new_section = {\"section_title\": \"\", \"papers\": [], \"subsections\": []}\n",
        "    secs.append(new_section)\n",
        "  return secs\n",
        "\n",
        "def populate_with_empty_sections(list_sections, n_sections_in_each_level):\n",
        "  if len(n_sections_in_each_level) == 0:\n",
        "    return\n",
        "\n",
        "  sections_in_level = create_empty_sections(n_sections_in_each_level[0])\n",
        "  list_sections.extend(sections_in_level)\n",
        "\n",
        "  for sec_in_level in sections_in_level:\n",
        "    populate_with_empty_sections(sec_in_level['subsections'], n_sections_in_each_level[1:])\n",
        "\n",
        "sections = []\n",
        "populate_with_empty_sections(sections, SURVEY_STRUCTURE['n_sections'])"
      ],
      "metadata": {
        "id": "yW8OqRyUx-z0"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial search - Get survey papers"
      ],
      "metadata": {
        "id": "UkSXrwtd073R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A partir dessa entrada, fazemos uma busca inicial de artigos usando a API do Semantic Scholar:"
      ],
      "metadata": {
        "id": "w8H31na0LGbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import pickle\n",
        "import time\n",
        "\n",
        "def search_by_keywords(query,\n",
        "                       fields='url,title,venue,year,authors,abstract,openAccessPdf,citationCount,referenceCount,publicationTypes,journal,tldr,publicationDate',\n",
        "                       fieldsOfStudy='Computer Science',\n",
        "                       year='2020-2023',\n",
        "                       openAccessPdf=True,\n",
        "                       offset=0,\n",
        "                       limit=100): #limit should be <= 100\n",
        "  query_openaccess = '&openAccessPdf' if openAccessPdf else ''\n",
        "  url = f'https://api.semanticscholar.org/graph/v1/paper/search?query={query}&fields={fields}&fieldsOfStudy={fieldsOfStudy}&year={year}{query_openaccess}&offset={offset}&limit={limit}'\n",
        "  headers = {\"x-api-key\": PARAMS['SEMANTIC_SCHOLAR_API']} if PARAMS['SEMANTIC_SCHOLAR_API'] else {}\n",
        "  time.sleep(1)\n",
        "  return requests.get(url, headers=headers).json()\n",
        "\n",
        "\n",
        "def search_all_by_keywords(query=\"\", fieldsOfStudy=\"Computer Science\", year='2020-2023', openAccessPdf=True, maxNumberOfPapers=1000):\n",
        "  offset = 0\n",
        "  limit = min(100, maxNumberOfPapers)\n",
        "  total = 1\n",
        "  all_papers = []\n",
        "\n",
        "  while True:\n",
        "    result = search_by_keywords(query, fieldsOfStudy=fieldsOfStudy, year=year, openAccessPdf=openAccessPdf, offset=offset, limit=limit)\n",
        "    all_papers.extend(result['data'])\n",
        "    total = result['total']\n",
        "\n",
        "    if (DEBUG):\n",
        "      print(f'Searching from {offset} to {offset+limit}. Total: {total}')\n",
        "\n",
        "    offset += limit\n",
        "\n",
        "    if offset > total or offset >= maxNumberOfPapers:\n",
        "      break\n",
        "\n",
        "    if (offset + limit) >= maxNumberOfPapers:\n",
        "      limit = maxNumberOfPapers - offset\n",
        "\n",
        "  return all_papers\n",
        "\n",
        "\n",
        "def save_file(obj, file_name):\n",
        "  with open(file_name, 'wb') as f:\n",
        "    pickle.dump(obj, f)"
      ],
      "metadata": {
        "id": "S84nS1qaLUMh"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Check total\n",
        "query=SURVEY_TOPIC_QUERY\n",
        "print(query)\n",
        "print(search_by_keywords(query=query,\n",
        "                   fieldsOfStudy=SURVEY_FILTERS['fields_of_study'],\n",
        "                    year=SURVEY_FILTERS['year'],\n",
        "                    openAccessPdf=SURVEY_FILTERS['only_open_access'],\n",
        "                    offset=0, limit=10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0u8QMxNy0Qe",
        "outputId": "15f0070a-90df-45e6-9ab6-7682bda88d28"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text neural information retrieval\n",
            "{'total': 3241, 'offset': 0, 'next': 10, 'data': [{'paperId': '0385e1986160069417c3fa497b08441c45ad150a', 'url': 'https://www.semanticscholar.org/paper/0385e1986160069417c3fa497b08441c45ad150a', 'title': 'Continual Learning of Long Topic Sequences in Neural Information Retrieval - abstract', 'abstract': None, 'venue': 'Joint Conference of the Information Retrieval Communities in Europe', 'year': 2022, 'referenceCount': 44, 'citationCount': 2, 'openAccessPdf': {'url': 'http://arxiv.org/pdf/2201.03356', 'status': 'GREEN'}, 'tldr': {'model': 'tldr@v2.0.0', 'text': 'This paper proposes a dataset based upon the MSMarco corpus aiming at modeling a long stream of topics as well as IR property-driven controlled settings and in-depth analyzes the ability of recent neural IR models while continually learning those streams.'}, 'publicationTypes': ['JournalArticle'], 'publicationDate': '2022-01-10', 'journal': {'name': 'ArXiv', 'volume': 'abs/2201.03356'}, 'authors': [{'authorId': '2066135377', 'name': 'Thomas Gerald'}, {'authorId': '145159652', 'name': 'L. Soulier'}]}, {'paperId': 'ac2d03a7f7def0c3103df45c491734939ccbf122', 'url': 'https://www.semanticscholar.org/paper/ac2d03a7f7def0c3103df45c491734939ccbf122', 'title': 'GazBy: Gaze-Based BERT Model to Incorporate Human Attention in Neural Information Retrieval', 'abstract': 'This paper is interested in investigating whether human gaze signals can be leveraged to improve state-of-the-art search engine performance and how to incorporate this new input signal marked by human attention into existing neural retrieval models. In this paper, we propose GazBy (Gaze-based Bert model for document relevancy), a lightweight joint model that integrates human gaze fixation estimation into transformer models to predict document relevance, incorporating more nuanced information about cognitive processing into information retrieval (IR). We evaluate our model on the Text Retrieval Conference (TREC) Deep Learning (DL) 2019 and 2020 Tracks. Our experiments show encouraging results and illustrate the effective and ineffective entry points for using human gaze to help with transformer-based neural retrievers. With the rise of virtual reality (VR) and augmented reality (AR), human gaze data will become more available. We hope this work serves as a first step exploring using gaze signals in modern neural search engines.', 'venue': 'International Conference on the Theory of Information Retrieval', 'year': 2022, 'referenceCount': 79, 'citationCount': 1, 'openAccessPdf': {'url': 'http://arxiv.org/pdf/2207.01674', 'status': 'GREEN'}, 'tldr': {'model': 'tldr@v2.0.0', 'text': 'GazBy (Gaze-based Bert model for document relevancy), a lightweight joint model that integrates human gaze fixation estimation into transformer models to predict document relevance, incorporating more nuanced information about cognitive processing into information retrieval (IR).'}, 'publicationTypes': ['JournalArticle', 'Book'], 'publicationDate': '2022-07-04', 'journal': {'name': 'Proceedings of the 2022 ACM SIGIR International Conference on Theory of Information Retrieval'}, 'authors': [{'authorId': '2174899004', 'name': 'Sibo Dong'}, {'authorId': '2174813621', 'name': 'Justin Goldstein'}, {'authorId': '2109540870', 'name': 'Grace Hui Yang'}]}, {'paperId': 'bbaa9599d10b5f29546a8c52eeb34b38ef4e3596', 'url': 'https://www.semanticscholar.org/paper/bbaa9599d10b5f29546a8c52eeb34b38ef4e3596', 'title': 'Learning Better Representations for Neural Information Retrieval with Graph Information', 'abstract': \"Neural ranking models have recently gained much attention in Information Retrieval community and obtain good ranking performance. However, most of these retrieval models focus on capturing the textual matching signals between query and document but do not consider user behavior information that may be helpful for the retrieval task. Specifically, users' click and query reformulation behavior can be represented by a click-through bipartite graph and a session-flow graph, respectively. Such graph representations contain rich user behavior information and may help us better understand users' search intent beyond the textual information. In this study, we aim to incorporate this rich information encoded in these two graphs into existing neural ranking models. We present two graph-based neural ranking models (\\\\emphEmbRanker and AggRanker ) to enrich learned text representations with graph information that captures rich users' interaction behavior information. Experimental results on a large-scale publicly available benchmark dataset show that the two models outperform most existing neural ranking models that only consider textual information, which illustrates the effectiveness of integrating graph information with textual information. Further analyses show how graph information complements text matching signals and examine whether these two models can be adopted in practical applications.\", 'venue': 'International Conference on Information and Knowledge Management', 'year': 2020, 'referenceCount': 45, 'citationCount': 7, 'openAccessPdf': {'url': 'https://pure.uva.nl/ws/files/53902637/li_2020_learning.pdf', 'status': 'GREEN'}, 'tldr': {'model': 'tldr@v2.0.0', 'text': 'Experimental results on a large-scale publicly available benchmark dataset show that the two models outperform most existing neural ranking models that only consider textual information, which illustrates the effectiveness of integrating graph information with textual information.'}, 'publicationTypes': ['Book', 'JournalArticle', 'Conference'], 'publicationDate': '2020-10-19', 'journal': {'name': 'Proceedings of the 29th ACM International Conference on Information & Knowledge Management'}, 'authors': [{'authorId': '2116837949', 'name': 'Xiangsheng Li'}, {'authorId': '1696030', 'name': 'M. de Rijke'}, {'authorId': '1783406', 'name': 'Yiqun Liu'}, {'authorId': '3340223', 'name': 'Jiaxin Mao'}, {'authorId': '2903964', 'name': 'Weizhi Ma'}, {'authorId': '38898636', 'name': 'Min Zhang'}, {'authorId': '8093158', 'name': 'Shaoping Ma'}]}, {'paperId': '1222fa1e218a8ba9af1c7b0d95caf78c53f7a784', 'url': 'https://www.semanticscholar.org/paper/1222fa1e218a8ba9af1c7b0d95caf78c53f7a784', 'title': 'Neural text generation for query expansion in information retrieval', 'abstract': 'Expanding users’ query is a well-known way to improve the performance of document retrieval systems. Several approaches have been proposed in the literature, and some of them are considered as yielding state-of-the-art results in Information Retrieval. In this paper, we explore the use of text generation to automatically expand the queries. We rely on a well-known neural generative model, OpenAI’s GPT-2, that comes with pre-trained models for English but can also be fine-tuned on specific corpora. Through different experiments and several datasets, we show that text generation is a very effective way to improve the performance of an IR system, with a large margin (+10 %MAP gains), and that it outperforms strong baselines also relying on query expansion (RM3). This conceptually simple approach can easily be implemented on any IR system thanks to the availability of GPT code and models.', 'venue': 'WI/IAT', 'year': 2021, 'referenceCount': 39, 'citationCount': 9, 'openAccessPdf': {'url': 'https://hal.archives-ouvertes.fr/hal-03494692/file/Claveau_GPT4IR_WI2021.pdf', 'status': 'GREEN'}, 'tldr': {'model': 'tldr@v2.0.0', 'text': 'It is shown that text generation is a very effective way to improve the performance of an IR system, with a large margin (+10 %MAP gains), and that it outperforms strong baselines also relying on query expansion (RM3).'}, 'publicationTypes': ['JournalArticle', 'Book'], 'publicationDate': '2021-12-14', 'journal': {'name': 'IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology'}, 'authors': [{'authorId': '1735666', 'name': 'V. Claveau'}]}, {'paperId': 'c9a018b08ca1f320ac63b3183de863059da88883', 'url': 'https://www.semanticscholar.org/paper/c9a018b08ca1f320ac63b3183de863059da88883', 'title': 'Answering Topical Information Needs Using Neural Entity-Oriented Information Retrieval and Extraction', 'abstract': 'In the modern world, search engines are an integral part of human lives. The field of Information Retrieval (IR) is concerned with finding material (usually documents) of an unstructured nature (usually text) that satisfies an information need (query) from within large collections (usually stored on computers). The search engine then displays a ranked list of results relevant to our query. Traditional document retrieval algorithms match a query to a document using the overlap of words in both. However, the last decade has seen the focus shifting to leveraging the rich semantic information available in the form of entities. Entities are uniquely identifiable objects or things such as places, events, diseases, etc. that exist in the real or fictional world. Entity-oriented search systems leverage the semantic information associated with entities (e.g., names, types, etc.) to better match documents to queries. Web search engines would provide better search results if they understand the meaning of a query. This dissertation advances the state-of-the-art in IR by developing novel algorithms that understand text (query, document, question, sentence, etc.) at the semantic level. To this end, this dissertation aims to understand the fine-grained meaning of entities from the context in which the entities have been mentioned, for example, \"oysters\" in the context of food versus ecosystems. Further, this dissertation aims to automatically learn (vector) representations of entities that incorporate this fine-grained knowledge and knowledge about the query. This dissertation refines the automatic understanding of text passages using deep learning, a modern artificial intelligence paradigm. This dissertation utilizes the semantic information extracted from entities to retrieve materials (text and entities) relevant to a query. The interplay between text and entities in the text is studied by addressing three related prediction problems: (1) Identify entities that are relevant for the query, (2) Understand an entity\\'s meaning in the context of the query, and (3) Identify text passages that elaborate the connection between the query and an entity. The research presented in this dissertation may be integrated into a larger system designed for answering complex topical queries such as dark chocolate health benefits which require the search engine to automatically understand the connections between the query and the relevant material, thus transforming the search engine into an answering engine. Awarded by: University of New Hampshire, Durham, USA on 1 September 2022. Supervised by: Laura Dietz. Available at: https://scholars.unh.edu/dissertation/2714/.', 'venue': 'SIGIR Forum', 'year': 2022, 'referenceCount': 6, 'citationCount': 0, 'openAccessPdf': {'url': 'https://scholars.unh.edu/cgi/viewcontent.cgi?article=3702&context=dissertation', 'status': 'GREEN'}, 'tldr': {'model': 'tldr@v2.0.0', 'text': 'This dissertation refines the automatic understanding of text passages using deep learning, a modern artificial intelligence paradigm, to understand the fine-grained meaning of entities from the context in which they have been mentioned, for example, \"oysters\" in the context of food versus ecosystems.'}, 'publicationTypes': ['JournalArticle'], 'publicationDate': '2022-12-01', 'journal': {'name': 'ACM SIGIR Forum', 'pages': '1 - 2', 'volume': '56'}, 'authors': [{'authorId': '2113355478', 'name': 'Shubham Chatterjee'}]}, {'paperId': '6d391498d4161532b0ea158cc11280e1096dbe8f', 'url': 'https://www.semanticscholar.org/paper/6d391498d4161532b0ea158cc11280e1096dbe8f', 'title': 'Text‐based question answering from information retrieval and deep neural network perspectives: A survey', 'abstract': \"Text‐based question answering (QA) is a challenging task which aims at finding short concrete answers for users' questions. This line of research has been widely studied with information retrieval (IR) techniques and has received increasing attention in recent years by considering deep neural network approaches. Deep learning (DL) approaches, which are the main focus of this paper, provide a powerful technique to learn multiple layers of representations and interaction between the questions and the answer sentences. In this paper, we provide a comprehensive overview of different models proposed for the QA task, including both a traditional IR perspective and a more recent deep neural network environment. We also introduce well‐known datasets for the task and present available results from the literature to have a comparison between different techniques.\", 'venue': 'WIREs Data Mining Knowl. Discov.', 'year': 2020, 'referenceCount': 79, 'citationCount': 45, 'openAccessPdf': {'url': 'http://arxiv.org/pdf/2002.06612', 'status': 'GREEN'}, 'tldr': {'model': 'tldr@v2.0.0', 'text': 'This paper provides a comprehensive overview of different models proposed for the QA task, including both a traditional IR perspective and a more recent deep neural network environment.'}, 'publicationTypes': ['JournalArticle', 'Review'], 'publicationDate': '2020-02-16', 'journal': {'name': 'Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery', 'volume': '11'}, 'authors': [{'authorId': '1492180855', 'name': 'Zahra Abbasiyantaeb'}, {'authorId': '2306648', 'name': 'S. Momtazi'}]}, {'paperId': 'f003191790ff27f4583f767148501f5112e82d8f', 'url': 'https://www.semanticscholar.org/paper/f003191790ff27f4583f767148501f5112e82d8f', 'title': 'Regulatory Compliance through Doc2Doc Information Retrieval: A case study in EU/UK legislation where text similarity has limitations', 'abstract': 'Major scandals in corporate history have urged the need for regulatory compliance, where organizations need to ensure that their controls (processes) comply with relevant laws, regulations, and policies. However, keeping track of the constantly changing legislation is difficult, thus organizations are increasingly adopting Regulatory Technology (RegTech) to facilitate the process. To this end, we introduce regulatory information retrieval (REG-IR), an application of document-to-document information retrieval (DOC2DOC IR), where the query is an entire document making the task more challenging than traditional IR where the queries are short. Furthermore, we compile and release two datasets based on the relationships between EU directives and UK legislation. We experiment on these datasets using a typical two-step pipeline approach comprising a pre-fetcher and a neural re-ranker. Experimenting with various pre-fetchers from BM25 to k nearest neighbors over representations from several BERT models, we show that fine-tuning a BERT model on an in-domain classification task produces the best representations for IR. We also show that neural re-rankers under-perform due to contradicting supervision, i.e., similar query-document pairs with opposite labels. Thus, they are biased towards the pre-fetcher’s score. Interestingly, applying a date filter further improves the performance, showcasing the importance of the time dimension.', 'venue': 'Conference of the European Chapter of the Association for Computational Linguistics', 'year': 2021, 'referenceCount': 48, 'citationCount': 10, 'openAccessPdf': {'url': 'https://aclanthology.org/2021.eacl-main.305.pdf', 'status': 'HYBRID'}, 'tldr': {'model': 'tldr@v2.0.0', 'text': 'It is shown that fine-tuning a BERT model on an in-domain classification task produces the best representations for IR, and that neural re-rankers under-perform due to contradicting supervision, i.e., similar query-document pairs with opposite labels.'}, 'publicationTypes': ['JournalArticle', 'Conference'], 'publicationDate': '2021-01-26', 'journal': {'pages': '3498-3511'}, 'authors': [{'authorId': '10783142', 'name': 'Ilias Chalkidis'}, {'authorId': '121316873', 'name': 'Manos Fergadiotis'}, {'authorId': '1994191609', 'name': 'Nikolaos Manginas'}, {'authorId': '2047357850', 'name': 'Eva Katakalou'}, {'authorId': '1950133', 'name': 'Prodromos Malakasiotis'}]}, {'paperId': '6ccac8a95bc77549b98d045db6d5e0de3d356ba4', 'url': 'https://www.semanticscholar.org/paper/6ccac8a95bc77549b98d045db6d5e0de3d356ba4', 'title': 'Exploring Classic and Neural Lexical Translation Models for Information Retrieval: Interpretability, Effectiveness, and Efficiency Benefits', 'abstract': None, 'venue': 'European Conference on Information Retrieval', 'year': 2021, 'referenceCount': 76, 'citationCount': 9, 'openAccessPdf': {'url': 'http://arxiv.org/pdf/2102.06815', 'status': 'GREEN'}, 'tldr': {'model': 'tldr@v2.0.0', 'text': 'It is shown that adding an interpretable neural Model 1 layer on top of BERT-based contextualized embeddings does not decrease accuracy and/or efficiency; and may overcome the limitation on the maximum sequence length of existing BERT models.'}, 'publicationTypes': ['JournalArticle', 'Conference'], 'publicationDate': '2021-02-12', 'journal': {'pages': '63-78'}, 'authors': [{'authorId': '3308561', 'name': 'Leonid Boytsov'}, {'authorId': '117539586', 'name': 'Zico Kolter'}]}, {'paperId': '27c12b8d9cfe4e88e513a53e620094e3a87a6ab2', 'url': 'https://www.semanticscholar.org/paper/27c12b8d9cfe4e88e513a53e620094e3a87a6ab2', 'title': 'Measuring the relative importance of full text sections for information retrieval from scientific literature.', 'abstract': 'With the growing availability of full-text articles, integrating abstracts and full texts of documents into a unified representation is essential for comprehensive search of scientific literature. However, previous studies have shown that naïvely merging abstracts with full texts of articles does not consistently yield better performance. Balancing the contribution of query terms appearing in the abstract and in sections of different importance in full text articles remains a challenge both with traditional bag-of-words IR approaches and for neural retrieval methods. In this work we establish the connection between the BM25 score of a query term appearing in a section of a full text document and the probability of that document being clicked or identified as relevant. Probability is computed using Pool Adjacent Violators (PAV), an isotonic regression algorithm, providing a maximum likelihood estimate based on the observed data. Using this probabilistic transformation of BM25 scores we show an improved performance on the PubMed Click dataset developed and presented in this study, as well as the 2007 TREC Genomics collection.', 'venue': 'Workshop on Biomedical Natural Language Processing', 'year': 2021, 'referenceCount': 27, 'citationCount': 1, 'openAccessPdf': {'url': 'https://aclanthology.org/2021.bionlp-1.27.pdf', 'status': 'HYBRID'}, 'tldr': {'model': 'tldr@v2.0.0', 'text': 'The connection between the BM25 score of a query term appearing in a section of a full text document and the probability of that document being clicked or identified as relevant is established and improved performance is shown on the PubMed Click dataset developed and presented.'}, 'publicationTypes': ['JournalArticle'], 'publicationDate': '2021-06-01', 'journal': {'pages': '247-256'}, 'authors': [{'authorId': '2016520', 'name': 'Lana Yeganova'}, {'authorId': '2152202170', 'name': 'Won Kim'}, {'authorId': '1753903', 'name': 'Donald C. Comeau'}, {'authorId': '1695611', 'name': 'W. Wilbur'}, {'authorId': '144202084', 'name': 'Zhiyong Lu'}]}, {'paperId': '017386502557c27d4ffd575b17ed7c2aafed2d95', 'url': 'https://www.semanticscholar.org/paper/017386502557c27d4ffd575b17ed7c2aafed2d95', 'title': 'Item Tagging for Information Retrieval: A Tripartite Graph Neural Network based Approach', 'abstract': 'Tagging has been recognized as a successful practice to boost relevance matching for information retrieval (IR), especially when items lack rich textual descriptions. A lot of research has been done for either multi-label text categorization or image annotation. However, there is a lack of published work that targets at item tagging specifically for IR. Directly applying a traditional multi-label classification model for item tagging is sub-optimal, due to the ignorance of unique characteristics in IR. In this work, we propose to formulate item tagging as a link prediction problem between item nodes and tag nodes. To enrich the representation of items, we leverage the query logs available in IR tasks, and construct a query-item-tag tripartite graph. This formulation results in a TagGNN model that utilizes heterogeneous graph neural networks with multiple types of nodes and edges. Different from previous research, we also optimize both full tag prediction and partial tag completion cases in a unified framework via a primary-dual loss mechanism. Experimental results on both open and industrial datasets show that our TagGNN approach outperforms the state-of-the-art multi-label classification approaches.', 'venue': 'Annual International ACM SIGIR Conference on Research and Development in Information Retrieval', 'year': 2020, 'referenceCount': 47, 'citationCount': 9, 'openAccessPdf': {'url': 'http://arxiv.org/pdf/2008.11567', 'status': 'GREEN'}, 'tldr': {'model': 'tldr@v2.0.0', 'text': 'This work proposes to formulate item tagging as a link prediction problem between item nodes and tag nodes, and results in a TagGNN model that utilizes heterogeneous graph neural networks with multiple types of nodes and edges.'}, 'publicationTypes': ['JournalArticle', 'Book', 'Conference'], 'publicationDate': '2020-07-25', 'journal': {'name': 'Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval'}, 'authors': [{'authorId': '1580228663', 'name': 'Kelong Mao'}, {'authorId': '144432144', 'name': 'Xi Xiao'}, {'authorId': '2108997533', 'name': 'Jieming Zhu'}, {'authorId': '2064916670', 'name': 'Biao Lu'}, {'authorId': '2824766', 'name': 'Ruiming Tang'}, {'authorId': '1996703', 'name': 'Xiuqiang He'}]}]}\n",
            "CPU times: user 26.1 ms, sys: 1.07 ms, total: 27.2 ms\n",
            "Wall time: 2.39 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "def get_survey_papers(query=SURVEY_TOPIC_QUERY, survey_filters=SURVEY_FILTERS):\n",
        "  return search_all_by_keywords(query=query,\n",
        "                                fieldsOfStudy=survey_filters['fields_of_study'],\n",
        "                                year=survey_filters['year'],\n",
        "                                openAccessPdf=survey_filters['only_open_access'],\n",
        "                                maxNumberOfPapers=survey_filters['max_number_of_papers'])\n",
        "\n",
        "all_survey_papers = get_survey_papers(query=SURVEY_TOPIC_QUERY, survey_filters=SURVEY_FILTERS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgmchA8eMhEB",
        "outputId": "1c87fbe3-e33c-4192-d875-ae3478e1624c"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Searching from 0 to 100. Total: 3241\n",
            "Searching from 100 to 200. Total: 3241\n",
            "Searching from 200 to 300. Total: 3241\n",
            "Searching from 300 to 400. Total: 3241\n",
            "Searching from 400 to 500. Total: 3241\n",
            "Searching from 500 to 600. Total: 3241\n",
            "Searching from 600 to 700. Total: 3241\n",
            "Searching from 700 to 800. Total: 3241\n",
            "Searching from 800 to 900. Total: 3241\n",
            "Searching from 900 to 1000. Total: 3241\n",
            "Searching from 1000 to 1100. Total: 3229\n",
            "Searching from 1100 to 1200. Total: 3229\n",
            "Searching from 1200 to 1300. Total: 3229\n",
            "Searching from 1300 to 1400. Total: 3229\n",
            "Searching from 1400 to 1500. Total: 3229\n",
            "Searching from 1500 to 1600. Total: 3229\n",
            "Searching from 1600 to 1700. Total: 3229\n",
            "Searching from 1700 to 1800. Total: 3229\n",
            "Searching from 1800 to 1900. Total: 3229\n",
            "Searching from 1900 to 2000. Total: 3229\n",
            "Searching from 2000 to 2100. Total: 3229\n",
            "Searching from 2100 to 2200. Total: 3229\n",
            "Searching from 2200 to 2300. Total: 3229\n",
            "Searching from 2300 to 2400. Total: 3229\n",
            "Searching from 2400 to 2500. Total: 3229\n",
            "Searching from 2500 to 2600. Total: 3229\n",
            "Searching from 2600 to 2700. Total: 3229\n",
            "Searching from 2700 to 2800. Total: 3229\n",
            "Searching from 2800 to 2900. Total: 3229\n",
            "Searching from 2900 to 3000. Total: 3229\n",
            "Searching from 3000 to 3100. Total: 3229\n",
            "Searching from 3100 to 3200. Total: 3229\n",
            "Searching from 3200 to 3300. Total: 3229\n",
            "CPU times: user 822 ms, sys: 69 ms, total: 891 ms\n",
            "Wall time: 1min 15s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract embeddings of survey papers"
      ],
      "metadata": {
        "id": "MKRVTuCd18z-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a class to extract the embeddings using Specter"
      ],
      "metadata": {
        "id": "urKAchz-5_W9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Based on https://github.com/hwchase17/langchain/blob/4379bd4cbb8482e70d8936f747abd5ae7663f977/langchain/embeddings/huggingface.py#L16\n",
        "\n",
        "from torch import cuda, bfloat16\n",
        "from pydantic import BaseModel, Extra, Field\n",
        "from langchain.embeddings.base import Embeddings\n",
        "from typing import Any, Dict, List, Optional\n",
        "import torch\n",
        "from transformers import AutoAdapterModel, AutoTokenizer, AutoModel\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "#import transformers\n",
        "\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "class SpecterEmbeddings(BaseModel, Embeddings):\n",
        "\n",
        "  \"\"\"Key word arguments to pass to the model.\"\"\"\n",
        "  encode_kwargs: Dict[str, Any] = Field(default_factory=dict)\n",
        "\n",
        "  def __init__(self, **kwargs: Any):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained('allenai/specter2')\n",
        "    self.model = AutoModel.from_pretrained('allenai/specter2')\n",
        "\n",
        "    self.model.load_adapter(\"allenai/specter2_proximity\", source=\"hf\", load_as=\"specter2_proximity\", set_active=False)\n",
        "    self.model.load_adapter(\"allenai/specter2_adhoc_query\", source=\"hf\", load_as=\"adhoc_query\", set_active=False)\n",
        "\n",
        "    self.device = device\n",
        "    self.model.eval()\n",
        "    self.model.to(self.device)\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "    \"\"\"Compute doc embeddings using a HuggingFace transformer model.\n",
        "\n",
        "    Args:\n",
        "        texts: The list of texts to embed.\n",
        "\n",
        "    Returns:\n",
        "        List of embeddings, one for each text.\n",
        "    \"\"\"\n",
        "\n",
        "    self.model.set_active_adapters(None)\n",
        "    self.model.set_active_adapters(\"specter2_proximity\")\n",
        "\n",
        "    all_embeddings = []\n",
        "\n",
        "    batch_size = 32\n",
        "    show_progress_bar = True\n",
        "\n",
        "    if 'batch_size' in self.encode_kwargs:\n",
        "      batch_size = self.encode_kwargs['batch_size']\n",
        "    if 'show_progress_bar' in self.encode_kwargs:\n",
        "      show_progress_bar = self.encode_kwargs['show_progress_bar']\n",
        "\n",
        "    # sort text for less padding\n",
        "    length_sorted_idx = np.argsort([-len(sen) for sen in texts])\n",
        "    texts_sorted = [texts[idx] for idx in length_sorted_idx]\n",
        "\n",
        "    for start_index in tqdm(range(0, len(texts_sorted), batch_size), desc=\"Batches\", disable=not show_progress_bar):\n",
        "      texts_batch = texts_sorted[start_index:start_index+batch_size]\n",
        "\n",
        "      inputs = self.tokenizer(texts_batch, padding=True, truncation=True,\n",
        "                              return_tensors=\"pt\", return_token_type_ids=False, max_length=512)\n",
        "\n",
        "      output = self.model(**inputs.to(self.device))\n",
        "      # take the first token in the batch as the embedding\n",
        "      embeddings = output.last_hidden_state[:, 0, :]\n",
        "\n",
        "      all_embeddings.extend(embeddings.tolist())\n",
        "\n",
        "    return all_embeddings\n",
        "\n",
        "  class Config:\n",
        "      \"\"\"Configuration for this pydantic object.\"\"\"\n",
        "\n",
        "      extra = Extra.allow\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def embed_query(self, text: str) -> List[float]:\n",
        "    \"\"\"Compute query embeddings using a HuggingFace transformer model.\n",
        "\n",
        "    Args:\n",
        "        text: The text to embed.\n",
        "\n",
        "    Returns:\n",
        "        Embeddings for the text.\n",
        "    \"\"\"\n",
        "    self.model.set_active_adapters(None)\n",
        "    self.model.set_active_adapters(\"adhoc_query\")\n",
        "\n",
        "    inputs = self.tokenizer(text, padding=True, truncation=True,\n",
        "                            return_tensors=\"pt\", return_token_type_ids=False, max_length=512)\n",
        "\n",
        "    output = self.model(**inputs.to(self.device))\n",
        "    # take the first token in the batch as the embedding\n",
        "    embeddings = output.last_hidden_state[:, 0, :]\n",
        "\n",
        "    return embeddings.squeeze(0).tolist()"
      ],
      "metadata": {
        "id": "u_np8sahzuPq"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size_embeddings = 1 if device == 'cpu' else PARAMS['batch_size']\n",
        "\n",
        "embeddings = SpecterEmbeddings(encode_kwargs={'batch_size': batch_size_embeddings})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "3583f13ec4d949c08c56fa01f5859c9a",
            "bfd89e1eba2d423b8224efeb05c63d15",
            "74649c6d23b94f60b0c979bc0179abbd",
            "ef90a087008e4577a2d714caa3d7790f",
            "77e68db54b064e09bb10966b2acd8366",
            "c0c3e8dedaec463c9549cd282db4daf8",
            "c27431dfe5634bc08ec9b70fabe53e5f",
            "55547d8cdc5841eebfac1a38651daf9d",
            "2bd4227089d44d48a946c22f0a0348c2",
            "3b006f7b7e8743a69e82127d5c1a7eb5",
            "f63206b02dc74ba8a15d834b5c1e7291",
            "68abe864bcdb474dbc5b4621141b1c47",
            "0ef2475d179b4cb68dc97c5d0819afe4",
            "fcaee5efc5f0487483eda658faf1feb1",
            "d3cf82f28f5240f58ba8616106bdf9bb",
            "5862c25cda594078b5ae87ad61b25580",
            "02df5b2e56904f96bfe32d7c84ec29fe",
            "5505aa16711445f8bc010bdee7fbc7b8",
            "8c5b5fd032474e61abe68439ea58bd04",
            "a3cf1051d16b47a290188646fff2f9d1",
            "2ab7186eccbd410ea03677eb816d38ab",
            "f56ec00989444745ba4921ff4f758c81"
          ]
        },
        "id": "Lg2AlGaF2Fgr",
        "outputId": "17a595a9-fb8a-4a0b-d1b0-08bbb8072b5e"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3583f13ec4d949c08c56fa01f5859c9a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "68abe864bcdb474dbc5b4621141b1c47"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a list of Document containing the page_content and metadata of all survey papers. The metadata is the paper structured as returned by the Semantic Scholar."
      ],
      "metadata": {
        "id": "_3asFwR06Ec4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import Document\n",
        "\n",
        "all_documents = []\n",
        "all_documents_str = []\n",
        "for paper in all_survey_papers:\n",
        "  title = paper['title']\n",
        "  abstract = paper['abstract']\n",
        "  # Replace [SEP] to {sep} and None to ''\n",
        "  title = title.replace('[SEP]', '{sep}') if title is not None else ''\n",
        "  abstract = abstract.replace('[SEP]', '{sep}') if abstract is not None else ''\n",
        "\n",
        "  contents = f'{title} [SEP] {abstract}'\n",
        "\n",
        "  all_documents_str.append(contents)\n",
        "  all_documents.append(Document(page_content=contents, metadata=paper))"
      ],
      "metadata": {
        "id": "_wMHmRBs28_v"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "all_documents_embeddings = embeddings.embed_documents(all_documents_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "2200d9cb98784f75b1128ad4f69e3ae0",
            "b5407b545a2c4752928ead55e41a8b38",
            "730d69cb56c14d7ab72f914463a31678",
            "ec2696171050498ea8b83f4d9bbd786c",
            "6d6d8660911b47ed98c9cc74c6e1c699",
            "71edc248a097478faf7dd1a3898ec4e1",
            "d3df3521876040ab8fc0ba241683f03b",
            "f648fd9f9d8d438e87a70b0a994663b6",
            "f7b754f0563e48f49aec626699b8f476",
            "e3f6502fb983404aa71ec9ee146a2b31",
            "9dbff6fcf8a84c379234073a5e465cc0"
          ]
        },
        "id": "rTFaNKMz7D-h",
        "outputId": "afc8ddbf-2833-49be-f2b0-8da2a544377f"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/101 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2200d9cb98784f75b1128ad4f69e3ae0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1min 2s, sys: 180 ms, total: 1min 2s\n",
            "Wall time: 1min 1s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%time\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "vector_db_all_documents = FAISS.from_documents(all_documents, embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "a194be1efb134d41a959fe78ba170fa5",
            "175465a2ed514f3485bf4aa2a89363e1",
            "7e11e8f338594a77bb79b3c4560d8797",
            "2d6f86aeca34416d83cbbd438d0a9181",
            "cf9c5f2ca0414a628c3f8f33d615d507",
            "93a14110625f4883a37ac8a5c7fbcc49",
            "38fb1de5f2ab4cdfb78a8428139999a0",
            "082d7e1fa49d4674ba9728b067af5c7c",
            "23c808b4d9fa40ea9b5ea64ed2120d5e",
            "22f13f50dc4a4fef9c9e20160522d20c",
            "69cac5931b3545edb922afe197838c29"
          ]
        },
        "id": "C9qroiunAcdT",
        "outputId": "cafe9ecc-5824-4d84-ee29-2dba6396aa8d"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
            "Wall time: 6.2 µs\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/101 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a194be1efb134d41a959fe78ba170fa5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create sections"
      ],
      "metadata": {
        "id": "t0bCULSJ9lVy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method to split a list of documents embeddings in clusters:"
      ],
      "metadata": {
        "id": "YqDdRDa8BtLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import SpectralClustering, KMeans\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "def split_documents_into_clusters(document_embeddings, n_clusters, random_state=42):\n",
        "  # Create model\n",
        "  kmeans_model = KMeans(n_clusters=n_clusters, random_state=random_state, n_init='auto')\n",
        "\n",
        "  # Fit\n",
        "  kmeans_model.fit(document_embeddings)\n",
        "\n",
        "  # Cluster labels\n",
        "  labels = kmeans_model.labels_\n",
        "\n",
        "  # Centroids\n",
        "  cluster_centers = kmeans_model.cluster_centers_\n",
        "  distance_to_center = np.linalg.norm(document_embeddings - cluster_centers[labels], axis=1)\n",
        "\n",
        "  # Split documents into clusters. And desc-sort using the distance to the centroid of the cluster\n",
        "  index_docs_in_cluster = defaultdict(list)\n",
        "  distances_docs_in_cluster = defaultdict(list)\n",
        "  for idx_doc, label in enumerate(labels):\n",
        "    index_docs_in_cluster[label].append(idx_doc)\n",
        "    distances_docs_in_cluster[label].append(distance_to_center[idx_doc])\n",
        "\n",
        "  # Sort\n",
        "  for cluster in index_docs_in_cluster:\n",
        "    index_docs_in_cluster[cluster] = [idx_doc for _, idx_doc in sorted(zip(distances_docs_in_cluster[cluster], index_docs_in_cluster[cluster]), reverse=True)]\n",
        "    if DEBUG:\n",
        "      print(f\"Cluster {cluster}: {len(index_docs_in_cluster[cluster])}\")\n",
        "\n",
        "  return index_docs_in_cluster"
      ],
      "metadata": {
        "id": "owNlQzLR9q5H"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method to suggest the name of a section given a list of papers:"
      ],
      "metadata": {
        "id": "lrVJGULdB2cs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    PromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "\n",
        "# SYSTEM MESSAGE\n",
        "system_template = \"You are a renowned scientist who is writing a survey on '{survey_topic}'. You are currently writing a section about '{query_section}'\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
        "\n",
        "# FIRST HUMAN MESSAGE - EXPLAINING THE TASK\n",
        "human_template_task = \"\"\"\\\n",
        "I will send you a list of title and abstract of scientific articles. Most of them cover a specific topic about the section '{query_section}'.\\n\\\n",
        "Your task is to find out what this topic is and suggest a good title for a section in a scientific survey that addresses it.\\n\\\n",
        "You should also explain your reasoning.\\n\\\n",
        "Your answer should be a valid RFC8259 compliant JSON object with three properties:\\n\\\n",
        "The first property, called \"topic\", describes the main topic that is a subset of '{query_section}' that are present in most abstracts.\\n\\\n",
        "The second property, called \"title\", is the title of the section that will cover this topic and must be clearly related to the property \"topic\".\\n\\\n",
        "The third property is called \"reasoning\" and should contains your reasoning to choose this topic as an answer.\\n\\\n",
        "Use this format: \\\n",
        "\\n\\\n",
        "{{\\n\\\n",
        "  \"topic\": {{TOPIC}},\\n\\\n",
        "  \"title\": {{TITLE}},\\n\\\n",
        "  \"reasoning\": {{REASONING}}\\n\\\n",
        "}}\\n\\\n",
        "\n",
        "Do you understand?\n",
        "\"\"\"\n",
        "#The first property, called \"topic\", describes the topic and MUST be a subset of '{query_section}'. \\\n",
        "#The second property, called \"title\", is the title of the section that will cover this topic and must be clearly related to the property \"topic\". \\\n",
        "#The last property is called \"reasoning\" and should contains your reasoning to choose this topic as an answer. \\\n",
        "\n",
        "human_message_prompt_task = HumanMessagePromptTemplate.from_template(human_template_task)\n",
        "\n",
        "# FIRST AI ANSWER - AGREEING\n",
        "ai_message_prompt_yes = AIMessagePromptTemplate.from_template('Sure, send me the list and I will give you what you need.')\n",
        "\n",
        "# SECOND HUMAN MESSAGE - ABSTRACT AND TITLE\n",
        "# This is a variable message that depends on the papers in the section. Papers has the structured returned by the semantic scholar\n",
        "def text_message_human_prompt_papers_in_section(papers):\n",
        "  message = ''\n",
        "  for paper in papers:\n",
        "    if (paper['abstract'] is None or paper['title'] is None):\n",
        "      continue\n",
        "    # Trunc first 3000 letters of the abstract\n",
        "    message = message + f\"Title: {paper['title']}\\nAbstract: {paper['abstract'][0:3000]}\\n\\n\"\n",
        "\n",
        "  # Encode \"{\" and \"}\" from the message\n",
        "  message = message.replace(\"{\", \"{{\")\n",
        "  message = message.replace(\"}\", \"}}\")\n",
        "  return HumanMessagePromptTemplate.from_template(message)"
      ],
      "metadata": {
        "id": "X_LCoYi7CVuw"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "import ast\n",
        "\n",
        "# papers is a list of Document\n",
        "def suggest_title_for_section(papers, query_section):\n",
        "  llm_gpt = ChatOpenAI(temperature=0, openai_api_key=PARAMS['OPENAI_API_KEY'], model_name=PARAMS['gpt_model_name'])\n",
        "\n",
        "  message_few_shot_prompt = text_message_human_prompt_papers_in_section(papers)\n",
        "\n",
        "  # Now, generate the chat messages for this subsection\n",
        "  chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt,\n",
        "                                                  human_message_prompt_task,\n",
        "                                                  ai_message_prompt_yes,\n",
        "                                                  message_few_shot_prompt])\n",
        "\n",
        "  question_chain = LLMChain(llm=llm_gpt, prompt=chat_prompt, verbose=True)\n",
        "  #chat_messages = chat_prompt.format_prompt(survey_topic=SURVEY_TOPIC_QUERY, query_section=query_section).to_messages()\n",
        "  result_gpt = question_chain.run(survey_topic=SURVEY_TOPIC_QUERY, query_section=query_section)\n",
        "\n",
        "  if (DEBUG):\n",
        "    print(result_gpt)\n",
        "\n",
        "  open_curly_brackets = result_gpt.find('{')\n",
        "  close_curly_brackets = result_gpt.rfind('}')+1\n",
        "  json_str = result_gpt[open_curly_brackets:close_curly_brackets]\n",
        "  json_str = json_str.replace('true,', 'True,').replace('false,', 'False,')\n",
        "  return ast.literal_eval(json_str)"
      ],
      "metadata": {
        "id": "aUna-VonCEY4"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first level of sections use SURVEY_FILTERS['max_number_of_papers'] to define the clusters\n",
        "\n",
        "The other levels use SURVEY_STRUCTURE['n_papers_to_cluster_in_subsections'] to define the clusters. In this case, we search for the super section title in a database containing SURVEY_FILTERS['max_number_of_papers'] papers and retrieve only the top SURVEY_STRUCTURE['n_papers_to_cluster_in_subsections'] papers to define the cluster."
      ],
      "metadata": {
        "id": "c6w2qVVsBC4G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def populate_sections_with_suggested_titles(sections=sections,\n",
        "                                            super_sections_docs=all_documents,\n",
        "                                            super_sections_docs_embeddings=all_documents_embeddings,\n",
        "                                            query_super_section=SURVEY_TOPIC_QUERY,\n",
        "                                            random_state=42):\n",
        "  # Get the number of sections in this level\n",
        "  n_sec_in_level = len(sections)\n",
        "  # Split the documents into {n_sec_in_level} clusters\n",
        "  index_docs_in_section = split_documents_into_clusters(super_sections_docs_embeddings, n_sec_in_level, random_state)\n",
        "\n",
        "  if (DEBUG):\n",
        "    print(f\"Extracting {n_sec_in_level} sections in this level:\")\n",
        "\n",
        "  # Get a suggestion title for each section\n",
        "  for i_section in range(n_sec_in_level):\n",
        "    # Get the suggested title for each section\n",
        "    docs_to_suggest_title_section = [super_sections_docs[i].metadata for i in index_docs_in_section[i_section]]\n",
        "    docs_to_suggest_title_section = docs_to_suggest_title_section[0:SURVEY_STRUCTURE['n_papers_to_suggest_title_section']]\n",
        "    result_for_section = suggest_title_for_section(docs_to_suggest_title_section, query_super_section)\n",
        "    sections[i_section]['section_title'] = result_for_section['title']\n",
        "\n",
        "    # Now that we have a title for the section, we can use the vector_db to retrieve the references for this section with\n",
        "    # greater precision\n",
        "    # Set the query subsection as all the name of the sections untill now, but removing the survey_topic_query of the variable\n",
        "    # We don't want the survey_topic_query at the begining because we will reuse this variable later without this information\n",
        "    query_section = f\"{query_super_section} - {sections[i_section]['section_title']}\"\n",
        "    query_section = query_section[len(SURVEY_TOPIC_QUERY)+3:] if query_section.startswith(SURVEY_TOPIC_QUERY) else query_section\n",
        "    docs_to_use_as_ref = vector_db_all_documents.similarity_search(f\"{SURVEY_TOPIC_QUERY} - {query_section}\", SURVEY_STRUCTURE['max_papers_per_section_as_ref'])\n",
        "    sections[i_section]['papers'] = [doc.metadata for doc in docs_to_use_as_ref]\n",
        "\n",
        "    # Save and print if debug=True\n",
        "    if (DEBUG):\n",
        "      sections[i_section]['result_for_section'] = result_for_section\n",
        "      print(f'Results for section {i_section}')\n",
        "      print(result_for_section)\n",
        "      print('.'*100)\n",
        "\n",
        "    # Now, populate each subsection\n",
        "    if (len(sections[i_section]['subsections']) > 0):\n",
        "      if (DEBUG):\n",
        "        print('Extracting subsections...')\n",
        "      # Extract the top docs of the query to extract the subsections:\n",
        "      docs_in_section = vector_db_all_documents.similarity_search(query_section, SURVEY_STRUCTURE['n_papers_to_cluster_in_subsections'])\n",
        "      # Todos os embeddings já foram calculados previamente e estão na variável all_documents_embeddings\n",
        "      # Daria pra pegar diretamente deles se tivéssemos os índices de docs_in_section.\n",
        "      # É possível também simplesmente varrer o all_documents buscando os documentos iguais ao dos docs_in_section\n",
        "      # e pegar o embedding correspondente. Mas pra facilitar o código e como isso aqui é rápido, vamos só calcular novamente mesmo\n",
        "      docs_embeddings_in_section = embeddings.embed_documents([doc.page_content for doc in docs_in_section])\n",
        "\n",
        "      populate_sections_with_suggested_titles(sections[i_section]['subsections'], docs_in_section, docs_embeddings_in_section, query_section, random_state)"
      ],
      "metadata": {
        "id": "qm2ejTMn_Oaw"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "sections = []\n",
        "\n",
        "populate_with_empty_sections(sections, SURVEY_STRUCTURE['n_sections'])\n",
        "populate_sections_with_suggested_titles(sections=sections,\n",
        "                                        super_sections_docs=all_documents,\n",
        "                                        super_sections_docs_embeddings=all_documents_embeddings,\n",
        "                                        query_super_section=SURVEY_TOPIC_QUERY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "aa57f5794886470289b4239b810e9049",
            "7f587605bd10480599ee7cfaeb0d56cb",
            "4572efbc1f904425a02153f8658a1336",
            "a6119bd0242443da804bbaabf1e585bf",
            "7bb1aa7efd60473a9f3b0fc12aca7a0d",
            "5a6073202e9745fa89e56be0418198c5",
            "52897c8aaf97456397e4d6575a9ee5bf",
            "bc55db25702947368d10987828162cd8",
            "ffa5964f347a43d6b2d83664b510b32b",
            "dfa9b96bdc9c431388f4be23c1314962",
            "d5596691c2904783b0fdb96cf022ff97",
            "68fde3edfa624e9cafa9c46abedd9b9e",
            "471fa00c991f455fbcf954962e98232e",
            "351c1e8753d749a6ac6b36cdf0867bfb",
            "cbf297e3587147aba77d8132ff50147d",
            "13f40cfb243249948cc6eeac7d87de0f",
            "76f56b47bbef411d8991ab31c3495cff",
            "9acdd1c32d484f4fbbbbdc7fadf97b24",
            "4f029aa5fb36451ea28c183e81d97a59",
            "41e05c3756aa40c7b26c12bc12e76631",
            "b3224b12fbd943a78328055bd8b4fdb1",
            "f664f4fc8ffd4201abe45362b7a6d810",
            "27ea8e1b05264095a468e705fdfaefa2",
            "0b4bdb98c8da473ab87daeea44c06bf4",
            "22e237ac8f784615a2b8dd13e3f2cbc7",
            "1f28da6a7ce143d1bfd067077d3dc0fb",
            "b10eac84f9f34147a274e2c8b1486fd9",
            "16034ba8d65d4ef488a63499fe49c7a1",
            "57ec6862a1b34dab923e2d52a20c422a",
            "a52665149d6f4595b39ad29125d6a466",
            "8b297fcde23548b88c9d1f306209a7de",
            "0e7c2d62ed2e48d4a7ca84889e8c8eea",
            "2f9a1f078b7c499098b6c3bc529128a7",
            "fe0ba0cd32174fc5a6faba6b5e981060",
            "33a213706c5146a3a6629afe7d19c6c7",
            "2da500ad4e194e98bcd933bffe5c76f5",
            "43e85765543b4d139c57e866bcefda82",
            "8141f965239347f69763c6be11508cec",
            "9c54e74bde624f8a95fd5cd145bdc159",
            "6cba0dc98de843cc9d5772cf5dc08daa",
            "0d37b9777d1448c99fc6bde45d8e525b",
            "d8a71a01121243f9bc609b7fb8eb87ad",
            "ed80f436acdb4d3091b6d47c75f34881",
            "0a7c3dd5973d4209843bae3916965c4d",
            "51a2b4396f4d40189890f3f454035d0f",
            "2727cd985ac84240ac2e49c091f3e7aa",
            "5f9dcb8f8b624a3c91907bbd3695a2da",
            "d165be1aa4ec407486384ca395ba3460",
            "7bab703985ee472eb183dd5dfe1ebd1c",
            "245536790dc34fc0b16252aa72a4116f",
            "a63d4f061f4e468cb1e9340753fca7e8",
            "f592edab70aa428488125c5cffd501b2",
            "794d46c1873b436cb552139fcc5e2f83",
            "8a23292148264ee3a96b68831119309a",
            "715fc585ed044c2d99075bc4996660b2"
          ]
        },
        "id": "vEf2cYHNhDLL",
        "outputId": "511193ca-8371-4334-9aa8-ce7b21d4d008"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster 4: 496\n",
            "Cluster 2: 836\n",
            "Cluster 0: 562\n",
            "Cluster 1: 496\n",
            "Cluster 3: 839\n",
            "Extracting 5 sections in this level:\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: You are a renowned scientist who is writing a survey on 'text neural information retrieval'. You are currently writing a section about 'text neural information retrieval'\n",
            "Human: I will send you a list of title and abstract of scientific articles. Most of them cover a specific topic about the section 'text neural information retrieval'.\n",
            "Your task is to find out what this topic is and suggest a good title for a section in a scientific survey that addresses it.\n",
            "You should also explain your reasoning.\n",
            "Your answer should be a valid RFC8259 compliant JSON object with three properties:\n",
            "The first property, called \"topic\", describes the main topic that is a subset of 'text neural information retrieval' that are present in most abstracts.\n",
            "The second property, called \"title\", is the title of the section that will cover this topic and must be clearly related to the property \"topic\".\n",
            "The third property is called \"reasoning\" and should contains your reasoning to choose this topic as an answer.\n",
            "Use this format: \n",
            "{\n",
            "  \"topic\": {TOPIC},\n",
            "  \"title\": {TITLE},\n",
            "  \"reasoning\": {REASONING}\n",
            "}\n",
            "\n",
            "Do you understand?\n",
            "\n",
            "AI: Sure, send me the list and I will give you what you need.\n",
            "Human: Title: Hybrid Japanese Language Teaching Aid System with Multi-Source Information Fusion Mapping\n",
            "Abstract: Learning Japanese can enhance competitiveness in a globalized economy, and we address the problems of poor open-source Japanese language teaching, cumbersome teaching tasks, and a single teaching model. We propose a hybrid Japanese teaching aid system with multiple information fusion mapping, which can effectively improve the efficiency of Japanese teaching and reduce the tedious human teaching procedures. The system is divided into two branches of Japanese language recognition, namely, the Japanese text recognition branch and the Japanese voice sequence recognition branch. In the Japanese text recognition branch, we integrate attention mechanisms and long short-term memory networks as the basic network for Japanese character text recognition. In addition, we set up separate text feature recognition systems for Japanese computer writing and handwriting to prevent feature overlap problems. For Japanese voice sequence recognition, we used a combination of memory gating unit and encoder, based on the network still extending the structure of the deep neural network and using the residual structure connection in the gating unit to avoid the gradient disappearance problem. At the end of the system, we use a softmax layer to connect the text recognition and voice recognition networks to form a Japanese language teaching aid system. To verify the efficiency of our system, we selected the Japanese text recognition public dataset and voice recognition public dataset for experimental validation. To match the practical application of the system, we created our dataset based on the dataset standard and conducted experimental validation. To compare other Japanese recognition methods, we selected the six most representative Japanese recognition algorithms for experimental comparison. To ensure the balance of the experiments, each algorithm is trained in a separate experimental environment for modeling and tuning parameters. Experimental performance and the experimental results show that our method significantly outperforms the other methods and has better system stability.\n",
            "\n",
            "Title: Two-Stream Network Based on Visual Saliency Sharing for 3D Model Recognition\n",
            "Abstract: Shape representation for 3D models is an important topic in computer vision, multimedia analysis, and computer graphics. Recent multiview-based methods demonstrate promising performance for 3D model recognition and retrieval. However, most of the multiview-based methods focus on the visual information from the taken views and ignore correlation information among these views, which means the similarity and differentiation of multiple views have lost in their methods. In order to address this issue, we propose a novel two-stream network architecture for 3D model recognition and retrieval. The proposed network includes two sub-networks: a multi-view convolutional neural network (MVCNN) that extracts the view information from the taken views, and an Visual Saliency model that defines the weight of views based on the similarity and differentiation information of multiple views. Special, the weight of views defined by the Visual Saliency model can effectively be used to guide the visual information fusion in MVCNN model. This design can make the MVCNN model save visual information and the correlation information of these views in the learning step. Finally, we employ early-fusion method to fuse the feature vectors from MVCNN model and Visual Saliency model respectively, to generate the shape descriptor for 3D model recognition and retrieval. The experimental result on two public datasets, ModelNet40 and ShapeNetCore55, demonstrates the correlation information of multiple views is crucial for view-based 3D model recognition methods and the proposed method can achieve the state-of-the-art performance on both 3D object classification and retrieval.\n",
            "\n",
            "Title: Deep Fusion Cnn Based Hybridized Strategy for Image Retrieval in Web: A Novel Data Fusion Technique\n",
            "Abstract: There is always a need for an efficient and effective Content Based Image Retrieval (CBIR) classification due to the constant development in the number of large-scale repository. During the last few years, there has been a tremendous increase in activity in using multimedia data including both scientific and commercial domains. As a result, it's necessary to organize, store, analyze, and present available facts that meet user needs. Visual components are used in CBIR to find a picture from a large image document based on the user's interest and instantaneously query sequence attributes. The term 'content' may relate to the image's low-level properties such as color, form, or material. The need for CBIR arises because most image retrieval algorithms rely solely on textual information, resulting in a lot of garbage in the outcomes. Furthermore, searching for photos in a large database using keywords might be costly, inefficient, and fail to convey the user's purpose to describe the picture. To address this, the proposed research suggests \"JustClick\": a unique data fusion approach based on the Deep Fusion Convolution Neural Network (DFCNN) method for enhanced extraction of features. With the notion of intent research, this approach hybridizes linguistic and visual commonalities to capture the user's purpose. Only one click on a query picture is required for the images returned by text-based searches to be re-ranked dependent on their linguistic and visual similarity to the image database. The suggested system's performance is proved by making comparisons to text-based and content-based systems. The suggested JustClick system provides an effective automatic retrieval of comparable photos with better extracting of the features, yielding encouraging results with retrieving effectiveness of 97.7% on average.\n",
            "\n",
            "Title: Neural Extractive Summarization with Hierarchical Attentive Heterogeneous Graph Network\n",
            "Abstract: Sentence-level extractive text summarization is substantially a node classification task of network mining, adhering to the informative components and concise representations. There are lots of redundant phrases between extracted sentences, but it is difficult to model them exactly by the general supervised methods. Previous sentence encoders, especially BERT, specialize in modeling the relationship between source sentences. While, they have no ability to consider the overlaps of the target selected summary, and there are inherent dependencies among target labels of sentences. In this paper, we propose HAHSum (as shorthand for Hierarchical Attentive Heterogeneous Graph for Text Summarization), which well models different levels of information, including words and sentences, and spotlights redundancy dependencies between sentences. Our approach iteratively refines the sentence representations with redundancy-aware graph and delivers the label dependencies by message passing. Experiments on large scale benchmark corpus (CNN/DM, NYT, and NEWSROOM) demonstrate that HAHSum yields ground-breaking performance and outperforms previous extractive summarizers.\n",
            "\n",
            "Title: AutoRet: A Self-Supervised Spatial Recurrent Network for Content-Based Image Retrieval\n",
            "Abstract: Image retrieval techniques are becoming famous due to the vast availability of multimedia data. The present image retrieval system performs excellently on labeled data. However, often, data labeling becomes costly and sometimes impossible. Therefore, self-supervised and unsupervised learning strategies are currently becoming illustrious. Most of the self/unsupervised strategies are sensitive to the number of classes and can not mix labeled data on availability. In this paper, we introduce AutoRet, a deep convolutional neural network (DCNN) based self-supervised image retrieval system. The system is trained on pairwise constraints. Therefore, it can work in self-supervision and can also be trained on a partially labeled dataset. The overall strategy includes a DCNN that extracts embeddings from multiple patches of images. Further, the embeddings are fused for quality information used for the image retrieval process. The method is benchmarked with three different datasets. From the overall benchmark, it is evident that the proposed method works better in a self-supervised manner. In addition, the evaluation exhibits the proposed method’s performance to be highly convincing while a small portion of labeled data are mixed on availability.\n",
            "\n",
            "Title: Contextualized Query Embeddings for Conversational Search\n",
            "Abstract: This paper describes a compact and effective model for low-latency passage retrieval in conversational search based on learned dense representations. Prior to our work, the state-of-the-art approach uses a multi-stage pipeline comprising conversational query reformulation and information retrieval modules. Despite its effectiveness, such a pipeline often includes multiple neural models that require long inference times. In addition, independently optimizing each module ignores dependencies among them. To address these shortcomings, we propose to integrate conversational query reformulation directly into a dense retrieval model. To aid in this goal, we create a dataset with pseudo-relevance labels for conversational search to overcome the lack of training data and to explore different training strategies. We demonstrate that our model effectively rewrites conversational queries as dense representations in conversational search and open-domain question answering datasets. Finally, after observing that our model learns to adjust the L2 norm of query token embeddings, we leverage this property for hybrid retrieval and to support error analysis.\n",
            "\n",
            "Title: Leveraging Equivariant Features for Absolute Pose Regression\n",
            "Abstract: While end-to-end approaches have achieved state-of-the-art performance in many perception tasks, they are not yet able to compete with 3D geometry-based methods in pose estimation. Moreover, absolute pose regression has been shown to be more related to image retrieval. As a result, we hypothesize that the statistical features learned by classical Convolutional Neural Networks do not carry enough geometric information to reliably solve this inherently geometric task. In this paper, we demonstrate how a translation and rotation equivariant Convolutional Neural Network directly induces representations of camera motions into the feature space. We then show that this geometric property allows for implicitly augmenting the training data under a whole group of image plane-preserving transformations. Therefore, we argue that directly learning equivariant features is preferable than learning data-intensive intermediate representations. Comprehensive experimental validation demonstrates that our lightweight model outperforms existing ones on standard datasets. 1\n",
            "\n",
            "Title: Text classification with semantically enriched word embeddings\n",
            "Abstract: Abstract The recent breakthroughs in deep neural architectures across multiple machine learning fields have led to the widespread use of deep neural models. These learners are often applied as black-box models that ignore or insufficiently utilize a wealth of preexisting semantic information. In this study, we focus on the text classification task, investigating methods for augmenting the input to deep neural networks (DNNs) with semantic information. We extract semantics for the words in the preprocessed text from the WordNet semantic graph, in the form of weighted concept terms that form a semantic frequency vector. Concepts are selected via a variety of semantic disambiguation techniques, including a basic, a part-of-speech-based, and a semantic embedding projection method. Additionally, we consider a weight propagation mechanism that exploits semantic relationships in the concept graph and conveys a spreading activation component. We enrich word2vec embeddings with the resulting semantic vector through concatenation or replacement and apply the semantically augmented word embeddings on the classification task via a DNN. Experimental results over established datasets demonstrate that our approach of semantic augmentation in the input space boosts classification performance significantly, with concatenation offering the best performance. We also note additional interesting findings produced by our approach regarding the behavior of term frequency - inverse document frequency normalization on semantic vectors, along with the radical dimensionality reduction potential with negligible performance loss.\n",
            "\n",
            "Title: A Cognitive Method for Automatically Retrieving Complex Information on a Large Scale\n",
            "Abstract: Modern retrieval systems tend to deteriorate because of their large output of useless and even misleading information, especially for complex search requests on a large scale. Complex information retrieval (IR) tasks requiring multi-hop reasoning need to fuse multiple scattered text across two or more documents. However, there are two challenges for multi-hop retrieval. To be specific, the first challenge is that since some important supporting facts have little lexical or semantic relationship with the retrieval query, the retriever often omits them; the second challenge is that once a retriever chooses misinformation related to the query as the entities of cognitive graphs, the retriever will fail. In this study, in order to improve the performance of retrievers in complex tasks, an intelligent sensor technique was proposed based on a sub-scope with cognitive reasoning (2SCR-IR), a novel method of retrieving reasoning paths over the cognitive graph to provide users with verified multi-hop reasoning chains. Inspired by the users’ process of step-by-step searching online, 2SCR-IR includes a dynamic fusion layer that starts from the entities mentioned in the given query, explores the cognitive graph dynamically built from the query and contexts, gradually finds relevant supporting entities mentioned in the given documents, and verifies the rationality of the retrieval facts. Our experimental results show that 2SCR-IR achieves competitive results on the HotpotQA full wiki and distractor settings, and outperforms the previous state-of-the-art methods by a more than two points absolute gain on the full wiki setting.\n",
            "\n",
            "Title: Bi-Directional Recurrent Neural Ordinary Differential Equations for Social Media Text Classification\n",
            "Abstract: Classification of posts in social media such as Twitter is difficult due to the noisy and short nature of texts. Sequence classification models based on recurrent neural networks (RNN) are popular for classifying posts that are sequential in nature. RNNs assume the hidden representation dynamics to evolve in a discrete manner and do not consider the exact time of the posting. In this work, we propose to use recurrent neural ordinary differential equations (RNODE) for social media post classification which consider the time of posting and allow the computation of hidden representation to evolve in a time-sensitive continuous manner. In addition, we propose a novel model, Bi-directional RNODE (Bi-RNODE), which can consider the information flow in both the forward and backward directions of posting times to predict the post label. Our experiments demonstrate that RNODE and Bi-RNODE are effective for the problem of stance classification of rumours in social media.\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{\n",
            "  \"topic\": \"Semantic Information Fusion for Text Neural Information Retrieval\",\n",
            "  \"title\": \"Semantic Fusion Strategies for Text Neural Information Retrieval\",\n",
            "  \"reasoning\": \"Based on the abstracts provided, the common theme among the articles is the fusion of different sources of information to improve text neural information retrieval. This includes the fusion of Japanese text and voice recognition, visual saliency sharing for 3D model recognition, deep fusion CNN for image retrieval, hierarchical attentive heterogeneous graph network for extractive summarization, self-supervised spatial recurrent network for content-based image retrieval, contextualized query embeddings for conversational search, leveraging equivariant features for absolute pose regression, semantically enriched word embeddings for text classification, cognitive reasoning for complex information retrieval, and recurrent neural ordinary differential equations for social media text classification. Therefore, the main topic is the fusion of semantic information for text neural information retrieval, which involves integrating different sources of information to enhance the retrieval process. The suggested title, 'Semantic Fusion Strategies for Text Neural Information Retrieval', accurately reflects this topic.\" \n",
            "}\n",
            "Results for section 0\n",
            "{'topic': 'Semantic Information Fusion for Text Neural Information Retrieval', 'title': 'Semantic Fusion Strategies for Text Neural Information Retrieval', 'reasoning': \"Based on the abstracts provided, the common theme among the articles is the fusion of different sources of information to improve text neural information retrieval. This includes the fusion of Japanese text and voice recognition, visual saliency sharing for 3D model recognition, deep fusion CNN for image retrieval, hierarchical attentive heterogeneous graph network for extractive summarization, self-supervised spatial recurrent network for content-based image retrieval, contextualized query embeddings for conversational search, leveraging equivariant features for absolute pose regression, semantically enriched word embeddings for text classification, cognitive reasoning for complex information retrieval, and recurrent neural ordinary differential equations for social media text classification. Therefore, the main topic is the fusion of semantic information for text neural information retrieval, which involves integrating different sources of information to enhance the retrieval process. The suggested title, 'Semantic Fusion Strategies for Text Neural Information Retrieval', accurately reflects this topic.\"}\n",
            "....................................................................................................\n",
            "Extracting subsections...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa57f5794886470289b4239b810e9049"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster 0: 12\n",
            "Cluster 2: 12\n",
            "Cluster 1: 26\n",
            "Extracting 3 sections in this level:\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: You are a renowned scientist who is writing a survey on 'text neural information retrieval'. You are currently writing a section about 'Semantic Fusion Strategies for Text Neural Information Retrieval'\n",
            "Human: I will send you a list of title and abstract of scientific articles. Most of them cover a specific topic about the section 'Semantic Fusion Strategies for Text Neural Information Retrieval'.\n",
            "Your task is to find out what this topic is and suggest a good title for a section in a scientific survey that addresses it.\n",
            "You should also explain your reasoning.\n",
            "Your answer should be a valid RFC8259 compliant JSON object with three properties:\n",
            "The first property, called \"topic\", describes the main topic that is a subset of 'Semantic Fusion Strategies for Text Neural Information Retrieval' that are present in most abstracts.\n",
            "The second property, called \"title\", is the title of the section that will cover this topic and must be clearly related to the property \"topic\".\n",
            "The third property is called \"reasoning\" and should contains your reasoning to choose this topic as an answer.\n",
            "Use this format: \n",
            "{\n",
            "  \"topic\": {TOPIC},\n",
            "  \"title\": {TITLE},\n",
            "  \"reasoning\": {REASONING}\n",
            "}\n",
            "\n",
            "Do you understand?\n",
            "\n",
            "AI: Sure, send me the list and I will give you what you need.\n",
            "Human: Title: Context-Aware Misinformation Detection: A benchmark of Deep Learning Architectures using Word Embeddings\n",
            "Abstract: New mass media paradigms for information distribution have emerged with the digital age. With new digital-enabled mass media, the communication process is centered around the user, while multimedia content is the new identity of news. Thus, the media landscape has shifted from mass media to personalized social media. While this progress brings advantages, it also carries the risk of being detrimental to society through the emergence of misinformation (false or inaccurate information) and disinformation (intentionally spreading misinformation) in the form of fake news. Fake news is a tool used to manipulate public opinion on particular topics, distort public perceptions, and generate social unrest while lacking the rigor of traditional journalism. Driven by this current and real-world problem, in this paper, we train multiple Deep Learning architectures for multi-class classification and compare their performance in detecting the veracity of the news articles. To achieve accurate models in detecting misinformation, we employ a large dataset containing 100 000 news articles labeled with ten classes (one with real news and the rest with different types of fake news). We use two preprocessing techniques, i.e., one simple and another very aggressive, to clean the dataset. We also employ three word embeddings that preserve the word context, i.e., Word2Vec, FastText, and GloVe, pre-trained and trained on our dataset to vectorize the preprocessed dataset. For the misinformation task, we train a Logistic Regression as a baseline and compare its results with the performance of ten Deep Learning architectures. We obtain the best results using a Recurrent Convolutional Neural Network based architecture. The experimental results show that the models are highly dependable on text preprocessing and the word embedding employed.\n",
            "\n",
            "Title: Extracting locations from sport and exercise-related social media messages using a neural network-based bilingual toponym recognition model\n",
            "Abstract: Sport and exercise contribute to health and well-being in cities. While previous research has mainly focused on activities at specific locations such as sport facilities, \"informal sport\" that occur at arbitrary locations across the city have been largely neglected. Such activities are more challenging to observe, but this challenge may be addressed using data collected from social media platforms, because social media users regularly generate content related to sports and exercise at given locations. This allows studying all sport, including those \"informal sport\" which are at arbitrary locations, to better understand sports and exercise-related activities in cities. However, user-generated geographical information available on social media platforms is becoming scarcer and coarser. This places increased emphasis on extracting location information from free-form text content on social media, which is complicated by multilingualism and informal language. To support this effort, this article presents an end-to-end deep learning-based bilingual toponym recognition model for extracting location information from social media content related to sports and exercise. We show that our approach outperforms five state-of-the-art deep learning and machine learning models. We further demonstrate how our model can be deployed in a geoparsing framework to support city planners in promoting healthy and active lifestyles.\n",
            "\n",
            "Title: Exploring Molecular Mechanisms Within Biomedical Literature\n",
            "Abstract: Text mining of the biomedical literature enables vast quantities of information to be extracted and summarised. Here we describe an updated and improved version of previous methodology for the analysis of gene and protein biomarkers that enables the use of the newer Pubtator Central annotations, based in full text, improving the performance using a local SQLite database, that reduces the running time and resources required to perform the analyses facilitating its use in any computer, and expands its capabilities to enable the retrieval and analysis of chemical and metabolic biomarkers.\n",
            "\n",
            "Title: Современные методы детектирования и классификации токсичных комментариев с использованием нейронных сетей\n",
            "Abstract: The growth of popularity of online platforms which allow users to communicate with each other, share opinions about various events, and leave comments boosted the development of natural language processing algorithms. Tens of millions of messages per day are published by users of a particular social network need to be analyzed in real time for moderation in order to prevent the spread of various illegal or offensive information, threats and other types of toxic comments. Of course, such a large amount of information can be processed quite quickly only automatically. that is why there is a need to and a way to teach computers to “understand” a text written by humans. It is a non-trivial task even if the word “understand” here means only “to classify”. the rapid evolution of machine learning technologies has led to ubiquitous implementation of new algorithms. A lot of tasks, which for many years were considered almost impossible to solve, are now quite successfully solved using deep learning technologies. this article considers algorithms built using deep learning technologies and neural networks which can successfully solve the problem of detection and classification of toxic comments. In addition, the article presents the results of the developed algorithms, as well as the results of the ensemble of all considered algorithms on a large training set collected and tagged by Google and Jigsaw.\n",
            "\n",
            "Title: MRE: A Military Relation Extraction Model Based on BiGRU and Multi-Head Attention\n",
            "Abstract: A great deal of operational information exists in the form of text. Therefore, extracting operational information from unstructured military text is of great significance for assisting command decision making and operations. Military relation extraction is one of the main tasks of military information extraction, which aims at identifying the relation between two named entities from unstructured military texts. However, the traditional methods of extracting military relations cannot easily resolve problems such as inadequate manual features and inaccurate Chinese word segmentation in military fields, failing to make full use of symmetrical entity relations in military texts. With our approach, based on the pre-trained language model, we present a Chinese military relation extraction method, which combines the bi-directional gate recurrent unit (BiGRU) and multi-head attention mechanism (MHATT). More specifically, the conceptual foundation of our method lies in constructing an embedding layer and combining word embedding with position embedding, based on the pre-trained language model; the output vectors of BiGRU neural networks are symmetrically spliced to learn the semantic features of context, and they fuse the multi-head attention mechanism to improve the ability of expressing semantic information. On the military text corpus that we have built, we conduct extensive experiments. We demonstrate the superiority of our method over the traditional non-attention model, attention model, and improved attention model, and the comprehensive evaluation value F1-score of the model is improved by about 4%.\n",
            "\n",
            "Title: An efficient approach for textual data classification using deep learning\n",
            "Abstract: Text categorization is an effective activity that can be accomplished using a variety of classification algorithms. In machine learning, the classifier is built by learning the features of categories from a set of preset training data. Similarly, deep learning offers enormous benefits for text classification since they execute highly accurately with lower-level engineering and processing. This paper employs machine and deep learning techniques to classify textual data. Textual data contains much useless information that must be pre-processed. We clean the data, impute missing values, and eliminate the repeated columns. Next, we employ machine learning algorithms: logistic regression, random forest, K-nearest neighbors (KNN), and deep learning algorithms: long short-term memory (LSTM), artificial neural network (ANN), and gated recurrent unit (GRU) for classification. Results reveal that LSTM achieves 92% accuracy outperforming all other model and baseline studies.\n",
            "\n",
            "Title: Tracking Your Mind's Eye during Recollection: Decoding the Long-Term Recall of Short Audiovisual Clips\n",
            "Abstract: Unlike familiarity, recollection involves the ability to reconstruct mentally previous events that results in a strong sense of reliving. According to the reinstatement hypothesis, this specific feature emerges from the reactivation of cortical patterns involved during information exposure. Over time, the retrieval of specific details becomes more difficult, and memories become increasingly supported by familiarity judgments. The multiple trace theory (MTT) explains the gradual loss of episodic details by a transformation in the memory representation, a view that is not shared by the standard consolidation model. In this study, we tested the MTT in light of the reinstatement hypothesis. The temporal dynamics of mental imagery from long-term memory were investigated and tracked over the passage of time. Participant EEG activity was recorded during the recall of short audiovisual clips that had been watched 3 weeks, 1 day, or a few hours beforehand. The recall of the audiovisual clips was assessed using a Remember/Know/New procedure, and snapshots of clips were used as recall cues. The decoding matrices obtained from the multivariate pattern analyses revealed sustained patterns that occurred at long latencies (>500 msec poststimulus onset) that faded away over the retention intervals and that emerged from the same neural processes. Overall, our data provide further evidence toward the MTT and give new insights into the exploration of our “mind's eye.”\n",
            "\n",
            "Title: Construction and Optimization of Distributed Electronic Archives System Merging Control Messages\n",
            "Abstract: This paper analyzes the whole of the electronic archives management system, describes the basic features and functions of the system, and gives the flow of each functional module of the system. Techniques for implementing important functions in the system are discussed and compared. Using the object-oriented idea, this paper designs the electronic archives management system and gives the hybrid structure model of the system. According to the needs of the electronic office, an official document transmission module is established to realize the electronic transmission of official documents, and the specific implementation process is given. According to the characteristics of archives information management and utilization, SQL Server is used as the background database and the database design is carried out. This paper introduces the full-text retrieval technology that realizes the query and retrieval function of electronic archives and gives how to configure the full-text retrieval service in SQL Server. This paper proposes a K-means-based merging control message clustering algorithm to solve the problem of communication topology changes caused by large-scale, high-mobility merging control messages. Based on the idea of K-means clustering, the calculation method of distance in K-means is improved by combining the distribution of the initial electronic archives message, and the fitness function of the electronic archives is designed by using the information of the electronic archives in the cluster and the average distance to the neighboring electronic archives. According to the maximum fitness value, the cluster head is selected to establish clustering. The file management system is tested and analyzed in a simulated environment. Based on the functional positioning of the system and the business environment, the functional performance and performance of the analysis system are examined and analyzed, and the research and development results are inspected according to the system development goals to check whether the development results of the system have reached the advanced stage. At the functional level, it can automate the management of the core business of the distributed electronic archives management department and support the distributed storage management of massive archive data, which effectively improves the informatization level and development efficiency of archives management.\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{\n",
            "  \"topic\": \"Text Classification for Misinformation Detection\",\n",
            "  \"title\": \"Deep Learning Approaches for Misinformation Detection in Text\",\n",
            "  \"reasoning\": \"The abstract discusses the use of deep learning architectures for detecting the veracity of news articles, specifically focusing on the detection of misinformation and fake news. The paper explores different preprocessing techniques and word embeddings to vectorize the dataset and compares the performance of various deep learning architectures. This topic is a subset of 'Semantic Fusion Strategies for Text Neural Information Retrieval' as it involves the fusion of semantic information from text to detect and classify misinformation.\"\n",
            "}\n",
            "Results for section 0\n",
            "{'topic': 'Text Classification for Misinformation Detection', 'title': 'Deep Learning Approaches for Misinformation Detection in Text', 'reasoning': \"The abstract discusses the use of deep learning architectures for detecting the veracity of news articles, specifically focusing on the detection of misinformation and fake news. The paper explores different preprocessing techniques and word embeddings to vectorize the dataset and compares the performance of various deep learning architectures. This topic is a subset of 'Semantic Fusion Strategies for Text Neural Information Retrieval' as it involves the fusion of semantic information from text to detect and classify misinformation.\"}\n",
            "....................................................................................................\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: You are a renowned scientist who is writing a survey on 'text neural information retrieval'. You are currently writing a section about 'Semantic Fusion Strategies for Text Neural Information Retrieval'\n",
            "Human: I will send you a list of title and abstract of scientific articles. Most of them cover a specific topic about the section 'Semantic Fusion Strategies for Text Neural Information Retrieval'.\n",
            "Your task is to find out what this topic is and suggest a good title for a section in a scientific survey that addresses it.\n",
            "You should also explain your reasoning.\n",
            "Your answer should be a valid RFC8259 compliant JSON object with three properties:\n",
            "The first property, called \"topic\", describes the main topic that is a subset of 'Semantic Fusion Strategies for Text Neural Information Retrieval' that are present in most abstracts.\n",
            "The second property, called \"title\", is the title of the section that will cover this topic and must be clearly related to the property \"topic\".\n",
            "The third property is called \"reasoning\" and should contains your reasoning to choose this topic as an answer.\n",
            "Use this format: \n",
            "{\n",
            "  \"topic\": {TOPIC},\n",
            "  \"title\": {TITLE},\n",
            "  \"reasoning\": {REASONING}\n",
            "}\n",
            "\n",
            "Do you understand?\n",
            "\n",
            "AI: Sure, send me the list and I will give you what you need.\n",
            "Human: Title: Automated retrieval of information on threatened species from online sources using machine learning\n",
            "Abstract: As resources for conservation are limited, gathering and analysing information from digital platforms can help investigate the global biodiversity crisis in a cost‐efficient manner. Development and application of methods for automated content analysis of digital data sources are especially important in the context of investigating human–nature interactions. In this study, we introduce novel application methods to automatically collect and analyse textual data on species of conservation concern from digital platforms. An end‐to‐end pipeline is constructed that begins from searching and downloading news articles about species listed in Appendix I of the Convention on International Trade in Endangered Species of Wild Fauna and Flora (CITES) along with news articles from specific Twitter handles and proceeds with implementing natural language processing and machine learning methods to filter and retain only relevant articles. A crucial aspect here is the automatic annotation of training data, which can be challenging in many machine learning applications. A Named Entity Recognition model is then used to extract additional relevant information for each article. The data collected over a 1‐month period included 15,088 articles focusing on 585 species listed in Appendix I of CITES. The accuracy of the neural network to detect relevant articles was 95.91% while the Named Entity recognition model helped extract information on prices, location and quantities of traded animals and plants. A regularly updated database, which can be queried and analysed for various research purposes and to inform conservation decision making, is generated by the system. The results demonstrate that natural language processing can be used successfully to extract information from digital text content. The proposed methods can be applied to multiple digital data platforms at the same time and used to investigate human–nature interactions in conservation science and practice.\n",
            "\n",
            "Title: A Sequential Graph Neural Network for Short Text Classification\n",
            "Abstract: Short text classification is an important problem of natural language processing (NLP), and graph neural networks (GNNs) have been successfully used to solve different NLP problems. However, few studies employ GNN for short text classification, and most of the existing graph-based models ignore sequential information (e.g., word orders) in each document. In this work, we propose an improved sequence-based feature propagation scheme, which fully uses word representation and document-level word interaction and overcomes the limitations of textual features in short texts. On this basis, we utilize this propagation scheme to construct a lightweight model, sequential GNN (SGNN), and its extended model, ESGNN. Specifically, we build individual graphs for each document in the short text corpus based on word co-occurrence and use a bidirectional long short-term memory network (Bi-LSTM) to extract the sequential features of each document; therefore, word nodes in the document graph retain contextual information. Furthermore, two different simplified graph convolutional networks (GCNs) are used to learn word representations based on their local structures. Finally, word nodes combined with sequential information and local information are incorporated as the document representation. Extensive experiments on seven benchmark datasets demonstrate the effectiveness of our method.\n",
            "\n",
            "Title: Illation of Video Visual Relation Detection Based on Graph Neural Network\n",
            "Abstract: Visual relation detection task is the bridge between semantic text and image information, and it can better express the content of images or video through the relation triple . This significant research can be applied to image question answering, video subtitles and other directions Using the video as input to the task of visual relationship detection receives less attention. Therefore, we propose an algorithm based on the graph convolution neural network and multi-hypothesis tree to implement video relationship prediction. Video visual relationship detection algorithm is divided into three steps: Firstly, the motion trajectories of the subject and object in the input video clip are generated; Secondly, a VRGE network module based on the graph convolution neural network is proposed to predict the relationship between objects in the video clip; Finally, the relationship triplets are formed through the multi-hypothesis fusion algorithm (MHF) and the visual relationship. We have verified our method on the benchmark ImageNet-VidVRD dataset. The experimental results demonstrate that our proposed method can achieve a satisfactory accuracy of 29.05% and recall of 10.18% for visual relation detection.\n",
            "\n",
            "Title: An Interpretable Deep Architecture for Similarity Learning Built Upon Hierarchical Concepts\n",
            "Abstract: In general, development of adequately complex mathematical models, such as deep neural networks, can be an effective way to improve the accuracy of learning models. However, this is achieved at the cost of reduced post-hoc model interpretability, because what is learned by the model can become less intelligible and tractable to humans as the model complexity increases. In this paper, we target a similarity learning task in the context of image retrieval, with a focus on the model interpretability issue. An effective similarity neural network (SNN) is proposed not only to seek robust retrieval performance but also to achieve satisfactory post-hoc interpretability. The network is designed by linking the neuron architecture with the organization of a concept tree and by formulating neuron operations to pass similarity information between concepts. Various ways of understanding and visualizing what is learned by the SNN neurons are proposed. We also exhaustively evaluate the proposed approach using a number of relevant datasets against a number of state-of-the-art approaches to demonstrate the effectiveness of the proposed network. Our results show that the proposed approach can offer superior performance when compared against state-of-the-art approaches. Neuron visualization results are demonstrated to support the understanding of the trained neurons.\n",
            "\n",
            "Title: Improving Neural Language Processing with Named Entities\n",
            "Abstract: Pretraining-based neural network models have demonstrated state-of-the-art (SOTA) performances on natural language processing (NLP) tasks. The most frequently used sentence representation for neural-based NLP methods is a sequence of subwords that is different from the sentence representation of non-neural methods that are created using basic NLP technologies, such as part-of-speech (POS) tagging, named entity (NE) recognition, and parsing. Most neural-based NLP models receive only vectors encoded from a sequence of subwords obtained from an input text. However, basic NLP information, such as POS tags, NEs, parsing results, etc, cannot be obtained explicitly from only the large unlabeled text used in pretraining-based models. This paper explores use of NEs on two Japanese tasks; document classification and headline generation using Transformer-based models, to reveal the effectiveness of basic NLP information. The experimental results with eight basic NEs and approximately 200 extended NEs show that NEs improve accuracy although a large pretraining-based model trained using 70 GB text data was used.\n",
            "\n",
            "Title: Attention-based Deep Learning Model for Arabic Handwritten Text Recognition\n",
            "Abstract: This work proposes a segmentation-free approach to Arabic Handwritten Text Recognition (AHTR): an attention-based Convolutional Neural Network - Recurrent Neural Network - Connectionist Temporal Classification (CNN-RNN-CTC) deep learning architecture. The model receives as input an image and provides, through a CNN, a sequence of essential features, which are transferred to an Attention-based Bidirectional Long Short-Term Memory Network (BLSTM). The BLSTM gives features sequence in order, and the attention mechanism allows the selection of relevant information from the features sequences. The selected information is then fed to the CTC, enabling the loss calculation and the transcription prediction. The contribution lies in extending the CNN by dropout layers, batch normalization, and dropout regularization parameters to prevent over-fitting. The output of the RNN block is passed through an attention mechanism to utilize the most relevant parts of the input sequence in a flexible manner. This solution enhances previous methods by improving the CNN speed and performance and controlling over model over-fitting. The proposed system achieves the best accuracy of 97.1% for the IFN-ENIT Arabic script database, which competes with the current state-of-the-art. It was also tested for the modern English handwriting of the IAM database, and the Character Error Rate of 2.9% is attained, which confirms the model's script independence.\n",
            "\n",
            "Title: Text detection and recognition through deep learning-based fusion neural network\n",
            "Abstract: Text recognition task involves recognizing the text from the natural image; it possesses various application, which aids information extraction through data mining from street view like images. Scene text recognition involves two stages i.e., text detection and text recognition, in the past several mechanisms has been proposed for accurate identification, these mechanisms are either traditional approach or deep learning-based. All the existing deep-learning methodology fails as this comprises character data and image data, further this research develops an optimal architecture fusion neural network (FNN) for text identification and recognition. FNN comprises several layers of convolutional neural network as well as recurrent neural network. Within FNN architecture convolutional layer is utilized for the feature extraction and recurrent layer is utilized for attaining the feature classification prediction. Further, an optimal training architecture is established for the enhancement of classification accuracy. Here Devanagari MLT-19 dataset is utilized for the evaluation of FNN. Three different parameters are considered during evaluation i.e., script word identification, character recognition rate (CRR) and word recognition rate (WRR). Further comparison with existing models is performed to establish the proposed model efficiency and it shows FNN methodology observes 98.67% of script identification accuracy, 84.65% of WRR and 92.93% of CRR.\n",
            "\n",
            "Title: Automatic Correction of Real-Word Errors in Spanish Clinical Texts\n",
            "Abstract: Real-word errors are characterized by being actual terms in the dictionary. By providing context, real-word errors are detected. Traditional methods to detect and correct such errors are mostly based on counting the frequency of short word sequences in a corpus. Then, the probability of a word being a real-word error is computed. On the other hand, state-of-the-art approaches make use of deep learning models to learn context by extracting semantic features from text. In this work, a deep learning model were implemented for correcting real-word errors in clinical text. Specifically, a Seq2seq Neural Machine Translation Model mapped erroneous sentences to correct them. For that, different types of error were generated in correct sentences by using rules. Different Seq2seq models were trained and evaluated on two corpora: the Wikicorpus and a collection of three clinical datasets. The medicine corpus was much smaller than the Wikicorpus due to privacy issues when dealing with patient information. Moreover, GloVe and Word2Vec pretrained word embeddings were used to study their performance. Despite the medicine corpus being much smaller than the Wikicorpus, Seq2seq models trained on the medicine corpus performed better than those models trained on the Wikicorpus. Nevertheless, a larger amount of clinical text is required to improve the results.\n",
            "\n",
            "Title: Analysis of Deep Learning Approach Based on Convolution Neural Network (CNN) for Classification of Web Page Title and Description Text\n",
            "Abstract: The volume of digital documents available online is growing exponentially due to the increasing use of the internet. Categorization of information obtained online is needed to make it easier for recipients of information to determine and filter which information is needed. Classification of web pages can be based on titles and descriptions, which are text data that can be done by utilizing deep learning technology for text classification. This study aimed to conduct data training and analysis experiments to determine the accuracy of the proposed deep learning architecture in classifying web page titles and descriptions. In this research, we proposed a Convolution Neural Network (CNN) architecture that generates few parameters. The training and evaluation set was conducted on the web page dataset provided by DMOZ. As a result, the proposed CNN architecture with the number of N (Dropout + 1D Convolution + ReLU activation) equal to 1 achieves the best validation accuracy. It achieves 79.51% with only generates 825,061 parameters. The proposed CNN architecture achieved outperformed performance on the accuracy of the five other technologies in the state-of-the-art.\n",
            "\n",
            "Title: Methods of visual graph-analytical presentation and retrieval of scientific and technical texts\n",
            "Abstract: The technology of constructing and visualizing a semantic image of the full text of the document represented by the ontology as a system of three systems is offered: functional, conceptual and terminological. Objects and connections of the functional system correspond to the names of entities and relations extracted from the text; to objects of the conceptual system - descriptors of the thesauri of subject areas. The problem of the variable representation of entities at the sign level is solved using the rules for the formation of phrases of different lengths. Functional relationships are classified according to the taxonomy of functional relationships and are used to construct aspect projections of ontologies. As the data model of the ontology, a labeled directed graph is used, which includes nodes and arcs of different types, which makes it possible to formalize operations on ontologies. Constructing a display of set elements of ontology into graph elements in such way that elements of different sets of different systems are distinguishable, recognizable and depicted in different ways, allows to implement the principle of correspondence of the graphic image with the semantics of the visualized data. Based on the search tasks typology, metaphors for visualizing the ontology graph are proposed: the “pathfinding” metaphor, characterize d by the construction of a directed chain of facts, and the “neighborhood analysis” metaphor, which is characterized by the study of the environment (context) of a fact. The technology and software for the construction and variant visualization of the ontology graph have been developed. Examples of using the proposed models for information retrieval through document texts are given.\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{\n",
            "  \"topic\": \"Text Classification using Graph Neural Networks\",\n",
            "  \"title\": \"Graph Neural Networks for Text Classification\",\n",
            "  \"reasoning\": \"Based on the abstracts provided, several articles discuss the use of graph neural networks (GNNs) for text classification tasks. These GNN models leverage the sequential and contextual information in text data to improve classification accuracy. The proposed models utilize various techniques such as attention mechanisms, feature propagation, and concept trees to enhance the performance of text classification. Therefore, a section titled 'Graph Neural Networks for Text Classification' would be appropriate to cover this topic.\"\n",
            "}\n",
            "Results for section 1\n",
            "{'topic': 'Text Classification using Graph Neural Networks', 'title': 'Graph Neural Networks for Text Classification', 'reasoning': \"Based on the abstracts provided, several articles discuss the use of graph neural networks (GNNs) for text classification tasks. These GNN models leverage the sequential and contextual information in text data to improve classification accuracy. The proposed models utilize various techniques such as attention mechanisms, feature propagation, and concept trees to enhance the performance of text classification. Therefore, a section titled 'Graph Neural Networks for Text Classification' would be appropriate to cover this topic.\"}\n",
            "....................................................................................................\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: You are a renowned scientist who is writing a survey on 'text neural information retrieval'. You are currently writing a section about 'Semantic Fusion Strategies for Text Neural Information Retrieval'\n",
            "Human: I will send you a list of title and abstract of scientific articles. Most of them cover a specific topic about the section 'Semantic Fusion Strategies for Text Neural Information Retrieval'.\n",
            "Your task is to find out what this topic is and suggest a good title for a section in a scientific survey that addresses it.\n",
            "You should also explain your reasoning.\n",
            "Your answer should be a valid RFC8259 compliant JSON object with three properties:\n",
            "The first property, called \"topic\", describes the main topic that is a subset of 'Semantic Fusion Strategies for Text Neural Information Retrieval' that are present in most abstracts.\n",
            "The second property, called \"title\", is the title of the section that will cover this topic and must be clearly related to the property \"topic\".\n",
            "The third property is called \"reasoning\" and should contains your reasoning to choose this topic as an answer.\n",
            "Use this format: \n",
            "{\n",
            "  \"topic\": {TOPIC},\n",
            "  \"title\": {TITLE},\n",
            "  \"reasoning\": {REASONING}\n",
            "}\n",
            "\n",
            "Do you understand?\n",
            "\n",
            "AI: Sure, send me the list and I will give you what you need.\n",
            "Human: Title: TripClick: The Log Files of a Large Health Web Search Engine\n",
            "Abstract: Click logs are valuable resources for a variety of information retrieval (IR) tasks. This includes query understanding/analysis, as well as learning effective IR models particularly when the models require large amounts of training data. We release a large-scale domain-specific dataset of click logs, obtained from user interactions of the Trip Database health web search engine. Our click log dataset comprises approximately 5.2 million user interactions collected between 2013 and 2020. We use this dataset to create a standard IR evaluation benchmark - TripClick - with around 700,000 unique free-text queries and 1.3 million pairs of query-document relevance signals, whose relevance is estimated by two click-through models. As such, the collection is one of the few datasets offering the necessary data richness and scale to train neural IR models with a large amount of parameters, and notably the first in the health domain. Using TripClick, we conduct experiments to evaluate a variety of IR models, showing the benefits of exploiting this data to train neural architectures. In particular, the evaluation results show that the best performing neural IR model significantly improves the performance by a large margin relative to classical IR models, especially for more frequent queries.\n",
            "\n",
            "Title: News Video Title Extraction Algorithm Based on Deep Learning\n",
            "Abstract: In order to better serve the needs of news business, researchers apply the information extraction technology of news headlines to the news field to assist decision-making. This article designs the shared convolutional layer and RPN network in the detection network respectively, and improves the depth of the shared convolutional layer, using VGGNet instead of ZFNet. A classification method incorporating semantic enhancement is designed for fine-grained topic category news classification. Combined with the idea of modular fusion mechanism, a semantic-enhanced classification model Multiple-Fusion is proposed. The Bert module replaces the traditional Word2Vec for semantic vector characterization, and introduces Bi-LSTM to adaptively extract context features, strengthens feature expression through the self-attention mechanism and adjusts network weights, and finally makes the model achieve accurate classification. This article designs a novel word-level data enhancement strategy for text data enhancement, which solves the problems of fewer training corpus samples and model overfitting. This article proposes a video target extraction method based on deep learning. The algorithm framework includes particle filtering, pre-training features, convolutional neural networks, discriminative classifiers, and online parameter updates. This method deeply combines deep learning models and traditional target extraction methods and frameworks. The experimental results show that the method in this article has relatively outstanding performance, and can adapt to many interferences encountered in the extraction process and the change of the target itself, and has strong robustness.\n",
            "\n",
            "Title: Linguistic knowledge-based vocabularies for Neural Machine Translation\n",
            "Abstract: Abstract Neural Networks applied to Machine Translation need a finite vocabulary to express textual information as a sequence of discrete tokens. The currently dominant subword vocabularies exploit statistically-discovered common parts of words to achieve the flexibility of character-based vocabularies without delegating the whole learning of word formation to the neural network. However, they trade this for the inability to apply word-level token associations, which limits their use in semantically-rich areas and prevents some transfer learning approaches e.g. cross-lingual pretrained embeddings, and reduces their interpretability. In this work, we propose new hybrid linguistically-grounded vocabulary definition strategies that keep both the advantages of subword vocabularies and the word-level associations, enabling neural networks to profit from the derived benefits. We test the proposed approaches in both morphologically rich and poor languages, showing that, for the former, the quality in the translation of out-of-domain texts is improved with respect to a strong subword baseline.\n",
            "\n",
            "Title: Group Assignments for Project-Based Learning Using Natural Language Processing—A Feasibility Study\n",
            "Abstract: Group learning is commonly used in a wide range of classes. However, effective methods used to form groups are not thoroughly understood. In this study, we explore a quantitative method for creating project teams based on student knowledge and interests expressed in project proposals. The proposals are encoded to vector representations, ensuring that closely related proposals yield similar vectors. During this step, two widely used natural language processing algorithms are used. The first algorithm is based solely on the frequency of words used in the text, while the other considers context information using a deep neural network. The similarity scores for the proposals generated by the two algorithms are compared with those generated by human evaluators. The proposed method was applied to a group of senior students in a capstone design course in South Korea based on their project proposals on autonomous cars written in Korean. The results indicate that the contextualized encoding scheme produces more human-like text similarity vectors compared to the word frequency-based encoding scheme. This discrepancy is discussed from a context information standpoint in this study.\n",
            "\n",
            "Title: A Comparative Study of Literature Based on Mathematical Statistics in Big Data Environment\n",
            "Abstract: Large data environment, clustering technology has become a powerful tool for information retrieval and text mining. The essence of it is to set up a target and divide a group of objects into different groups according to the similarity. This process can set the target as similar elements in the same group, or different groups of different elements, thus achieving different purposes is a kind of undirected learning research method. This paper compares the two literature by using the frequency and clustering of the computer mathematical statistics, and clearly shows that the key to achieving harmony is to build a good society.\n",
            "\n",
            "Title: Improving extractive text summarization performance using enhanced feature based rbm method\n",
            "Abstract: Text summarization is the process of creating a short, accurate and fluent summary of a longer text document. As plenty of digital data is available online, automatic text summarization methods greatly needed to help and understand the lengthy & complex documents quickly by discovering the relevant information. This paper proposes the text summarization method for short news articles and long scientific papers using unsupervised neural network model. The proposed method works in four main steps: input document pre-processing, feature extraction, feature enhancement and final summary generation. We have extracted combination of various statistical and linguistic features from input document, which helps in improving the quality of sentence selection. Further Restricted Boltzmann Machine (RBM) model is used to capture & enhance the discriminative, abstract features in an unsupervised way to improve the overall performance without losing any significant information. Sentences are scored based on enhanced feature set and top sentences are selected for final extractive summary. Performance of the proposed method is evaluated using Rouge score and compared with TextRank, LexRank, LSA & Luhn baseline methods and the results demonstrates that proposed methodology performs better compared to other methods.\n",
            "\n",
            "Title: Neural Network: An Improved FCM for Multimodal Cultural Data Analysis\n",
            "Abstract: With the development of cloud computing, big data, and artificial intelligence (AI) technology, there is a growing interest in “cultural analysis.” Cultural analysis requires different types of data such as texts, pictures, and videos. The richness and differences of resources in the cultural field lead to diverse modalities of cultural data. Traditional text analysis methods can no longer meet the data analysis needs of current multimedia cultural resources. This article starts from cultural data’s feature information to solve the heterogeneity problem faced by massive multimodal cultural data analysis. It analyzes it from geography, time, art, and thematic character, classified and aggregated to form a multimodal cultural feature information matrix. The corresponding correlation measurement methods for different matrices from the above dimensions are proposed, solved in turn, and substituted into the optimized training back propagation (BP) neural network to obtain the final correlation degree. The improved fuzzy C-means (FCM) clustering algorithm is used to aggregate the high correlation cultural data based on the degree. The algorithm proposed in this study is compared with the existing algorithm. The experimental results show that the optimized BP neural network is at least 58% more accurate than the current method for calculating different matrices’ correlation degrees. In terms of accuracy, the improved fuzzy C-means algorithm effectively reduces the random interference in the selection of the initial clustering center, which is significantly higher than other clustering algorithms.\n",
            "\n",
            "Title: Aspect-Based Sentiment Analysis with Dependency Relation Weighted Graph Attention\n",
            "Abstract: Aspect-based sentiment analysis is a fine-grained sentiment analysis that focuses on the sentiment polarity of different aspects of text, and most current research methods use a combination of dependent syntactic analysis and graphical neural networks. In this paper, a graph attention network aspect-based sentiment analysis model based on the weighting of dependencies (WGAT) is designed to address the problem in that traditional models do not sufficiently analyse the types of syntactic dependencies; in the proposed model, graph attention networks can be weighted and averaged according to the importance of different nodes when aggregating information. The model first transforms the input text into a low-dimensional word vector through pretraining, while generating a dependency syntax graph by analysing the dependency syntax of the input text and constructing a dependency weighted adjacency matrix according to the importance of different dependencies in the graph. The word vector and the dependency weighted adjacency matrix are then fed into a graph attention network for feature extraction, and sentiment polarity is predicted through the classification layer. The model can focus on syntactic dependencies that are more important for sentiment classification during training, and the results of the comparison experiments on the Semeval-2014 laptop and restaurant datasets and the ACL-14 Twitter social comment dataset show that the WGAT model has significantly improved accuracy and F1 values compared to other baseline models, validating its effectiveness in aspect-level sentiment analysis tasks.\n",
            "\n",
            "Title: Sentiment Polarity Identification of Social Media content using Artificial Neural Networks\n",
            "Abstract: Sentiment of people about consumer goods and government policies for decision making is normally collected through feedback forms, surveys etc. The social network sites and micro blogging sites are considered a very good source of information nowadays because people share and discuss their opinions about a certain topic freely. With the increased use of technology and social media, people proactively express their opinion through social media sites like Twitter, Facebook, Instagram etc. A social media sentiment analysis can help companies to understand how people feel about their products. On the other hand, extracting the sentiment from social media text is a challenging task due to the complexity of natural language processing of social media language. Often these messages reflect the emotion, opinion and sentiment of the public through a mix of text, image, emoticons etc. These statements are often called electronic Word of Mouth (eWOM) and are much prevalent in business and service industry to enable customers to share their point of view.\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{\n",
            "  \"topic\": \"Semantic Fusion for Text Classification\",\n",
            "  \"title\": \"Enhancing Text Classification with Semantic Fusion Strategies\",\n",
            "  \"reasoning\": \"Based on the abstracts provided, the common theme among the articles is the use of semantic fusion strategies to improve text classification tasks. These strategies involve incorporating semantic information, such as word embeddings, linguistic knowledge, and syntactic dependencies, into neural network models for better performance in tasks like information retrieval, news video title extraction, machine translation, project-based learning, literature analysis, text summarization, cultural data analysis, aspect-based sentiment analysis, and social media sentiment analysis. Therefore, a section titled 'Enhancing Text Classification with Semantic Fusion Strategies' would encompass the main topic and provide a comprehensive overview of the approaches and advancements in this area.\"\n",
            "}\n",
            "Results for section 2\n",
            "{'topic': 'Semantic Fusion for Text Classification', 'title': 'Enhancing Text Classification with Semantic Fusion Strategies', 'reasoning': \"Based on the abstracts provided, the common theme among the articles is the use of semantic fusion strategies to improve text classification tasks. These strategies involve incorporating semantic information, such as word embeddings, linguistic knowledge, and syntactic dependencies, into neural network models for better performance in tasks like information retrieval, news video title extraction, machine translation, project-based learning, literature analysis, text summarization, cultural data analysis, aspect-based sentiment analysis, and social media sentiment analysis. Therefore, a section titled 'Enhancing Text Classification with Semantic Fusion Strategies' would encompass the main topic and provide a comprehensive overview of the approaches and advancements in this area.\"}\n",
            "....................................................................................................\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: You are a renowned scientist who is writing a survey on 'text neural information retrieval'. You are currently writing a section about 'text neural information retrieval'\n",
            "Human: I will send you a list of title and abstract of scientific articles. Most of them cover a specific topic about the section 'text neural information retrieval'.\n",
            "Your task is to find out what this topic is and suggest a good title for a section in a scientific survey that addresses it.\n",
            "You should also explain your reasoning.\n",
            "Your answer should be a valid RFC8259 compliant JSON object with three properties:\n",
            "The first property, called \"topic\", describes the main topic that is a subset of 'text neural information retrieval' that are present in most abstracts.\n",
            "The second property, called \"title\", is the title of the section that will cover this topic and must be clearly related to the property \"topic\".\n",
            "The third property is called \"reasoning\" and should contains your reasoning to choose this topic as an answer.\n",
            "Use this format: \n",
            "{\n",
            "  \"topic\": {TOPIC},\n",
            "  \"title\": {TITLE},\n",
            "  \"reasoning\": {REASONING}\n",
            "}\n",
            "\n",
            "Do you understand?\n",
            "\n",
            "AI: Sure, send me the list and I will give you what you need.\n",
            "Human: Title: An Improved Method Combining ANN and 1D-Var for the Retrieval of Atmospheric Temperature Profiles from FY-4A/GIIRS Hyperspectral Data\n",
            "Abstract: In our study, a retrieval method of temperature profiles is proposed which combines an improved one-dimensional variational algorithm (1D-Var) and artificial neural network algorithm (ANN), using FY-4A/GIIRS (Geosynchronous Interferometric Infrared Sounder) infrared hyperspectral data. First, according to the characteristics of the FY-4A/GIIRS observation data using the conventional 1D-Var, we introduced channel blacklists and discarded the channels that have a large negative impact on retrieval, then used the information capacity method for channel selection and introduced a neural network to correct the satellite observation data. The improved 1D-Var effectively used the observation information of 1415 channels, reducing the impact of the error of the satellite observation and radiative transfer model, and realizing the improvement of retrieval accuracy. We subsequently used the improved 1D-Var and ANN algorithms to retrieve the temperature profiles, respectively, from the GIIRS data. The results showed that the accuracy when using ANN is better than using improved 1D-Var in situations where the pressure ranges from 800 hPa to 1000 hPa. Therefore, we combined the improved 1D-Var and ANN method to retrieve temperature profiles for different pressure levels, calculating the error by taking sounding data published by the University of Wyoming as the true values. The results show that the average error of the retrieved temperature profiles is smaller than 2 K when using our method, this method makes the accuracy of the retrieved temperature profiles superior to the accuracy of the GIIRS products from 10 hPa to 575 hPa. All in all, through the combination of the physical retrieval method and the machine learning retrieval method, this paper can certainly provide a reference for improving the accuracy of products.\n",
            "\n",
            "Title: Research on Semantic Similarity of Short Text Based on Bert and Time Warping Distance\n",
            "Abstract: The research on semantic similarity of short text plays an important role in machine translation, emotion analysis, information retrieval and other AI business applications. However, according to existing short text similarity research, the characteristics of ambiguous vocabularies are difficult to be effectively analyzed, the solution of the problem caused by words order needs to be further optimized as well. This paper proposes a short text semantic similarity calculation method that combines BERT and time warping distance algorithm, in order to solve the problem of vocabulary ambiguity. The model first uses the pre trained Bert model to extract the semantic features of the short text from the whole level, and obtains a 768 dimensional short text feature vector. Then, it transforms the extracted feature vector into a point sequence in space, uses the CTW algorithm to calculate the time warping distance between the curves connected by the point sequence, and finally uses the weight function designed by the analysis, according to the smaller the time warpage distance is, the higher the degree of small similarity is, to calculate the similarity between short texts. The experimental results show that this model can mine the feature information of ambiguous words, and calculate the similarity of short texts with lexical ambiguity effectively. Compared with other models, it can distinguish the semantic features of ambiguous words more accurately.\n",
            "\n",
            "Title: Cross – Language Information Retrieval\n",
            "Abstract: In today‘s growing nation, local database storage and retrieval is crucial for developing countries. A huge amount of information on Internet is available in numerous language that can be access by anybody at any time. Data Retrieval systems are fundamentally linguistic: the content or context of documents must be described, and the inquirers needed for documents must be expressed. These descriptions and expressions are most frequentlyarticulated in free or controlled vocabularies that have some of the same characteristics as natural language. Therefore, the processes of document description or request formulation must be strongly related to the processes of description and inquiry in natural language. Natural language processing is used to (a) preprocess the documents to extract content-carrying terms, (b) discover inter-term dependencies and build a conceptual hierarchy specific to the database domain, and (c) process the user's natural language requests into effective search queries. Since we assume that the information is primarily encoded as text, IR is also a natural language processing problem: to decide if a document is relevant to a given information need, one needs to be able to understand its content. CLIR deals with asking queries in one language and demand for retrieving documents in another language.This paper takes an overview of the new application areas of CLIR and reviews the approaches used in the process of CLIR research for query and document translation.\n",
            "\n",
            "Title: Protocol for a reproducible experimental survey on biomedical sentence similarity\n",
            "Abstract: Measuring semantic similarity between sentences is a significant task in the fields of Natural Language Processing (NLP), Information Retrieval (IR), and biomedical text mining. For this reason, the proposal of sentence similarity methods for the biomedical domain has attracted a lot of attention in recent years. However, most sentence similarity methods and experimental results reported in the biomedical domain cannot be reproduced for multiple reasons as follows: the copying of previous results without confirmation, the lack of source code and data to replicate both methods and experiments, and the lack of a detailed definition of the experimental setup, among others. As a consequence of this reproducibility gap, the state of the problem can be neither elucidated nor new lines of research be soundly set. On the other hand, there are other significant gaps in the literature on biomedical sentence similarity as follows: (1) the evaluation of several unexplored sentence similarity methods which deserve to be studied; (2) the evaluation of an unexplored benchmark on biomedical sentence similarity, called Corpus-Transcriptional-Regulation (CTR); (3) a study on the impact of the pre-processing stage and Named Entity Recognition (NER) tools on the performance of the sentence similarity methods; and finally, (4) the lack of software and data resources for the reproducibility of methods and experiments in this line of research. Identified these open problems, this registered report introduces a detailed experimental setup, together with a categorization of the literature, to develop the largest, updated, and for the first time, reproducible experimental survey on biomedical sentence similarity. Our aforementioned experimental survey will be based on our own software replication and the evaluation of all methods being studied on the same software platform, which will be specially developed for this work, and it will become the first publicly available software library for biomedical sentence similarity. Finally, we will provide a very detailed reproducibility protocol and dataset as supplementary material to allow the exact replication of all our experiments and results.\n",
            "\n",
            "Title: Fuzzy Keywords-Driven Public Sports Resource Allocation Strategies Retrieval With Privacy-Preservation\n",
            "Abstract: Currently, the government proposes various public sports resource allocation strategies to promote the construction of public sports in the local. Thus, searching for the document database becomes an economic and efficient way for government officials to understand the contents of various public sports resource allocation strategies. As the government officials lack background knowledge of public sports resource allocation, the government officials are hard to retrieve appropriate documents. What’s more, the document database is generally unwilling to share all text document information with the government official as some documents contain sensitive data information. Considering the above drawbacks, we put forward a fuzzy keywords-driven public sports resource allocation strategies retrieval approach based on the Simhash (named $PSRAS_{\\mathrm {k}+\\mathrm {S}}$ ) in the paper. The retrieval approach can accurately recommend appropriate documents to the government officials, by using the text description mining of documents and the fuzzy keywords search technology. Furthermore, the retrieval process is not revealing the sensitive data information of the irrelevant public sports resource allocation strategy document to the government officials, so the retrieval process can achieve local information protection mechanism. Finally, a case study explains the feasibility and effectiveness of our retrieval approach step by step.\n",
            "\n",
            "Title: Multi-modal Retrieval of Tables and Texts Using Tri-encoder Models\n",
            "Abstract: Open-domain extractive question answering works well on textual data by first retrieving candidate texts and then extracting the answer from those candidates. However, some questions cannot be answered by text alone but require information stored in tables. In this paper, we present an approach for retrieving both texts and tables relevant to a question by jointly encoding texts, tables and questions into a single vector space. To this end, we create a new multi-modal dataset based on text and table datasets from related work and compare the retrieval performance of different encoding schemata. We find that dense vector embeddings of transformer models outperform sparse embeddings on four out of six evaluation datasets. Comparing different dense embedding models, tri-encoders with one encoder for each question, text and table increase retrieval performance compared to bi-encoders with one encoder for the question and one for both text and tables. We release the newly created multi-modal dataset to the community so that it can be used for training and evaluation.\n",
            "\n",
            "Title: A Multi-Modal Neural Embeddings Approach for Detecting Mobile Counterfeit Apps: A Case Study on Google Play Store\n",
            "Abstract: Counterfeit apps impersonate existing popular apps in attempts to misguide users to install them for various reasons such as collecting personal information, spreading malware, or simply to increase their advertisement revenue. Many counterfeits can be identified once installed, however even a tech-savvy user may struggle to detect them before installation as app icons and descriptions can be quite similar to the original app. To this end, this paper proposes to leverage the recent advances in deep learning methods to create image and text embeddings so that counterfeit apps can be efficiently identified when they are submitted to be published in app markets. We show that for the problem of counterfeit detection, a novel approach of combining <italic>content embeddings</italic> and <italic>style embeddings</italic> (given by the Gram matrix of CNN feature maps) outperforms the baseline methods for image similarity such as SIFT, SURF, LATCH, and various image hashing methods. We first evaluate the performance of the proposed method on two well-known datasets for evaluating image similarity methods and show that, content, style, and combined embeddings increase <italic>precision@k</italic> and <italic>recall@k</italic> by 10-15 percent and 12-25 percent, respectively when retrieving five nearest neighbours. Second specifically for the app counterfeit detection problem, combined content and style embeddings achieve 12 and 14 percent increase in <italic>precision@k</italic> and <italic>recall@k</italic>, respectively compared to the baseline methods. We also show that adding text embeddings further increases the performance by 5 and 6 percent in terms of <italic>precision@k</italic> and <italic>recall@k</italic>, respectively when <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=\"seneviratne-ieq1-3007260.gif\"/></alternatives></inline-formula> is five. Third, we present an analysis of approximately 1.2 million apps from Google Play Store and identify a set of potential counterfeits for top-10,000 popular apps. Under a conservative assumption, we were able to find 2,040 potential counterfeits that contain malware in a set of 49,608 apps that showed high similarity to one of the top-10,000 popular apps in Google Play Store. We also find 1,565 potential counterfeits asking for at least five additional dangerous permissions than the original app and 1,407 potential counterfeits having at least five extra third party advertisement libraries.\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{\n",
            "  \"topic\": \"Multi-modal retrieval\",\n",
            "  \"title\": \"Multi-modal Retrieval in Text Neural Information Retrieval\",\n",
            "  \"reasoning\": \"The abstracts of the articles mention the use of multiple modalities, such as text and tables, or text and images, for retrieval purposes. This indicates a common topic of multi-modal retrieval within the broader field of text neural information retrieval. The suggested title reflects this focus on multi-modal retrieval within the context of text neural information retrieval.\"\n",
            "}\n",
            "Results for section 1\n",
            "{'topic': 'Multi-modal retrieval', 'title': 'Multi-modal Retrieval in Text Neural Information Retrieval', 'reasoning': 'The abstracts of the articles mention the use of multiple modalities, such as text and tables, or text and images, for retrieval purposes. This indicates a common topic of multi-modal retrieval within the broader field of text neural information retrieval. The suggested title reflects this focus on multi-modal retrieval within the context of text neural information retrieval.'}\n",
            "....................................................................................................\n",
            "Extracting subsections...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "68fde3edfa624e9cafa9c46abedd9b9e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster 2: 12\n",
            "Cluster 1: 29\n",
            "Cluster 0: 9\n",
            "Extracting 3 sections in this level:\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: You are a renowned scientist who is writing a survey on 'text neural information retrieval'. You are currently writing a section about 'Multi-modal Retrieval in Text Neural Information Retrieval'\n",
            "Human: I will send you a list of title and abstract of scientific articles. Most of them cover a specific topic about the section 'Multi-modal Retrieval in Text Neural Information Retrieval'.\n",
            "Your task is to find out what this topic is and suggest a good title for a section in a scientific survey that addresses it.\n",
            "You should also explain your reasoning.\n",
            "Your answer should be a valid RFC8259 compliant JSON object with three properties:\n",
            "The first property, called \"topic\", describes the main topic that is a subset of 'Multi-modal Retrieval in Text Neural Information Retrieval' that are present in most abstracts.\n",
            "The second property, called \"title\", is the title of the section that will cover this topic and must be clearly related to the property \"topic\".\n",
            "The third property is called \"reasoning\" and should contains your reasoning to choose this topic as an answer.\n",
            "Use this format: \n",
            "{\n",
            "  \"topic\": {TOPIC},\n",
            "  \"title\": {TITLE},\n",
            "  \"reasoning\": {REASONING}\n",
            "}\n",
            "\n",
            "Do you understand?\n",
            "\n",
            "AI: Sure, send me the list and I will give you what you need.\n",
            "Human: Title: Improving Neural Language Processing with Named Entities\n",
            "Abstract: Pretraining-based neural network models have demonstrated state-of-the-art (SOTA) performances on natural language processing (NLP) tasks. The most frequently used sentence representation for neural-based NLP methods is a sequence of subwords that is different from the sentence representation of non-neural methods that are created using basic NLP technologies, such as part-of-speech (POS) tagging, named entity (NE) recognition, and parsing. Most neural-based NLP models receive only vectors encoded from a sequence of subwords obtained from an input text. However, basic NLP information, such as POS tags, NEs, parsing results, etc, cannot be obtained explicitly from only the large unlabeled text used in pretraining-based models. This paper explores use of NEs on two Japanese tasks; document classification and headline generation using Transformer-based models, to reveal the effectiveness of basic NLP information. The experimental results with eight basic NEs and approximately 200 extended NEs show that NEs improve accuracy although a large pretraining-based model trained using 70 GB text data was used.\n",
            "\n",
            "Title: Construction and Application of Text Entity Relation Joint Extraction Model Based on Multi-Head Attention Neural Network\n",
            "Abstract: Entity relationship extraction is one of the key areas of information extraction and is an important research content in the field of natural language processing. Based on past research, this paper proposes a combined extraction model based on a multi-headed attention neural network. Based on the BERT training model architecture, this paper extracts textual entities and relations tasks. At the same time, it integrates the naming entity feature, the terminology labeling characteristics, and the training relationship. The multi-attention mechanism and improved neural structures are added to the model to enhance the characteristic extraction capacity of the model. By studying the parameters of the multi-head attention mechanism, it is shown that the optimal parameters of the multi-head attention are h = 8, dv = 16, and the classification effect of the model is the best at this time. After experimental analysis, comparing the traditional text entity relationship extraction model and the multi-head attention neural network joint extraction model, the model entity relationship extraction effect was evaluated from the aspects of comprehensive evaluation index F1, accuracy rate P, and system time consumed. Experiments show: First, in the accuracy indicator, Xception performance is best, reaching 87.7%, indicating that the model extraction feature effect is enhanced. Second, with the increase of the number of iterative times, the verification set curve and the training set curve have increased to 96% and 98%, respectively, and the model has a strong generalization ability. Third, the model completes the extraction of all data in the test set in 1005 ms, which is an acceptable speed. Therefore, the model test results in this article are good, with a strong practical value.\n",
            "\n",
            "Title: An Unhealthy Webpage Discovery System Based on Convolutional Neural Network\n",
            "Abstract: Currently, with the popularity of the internet, people are surrounded by a large number of unhealthy pages which have a serious impact on the physical and mental health of visitors. To protect the legitimate rights and interests of internet users from infringement and maintain the harmonious and stable development of society, a new unhealthy webpage discovery system is needed. First, this paper proposed the knowledge of unhealthy webpages and web crawlers, and then the whole system's plan and design were introduced. The test results show that the unhealthy webpage discovery system can meet the needs of users. This experiment uses a CNN algorithm to classify the text and completes the collection and classification of unhealthy information through URL acquisition and URL filtering. The experimental results show that the unhealthy webpage discovery system based on a convolutional neural network can greatly improve the accuracy of unhealthy webpage discovery and reduce the omission rate, which can meet the needs of users for unhealthy webpage discovery.\n",
            "\n",
            "Title: User OCEAN Personality Model Construction Method Using a BP Neural Network\n",
            "Abstract: In the era of big data, the Internet is enmeshed in people’s lives and brings conveniences to their production and lives. The analysis of user preferences and behavioral predictions of user data can provide references for optimizing information structure and improving service accuracy. According to the present research, user’s behavior on social networking sites has a great correlation with their personality, and the five characteristics of the OCEAN (Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism) personality model can cover all aspects of a user’s personality. It is important in identifying a user’s OCEAN personality model to analyze their digital footprints left on social networking sites and to extract the rules of users’ behavior, and then to make predictions about user behavior. In this paper, the Latent Dirichlet Allocation (LDA) topic model is first used to extract the user’s text features. Second, the extracted features are used as sample input for a BP neural network. The results of the user’s OCEAN personality model obtained by a questionnaire are used as sample output for a BP neural network. Finally, the neural network is trained. A mapping model between the probability of the user’s text topic and their OCEAN personality model is established to predict the latter. The results show that the present approach improves the efficiency and accuracy of such a prediction.\n",
            "\n",
            "Title: Multi Label Automatic Image Annotation Neural Network to Handle Multi Media Image Retrieval\n",
            "Abstract: The present-day business online web indexes have embraced electronic picture search to further develop precision in picture information recovery. However Re-positioning is expectedly considered as a successful cycle for deciding the situation with electronic picture web search tools, yet it experiences a lack of a couple of. Consequently, some grouping methods, particularly (Novel Image Re-positioning System) NIRS have to be proposed to carry out inquiry picture re-positioning with semantic marks in electronic picture information recovery, which naturally recovers results in view of visual semantic highlights for various question or catchphrase extensions. To get to productive picture with the annotation is an aggressive concept in present. So that in the present paper, we are going to propose the Unsupervised Multi Labeled Image Annotate Learning Approach (UMLIALA) to decrease complexity in indexing of image with mining of web related convex optimization and classify required image data from large image data sets. And also use group based approximation calculation to improve accuracy in retrieval of images from different image data sources. Experiments of proposed approach give better and efficient results when compare to traditional approaches in terms of different image exploration parameters studies on different large image data sets.\n",
            "\n",
            "Title: Arabic Rumor Detection Using Contextual Deep Bidirectional Language Modeling\n",
            "Abstract: In today’s world, news outlets have changed dramatically; newspapers are obsolete, and radio is no longer in the picture. People look for news online and on social media, such as Twitter and Facebook. Social media contributors share information and trending stories before verifying their truthfulness, thus, spreading rumors. Early identification of rumors from social media has attracted many researchers. However, a relatively smaller number of studies focused on other languages, such as Arabic. In this study, an Arabic rumor detection model is proposed. The model was built using transformer-based deep learning architecture. According to the literature, transformers are neural networks with outstanding performance in natural language processing tasks. Two transformers-based models, AraBERT and MARBERT, were employed, tested, and evaluated using three recently developed Arabic datasets. These models are extensions to the BERT, Bidirectional Encoder Representations from Transformers, a deep learning model that uses transformer architecture to learn the text representations and leverages the attention mechanism. We have also mitigated the challenges introduced by the imbalanced training datasets by employing two sampling techniques. The experimental results of our proposed approaches achieved a maximum accuracy of 0.97. This result demonstrated the effectiveness of the proposed method and outperformed other existing Arabic rumor detection methods.\n",
            "\n",
            "Title: Современные методы детектирования и классификации токсичных комментариев с использованием нейронных сетей\n",
            "Abstract: The growth of popularity of online platforms which allow users to communicate with each other, share opinions about various events, and leave comments boosted the development of natural language processing algorithms. Tens of millions of messages per day are published by users of a particular social network need to be analyzed in real time for moderation in order to prevent the spread of various illegal or offensive information, threats and other types of toxic comments. Of course, such a large amount of information can be processed quite quickly only automatically. that is why there is a need to and a way to teach computers to “understand” a text written by humans. It is a non-trivial task even if the word “understand” here means only “to classify”. the rapid evolution of machine learning technologies has led to ubiquitous implementation of new algorithms. A lot of tasks, which for many years were considered almost impossible to solve, are now quite successfully solved using deep learning technologies. this article considers algorithms built using deep learning technologies and neural networks which can successfully solve the problem of detection and classification of toxic comments. In addition, the article presents the results of the developed algorithms, as well as the results of the ensemble of all considered algorithms on a large training set collected and tagged by Google and Jigsaw.\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{\n",
            "  \"topic\": \"Entity-based Information Retrieval\",\n",
            "  \"title\": \"Enhancing Text Neural Information Retrieval with Named Entities\",\n",
            "  \"reasoning\": \"The first abstract discusses the use of named entities (NEs) in neural-based NLP models to improve accuracy in document classification and headline generation tasks. This indicates a focus on leveraging entity-based information for retrieval purposes, which is a subset of multi-modal retrieval in text neural information retrieval.\"\n",
            "}\n",
            "Results for section 0\n",
            "{'topic': 'Entity-based Information Retrieval', 'title': 'Enhancing Text Neural Information Retrieval with Named Entities', 'reasoning': 'The first abstract discusses the use of named entities (NEs) in neural-based NLP models to improve accuracy in document classification and headline generation tasks. This indicates a focus on leveraging entity-based information for retrieval purposes, which is a subset of multi-modal retrieval in text neural information retrieval.'}\n",
            "....................................................................................................\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: You are a renowned scientist who is writing a survey on 'text neural information retrieval'. You are currently writing a section about 'Multi-modal Retrieval in Text Neural Information Retrieval'\n",
            "Human: I will send you a list of title and abstract of scientific articles. Most of them cover a specific topic about the section 'Multi-modal Retrieval in Text Neural Information Retrieval'.\n",
            "Your task is to find out what this topic is and suggest a good title for a section in a scientific survey that addresses it.\n",
            "You should also explain your reasoning.\n",
            "Your answer should be a valid RFC8259 compliant JSON object with three properties:\n",
            "The first property, called \"topic\", describes the main topic that is a subset of 'Multi-modal Retrieval in Text Neural Information Retrieval' that are present in most abstracts.\n",
            "The second property, called \"title\", is the title of the section that will cover this topic and must be clearly related to the property \"topic\".\n",
            "The third property is called \"reasoning\" and should contains your reasoning to choose this topic as an answer.\n",
            "Use this format: \n",
            "{\n",
            "  \"topic\": {TOPIC},\n",
            "  \"title\": {TITLE},\n",
            "  \"reasoning\": {REASONING}\n",
            "}\n",
            "\n",
            "Do you understand?\n",
            "\n",
            "AI: Sure, send me the list and I will give you what you need.\n",
            "Human: Title: Paraphrase Generation by Learning How to Edit from Samples\n",
            "Abstract: Neural sequence to sequence text generation has been proved to be a viable approach to paraphrase generation. Despite promising results, paraphrases generated by these models mostly suffer from lack of quality and diversity. To address these problems, we propose a novel retrieval-based method for paraphrase generation. Our model first retrieves a paraphrase pair similar to the input sentence from a pre-defined index. With its novel editor module, the model then paraphrases the input sequence by editing it using the extracted relations between the retrieved pair of sentences. In order to have fine-grained control over the editing process, our model uses the newly introduced concept of Micro Edit Vectors. It both extracts and exploits these vectors using the attention mechanism in the Transformer architecture. Experimental results show the superiority of our paraphrase generation method in terms of both automatic metrics, and human evaluation of relevance, grammaticality, and diversity of generated paraphrases.\n",
            "\n",
            "Title: A survey on knowledge-aware news recommender systems\n",
            "Abstract: News consumption has shifted over time from traditional media to online platforms, which use recommendation algorithms to help users navigate through the large incoming streams of daily news by suggesting relevant articles based on their preferences and reading behavior. In comparison to domains such as movies or e-commerce, where recommender systems have proved highly successful, the characteristics of the news domain (e.g., high frequency of articles appearing and becoming outdated, greater dynamics of user interest, less explicit relations between articles, and lack of explicit user feedback) pose additional challenges for the recommendation models. While some of these can be overcome by conventional recommendation techniques, injecting external knowledge into news recommender systems has been proposed in order to enhance recommendations by capturing information and patterns not contained in the text and metadata of articles, and hence, tackle shortcomings of traditional models. This survey provides a comprehensive review of knowledge-aware news recommender systems. We propose a taxonomy that divides the models into three categories: neural methods, non-neural entity-centric methods, and non-neural path-based methods. Moreover, the underlying recommendation algorithms, as well as their evaluations are analyzed. Lastly, open issues in the domain of knowledge-aware news recommendations are identified and potential research directions are proposed.\n",
            "\n",
            "Title: Neural Network: An Improved FCM for Multimodal Cultural Data Analysis\n",
            "Abstract: With the development of cloud computing, big data, and artificial intelligence (AI) technology, there is a growing interest in “cultural analysis.” Cultural analysis requires different types of data such as texts, pictures, and videos. The richness and differences of resources in the cultural field lead to diverse modalities of cultural data. Traditional text analysis methods can no longer meet the data analysis needs of current multimedia cultural resources. This article starts from cultural data’s feature information to solve the heterogeneity problem faced by massive multimodal cultural data analysis. It analyzes it from geography, time, art, and thematic character, classified and aggregated to form a multimodal cultural feature information matrix. The corresponding correlation measurement methods for different matrices from the above dimensions are proposed, solved in turn, and substituted into the optimized training back propagation (BP) neural network to obtain the final correlation degree. The improved fuzzy C-means (FCM) clustering algorithm is used to aggregate the high correlation cultural data based on the degree. The algorithm proposed in this study is compared with the existing algorithm. The experimental results show that the optimized BP neural network is at least 58% more accurate than the current method for calculating different matrices’ correlation degrees. In terms of accuracy, the improved fuzzy C-means algorithm effectively reduces the random interference in the selection of the initial clustering center, which is significantly higher than other clustering algorithms.\n",
            "\n",
            "Title: Deep Visual Semantic Embedding with Text Data Augmentation and Word Embedding Initialization\n",
            "Abstract: Language and vision are the two most essential parts of human intelligence for interpreting the real world around us. How to make connections between language and vision is the key point in current research. Multimodality methods like visual semantic embedding have been widely studied recently, which unify images and corresponding texts into the same feature space. Inspired by the recent development of text data augmentation and a simple but powerful technique proposed called EDA (easy data augmentation), we can expand the information with given data using EDA to improve the performance of models. In this paper, we take advantage of the text data augmentation technique and word embedding initialization for multimodality retrieval. We utilize EDA for text data augmentation, word embedding initialization for text encoder based on recurrent neural networks, and minimizing the gap between the two spaces by triplet ranking loss with hard negative mining. On two Flickr-based datasets, we achieve the same recall with only 60% of the training dataset as the normal training with full available data. Experiment results show the improvement of our proposed model; and, on all datasets in this paper (Flickr8k, Flickr30k, and MS-COCO), our model performs better on image annotation and image retrieval tasks; the experiments also demonstrate that text data augmentation is more suitable for smaller datasets, while word embedding initialization is suitable for larger ones.\n",
            "\n",
            "Title: Enhancing Cross-Modal Retrieval Based on Modality-Specific and Embedding Spaces\n",
            "Abstract: A new approach that drastically improves cross-modal retrieval performance in vision and language (hereinafter referred to as “vision and language retrieval”) is proposed in this paper. Vision and language retrieval takes data of one modality as a query to retrieve relevant data of another modality, and it enables flexible retrieval across different modalities. Most of the existing methods learn optimal embeddings of visual and lingual information to a single common representation space. However, we argue that the forced embedding optimization results in loss of key information for sentences and images. In this paper, we propose an effective utilization of representation spaces in a simple but robust vision and language retrieval method. The proposed method makes use of multiple individual representation spaces through text-to-image and image-to-text models. Experimental results showed that the proposed approach enhances the performance of existing methods that embed visual and lingual information to a single common representation space.\n",
            "\n",
            "Title: The Construction of Sports Health Management Model Based on Deep Learning\n",
            "Abstract: Deep learning is a new direction in the field of machine learning. It learns the inherent laws and representation levels of sample data. The information obtained in the learning process plays a great role in interpreting data such as text, images, and speech. Health management refers to the process of identifying, evaluating, and effectively intervening the health of individuals or groups of people. The purpose of this article is to build a sports health management model based on deep learning, to train students to actively take physical exercises, thereby promoting their physical fitness. This article first introduces related concepts such as deep learning and convolutional neural networks and then conducts an experimental exploration on the combination of convolutional neural networks and multilayer perceptrons. Secondly, this article designs a plan for sports health management of sports interventions and compares and analyzes the data before and after health management. The experimental results show that the average value of the overall health dimension after the experiment has reached 85.28, and the average value has reached a very high score, indicating that exercise intervention can effectively improve the physical fitness of students. At the same time, it improves the physical condition of students.\n",
            "\n",
            "Title: Review of feature selection methods for text classification\n",
            "Abstract: For the last three decades, the World Wide Web (WWW) has become one of the most widely used podium to generate an immense amount of heterogeneous data in a single day. Presently, many organizations aimed to process their domain data for taking quick decisions to improve their organizational performance. However, high dimensionality in datasets is a biggest obstacle for researchers and domain engineers to achieve their desired performance through their selected machine learning (ML) algorithms. In ML, feature selection is a core concept used for selecting most relevant features of high dimension data and thus improve the performance of the trained learning model. Moreover, the feature selection process also provides an effective way by eliminating in appropriate and redundant features and ultimately shrinks the computational time. Due to the significance and applications of feature selection, it has become a well-researched area of ML. Nowadays, feature selection has a vital role in most of the effective spam detection systems, pattern recognition systems, automated organization, management of documents, and information retrieval systems. In order to do accurate classification, the relevant feature selection is the most important task, and to achieve its objectives, this study starts with an overview of text classification. This overview is then followed by a survey. The survey covered the popular feature selection methods commonly used for text classification. This survey also sheds light on applications of feature selection methods. The focus of this study is three feature selection algorithms, i.e., Principal Component Analysis (PCA), ChiSquare (CS) and Information Gain (IG). This study is helpful for researchers looking for some suitable criterion to decide the suitable technique to be used for better understanding of the performance of the classifier. In order to conduct experiments, web spam uk2007 dataset is considered. Ten, twenty, thirty, and forty features were selected as an optimal subset from web spam uk2007 dataset. Among all three feature selection algorithms, CS and IG had highest F1Score (Fmeasure =0.911) but at the same time suffered with model building time.\n",
            "\n",
            "Title: An efficient approach for textual data classification using deep learning\n",
            "Abstract: Text categorization is an effective activity that can be accomplished using a variety of classification algorithms. In machine learning, the classifier is built by learning the features of categories from a set of preset training data. Similarly, deep learning offers enormous benefits for text classification since they execute highly accurately with lower-level engineering and processing. This paper employs machine and deep learning techniques to classify textual data. Textual data contains much useless information that must be pre-processed. We clean the data, impute missing values, and eliminate the repeated columns. Next, we employ machine learning algorithms: logistic regression, random forest, K-nearest neighbors (KNN), and deep learning algorithms: long short-term memory (LSTM), artificial neural network (ANN), and gated recurrent unit (GRU) for classification. Results reveal that LSTM achieves 92% accuracy outperforming all other model and baseline studies.\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{\n",
            "  \"topic\": \"Cross-modal Retrieval in Text Neural Information Retrieval\",\n",
            "  \"title\": \"Enhancing Cross-Modal Retrieval in Text Neural Information Retrieval\",\n",
            "  \"reasoning\": \"The abstract discusses a new approach to improve cross-modal retrieval performance in vision and language. It highlights the challenges of learning optimal embeddings for visual and lingual information and proposes a method that utilizes multiple individual representation spaces. This aligns with the topic of cross-modal retrieval in text neural information retrieval, where the goal is to improve the retrieval of information across different modalities.\"\n",
            "}\n",
            "Results for section 1\n",
            "{'topic': 'Cross-modal Retrieval in Text Neural Information Retrieval', 'title': 'Enhancing Cross-Modal Retrieval in Text Neural Information Retrieval', 'reasoning': 'The abstract discusses a new approach to improve cross-modal retrieval performance in vision and language. It highlights the challenges of learning optimal embeddings for visual and lingual information and proposes a method that utilizes multiple individual representation spaces. This aligns with the topic of cross-modal retrieval in text neural information retrieval, where the goal is to improve the retrieval of information across different modalities.'}\n",
            "....................................................................................................\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: You are a renowned scientist who is writing a survey on 'text neural information retrieval'. You are currently writing a section about 'Multi-modal Retrieval in Text Neural Information Retrieval'\n",
            "Human: I will send you a list of title and abstract of scientific articles. Most of them cover a specific topic about the section 'Multi-modal Retrieval in Text Neural Information Retrieval'.\n",
            "Your task is to find out what this topic is and suggest a good title for a section in a scientific survey that addresses it.\n",
            "You should also explain your reasoning.\n",
            "Your answer should be a valid RFC8259 compliant JSON object with three properties:\n",
            "The first property, called \"topic\", describes the main topic that is a subset of 'Multi-modal Retrieval in Text Neural Information Retrieval' that are present in most abstracts.\n",
            "The second property, called \"title\", is the title of the section that will cover this topic and must be clearly related to the property \"topic\".\n",
            "The third property is called \"reasoning\" and should contains your reasoning to choose this topic as an answer.\n",
            "Use this format: \n",
            "{\n",
            "  \"topic\": {TOPIC},\n",
            "  \"title\": {TITLE},\n",
            "  \"reasoning\": {REASONING}\n",
            "}\n",
            "\n",
            "Do you understand?\n",
            "\n",
            "AI: Sure, send me the list and I will give you what you need.\n",
            "Human: Title: Storage capacity of networks with discrete synapses and sparsely encoded memories.\n",
            "Abstract: Attractor neural networks are one of the leading theoretical frameworks for the formation and retrieval of memories in networks of biological neurons. In this framework, a pattern imposed by external inputs to the network is said to be learned when this pattern becomes a fixed point attractor of the network dynamics. The storage capacity is the maximum number of patterns that can be learned by the network. In this paper, we study the storage capacity of fully connected and sparsely connected networks with a binarized Hebbian rule, for arbitrary coding levels. Our results show that a network with discrete synapses has a similar storage capacity as the model with continuous synapses, and that this capacity tends asymptotically towards the optimal capacity, in the space of all possible binary connectivity matrices, in the sparse coding limit. We also derive finite coding level corrections for the asymptotic solution in the sparse coding limit. The result indicates the capacity of networks with Hebbian learning rules converges to the optimal capacity extremely slowly when the coding level becomes small. Our results also show that in networks with sparse binary connectivity matrices, the information capacity per synapse is larger than in the fully connected case, and thus such networks store information more efficiently.\n",
            "\n",
            "Title: Pattern Separation Underpins Expectation-Modulated Memory\n",
            "Abstract: Pattern separation and completion are fundamental hippocampal computations supporting memory encoding and retrieval. However, despite extensive exploration of these processes, it remains unclear whether and how top-down processes adaptively modulate the dynamics between these computations. Here we examine the role of expectation in shifting the hippocampus to perform pattern separation. In a behavioral task, 29 participants (7 males) learned a cue-object category contingency. Pattern separation and completion are fundamental hippocampal computations supporting memory encoding and retrieval. However, despite extensive exploration of these processes, it remains unclear whether and how top-down processes adaptively modulate the dynamics between these computations. Here we examine the role of expectation in shifting the hippocampus to perform pattern separation. In a behavioral task, 29 participants (7 males) learned a cue-object category contingency. Then, at encoding, one-third of the cues preceding the to-be-memorized objects, violated the studied rule. At test, participants performed a recognition task with old objects (targets) and a set of parametrically manipulated (very similar to dissimilar) foils for each object. Accuracy was found to be better for foils of high similarity to targets that were contextually unexpected at encoding compared with expected ones. Critically, there were no expectation-driven differences for targets and low similarity foils. To further explore these effects, we implemented a computational model of the hippocampus, performing the same task as the human participants. We used representational similarity analysis to examine how top-down expectation interacts with bottom-up perceptual input, in each layer. All subfields showed more dissimilar representations for unexpected items, with dentate gyrus (DG) and CA3 being more sensitive to expectation violation than CA1. Again, representational differences between expected and unexpected inputs were prominent for moderate to high levels of input similarity. This effect diminished when inputs from DG and CA3 into CA1 were lesioned. Overall, these novel findings strongly suggest that pattern separation in DG/CA3 underlies the effect that violation of expectation exerts on memory. SIGNIFICANCE STATEMENT What makes some events more memorable than others is a key question in cognitive neuroscience. Violation of expectation often leads to better memory performance, but the neural mechanism underlying this benefit remains elusive. In a behavioral study, we found that memory accuracy is enhanced selectively for unexpected highly similar foils, suggesting expectation violation does not enhance memory indiscriminately, but specifically aids the disambiguation of overlapping inputs. This is further supported by our subsequent investigation using a hippocampal computational model, revealing increased representational dissimilarity for unexpected highly similar foils in DG and CA3. These\n",
            "\n",
            "Title: Graph-to-Tree Learning for Solving Math Word Problems\n",
            "Abstract: While the recent tree-based neural models have demonstrated promising results in generating solution expression for the math word problem (MWP), most of these models do not capture the relationships and order information among the quantities well. This results in poor quantity representations and incorrect solution expressions. In this paper, we propose Graph2Tree, a novel deep learning architecture that combines the merits of the graph-based encoder and tree-based decoder to generate better solution expressions. Included in our Graph2Tree framework are two graphs, namely the Quantity Cell Graph and Quantity Comparison Graph, which are designed to address limitations of existing methods by effectively representing the relationships and order information among the quantities in MWPs. We conduct extensive experiments on two available datasets. Our experiment results show that Graph2Tree outperforms the state-of-the-art baselines on two benchmark datasets significantly. We also discuss case studies and empirically examine Graph2Tree’s effectiveness in translating the MWP text into solution expressions.\n",
            "\n",
            "Title: Image Based Classification of Rumor Information from the Social Network Platform\n",
            "Abstract: Received: 12 December 2020 Accepted: 25 July 2021 Spam emails based on images readily evade text-based spam email filters. More and more spammers are adopting the technology. The essence of email is necessary in order to recognize image content. Web-based social networking is a method of communication between the information owner and end users for online exchanges that use social network data in the form of images and text. Nowadays, information is passed on to users in shorter time using social networks, and the spread of fraudulent material on social networks has become a major issue. It is critical to assess and decide which features the filters require to combat spammers. Spammers also insert text into photographs, causing text filters to fail. The detection of visual garbage material has become a hotspot study on spam filters on the Internet. The suggested approach includes a supplementary detection engine that uses visuals as well as text input. This paper proposed a system for the assessment of information, the detection of information on fraud-based mails and the avoidance of distribution to end users for the purpose of enhancing data protection and preventing safety problems. The proposed model utilizes Machine Learning and Convolutional Neural Network (CNN) methods to recognize and prevent fraud information being transmitted to end users.\n",
            "\n",
            "Title: Extension of Direct Citation Model Using In-Text Citations\n",
            "Abstract: : Citations based relevant research paper recommendations can be generated primarily with the assistance of three citation models: (1) Bibliographic Coupling, (2) Co-Citation, and (3) Direct Citations. Millions of new scholarly articles are published every year. This flux of scientific information has made it a challenging task to devise techniques that could help researchers to find the most relevant research papers for the paper at hand. In this study, we have deployed an in-text citation analysis that extends the Direct Citation Model to discover the nature of the relationship degree-of-relevancy among scientific papers. For this purpose, the relationship between citing and cited articles is categorized into three categories: weak, medium, and strong. As an experiment, around 5,000 research papers were crawled from the CiteSeerX. These research papers were parsed for the identification of in-text citation frequencies. Subsequently, 0.1 million references of those articles were extracted, and their in-text citation frequencies were computed. A comprehensive benchmark dataset was established based on the user study. Afterwards, the results were validated with the help of Least Square Approximation by Quadratic Polynomial method. It was found that degree-of-relevancy between scientific papers is a quadratic increasing/decreasing polynomial with respect to-increase/decrease in the in-text citation frequencies of a cited article. Furthermore, the results of the proposed model were compared with state-of-the-art techniques by utilizing a well-known measure, known as the normalized Discount Cumulative Gain (nDCG). The proposed method received an nDCG score of 0.89, whereas the state-of-the-art models such as the Content, Bibliographic-coupling, and Metadata-based Models were able to acquire the nDCG values of 0.65, 0.54, and 0.51 respectively. These results indicate that the proposed mechanism may be applied in future information retrieval systems for better results.\n",
            "\n",
            "Title: Linguistic knowledge-based vocabularies for Neural Machine Translation\n",
            "Abstract: Abstract Neural Networks applied to Machine Translation need a finite vocabulary to express textual information as a sequence of discrete tokens. The currently dominant subword vocabularies exploit statistically-discovered common parts of words to achieve the flexibility of character-based vocabularies without delegating the whole learning of word formation to the neural network. However, they trade this for the inability to apply word-level token associations, which limits their use in semantically-rich areas and prevents some transfer learning approaches e.g. cross-lingual pretrained embeddings, and reduces their interpretability. In this work, we propose new hybrid linguistically-grounded vocabulary definition strategies that keep both the advantages of subword vocabularies and the word-level associations, enabling neural networks to profit from the derived benefits. We test the proposed approaches in both morphologically rich and poor languages, showing that, for the former, the quality in the translation of out-of-domain texts is improved with respect to a strong subword baseline.\n",
            "\n",
            "Title: Analysis of natural language processing technology: modern problems and approaches\n",
            "Abstract: Introduction. The article presents an overview of modern neural network models for natural language processing. Research into natural language processing is of interest as the need to process large amounts of audio and text information accumulated in recent decades has increased. The most discussed in foreign literature are the features of the processing of spoken language. The aim of the work is to present modern models of neural networks in the field of oral speech processing.Materials and Methods. Applied research on understanding spoken language is an important and far-reaching topic in the natural language processing. Listening comprehension is central to practice and presents a challenge. This study meets a method of hearing detection based on deep learning. The article briefly outlines the substantive aspects of various neural networks for speech recognition, using the main terms associated with this theory. A brief description of the main points of the transformation of neural networks into a natural language is given.Results. A retrospective analysis of foreign and domestic literary sources was carried out alongside with a description of new methods for oral speech processing, in which neural networks were used. Information about neural networks, methods of speech recognition and synthesis is provided. The work includes the results of diverse experimental works of recent years. The article elucidates the main approaches to natural language processing and their changes over time, as well as the emergence of new technologies. The major problems currently existing in this area are considered.Discussion and Conclusions. The analysis of the main aspects of speech recognition systems has shown that there is currently no universal system that would be self-learning, noise-resistant, recognizing continuous speech, capable of working with large dictionaries and at the same time having a low error rate.\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{\n",
            "  \"topic\": \"Pattern Separation and Completion in Memory Retrieval\",\n",
            "  \"title\": \"Pattern Separation and Completion in Memory Retrieval\",\n",
            "  \"reasoning\": \"Based on the abstract, the article discusses the role of pattern separation and completion in memory encoding and retrieval. It explores how top-down processes, specifically expectation, modulate the dynamics between these computations. The study investigates the effects of expectation violation on memory accuracy and uses a computational model of the hippocampus to understand the underlying neural mechanisms. Therefore, the main topic of this article is pattern separation and completion in memory retrieval.\"\n",
            "}\n",
            "Results for section 2\n",
            "{'topic': 'Pattern Separation and Completion in Memory Retrieval', 'title': 'Pattern Separation and Completion in Memory Retrieval', 'reasoning': 'Based on the abstract, the article discusses the role of pattern separation and completion in memory encoding and retrieval. It explores how top-down processes, specifically expectation, modulate the dynamics between these computations. The study investigates the effects of expectation violation on memory accuracy and uses a computational model of the hippocampus to understand the underlying neural mechanisms. Therefore, the main topic of this article is pattern separation and completion in memory retrieval.'}\n",
            "....................................................................................................\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: You are a renowned scientist who is writing a survey on 'text neural information retrieval'. You are currently writing a section about 'text neural information retrieval'\n",
            "Human: I will send you a list of title and abstract of scientific articles. Most of them cover a specific topic about the section 'text neural information retrieval'.\n",
            "Your task is to find out what this topic is and suggest a good title for a section in a scientific survey that addresses it.\n",
            "You should also explain your reasoning.\n",
            "Your answer should be a valid RFC8259 compliant JSON object with three properties:\n",
            "The first property, called \"topic\", describes the main topic that is a subset of 'text neural information retrieval' that are present in most abstracts.\n",
            "The second property, called \"title\", is the title of the section that will cover this topic and must be clearly related to the property \"topic\".\n",
            "The third property is called \"reasoning\" and should contains your reasoning to choose this topic as an answer.\n",
            "Use this format: \n",
            "{\n",
            "  \"topic\": {TOPIC},\n",
            "  \"title\": {TITLE},\n",
            "  \"reasoning\": {REASONING}\n",
            "}\n",
            "\n",
            "Do you understand?\n",
            "\n",
            "AI: Sure, send me the list and I will give you what you need.\n",
            "Human: Title: Customized mask region based convolutional neural networks for un-uniformed shape text detection and text recognition\n",
            "Abstract: In image scene, text contains high-level of important information that helps to analyze and consider the particular environment. In this paper, we adapt image mask and original identification of the mask region based convolutional neural networks (R-CNN) to allow recognition at 3 levels such as sequence, holistic and pixel-level semantics. Particularly, pixel and holistic level semantics can be utilized to recognize the texts and define the text shapes, respectively. Precisely, in mask and detection, we segment and recognize both character and word instances. Furthermore, we implement text detection through the outcome of instance segmentation on 2-D feature-space. Also, to tackle and identify the text issues of smaller and blurry texts, we consider text recognition by attention-based of optical character recognition (OCR) model with the mask R-CNN at sequential level. The OCR module is used to estimate character sequence through feature maps of the word instances in sequence to sequence. Finally, we proposed a fine-grained learning technique that trains a more accurate and robust model by learning models from the annotated datasets at the word level. Our proposed approach is evaluated on popular benchmark dataset ICDAR 2013 and ICDAR 2015.\n",
            "\n",
            "Title: Deep Hash with Improved Dual Attention for Image Retrieval\n",
            "Abstract: Recently, deep learning to hash has extensively been applied to image retrieval, due to its low storage cost and fast query speed. However, there is a defect of insufficiency and imbalance when existing hashing methods utilize the convolutional neural network (CNN) to extract image semantic features and the extracted features do not include contextual information and lack relevance among features. Furthermore, the process of the relaxation hash code can lead to an inevitable quantization error. In order to solve these problems, this paper proposes deep hash with improved dual attention for image retrieval (DHIDA), which chiefly has the following contents: (1) this paper introduces the improved dual attention mechanism (IDA) based on the ResNet18 pre-trained module to extract the feature information of the image, which consists of the position attention module and the channel attention module; (2) when calculating the spatial attention matrix and channel attention matrix, the average value and maximum value of the column of the feature map matrix are integrated in order to promote the feature representation ability and fully leverage the features of each position; and (3) to reduce quantization error, this study designs a new piecewise function to directly guide the discrete binary code. Experiments on CIFAR-10, NUS-WIDE and ImageNet-100 show that the DHIDA algorithm achieves better performance.\n",
            "\n",
            "Title: MedKnowts: Unified Documentation and Information Retrieval for Electronic Health Records\n",
            "Abstract: Clinical documentation can be transformed by Electronic Health Records, yet the documentation process is still a tedious, time-consuming, and error-prone process. Clinicians are faced with multi-faceted requirements and fragmented interfaces for information exploration and documentation. These challenges are only exacerbated in the Emergency Department—clinicians often see 35 patients in one shift, during which they have to synthesize an often previously unknown patient’s medical records in order to reach a tailored diagnosis and treatment plan. To better support this information synthesis, clinical documentation tools must enable rapid contextual access to the patient’s medical record. MedKnowts is an integrated note-taking editor and information retrieval system which unifies the documentation and search process and provides concise synthesized concept-oriented slices of the patient’s medical record. MedKnowts automatically captures structured data while still allowing users the flexibility of natural language. MedKnowts leverages this structure to enable easier parsing of long notes, auto-populated text, and proactive information retrieval, easing the documentation burden.\n",
            "\n",
            "Title: Estimation of Arctic Winter Snow Depth, Sea Ice Thickness and Bulk Density, and Ice Freeboard by Combining CryoSat-2, AVHRR, and AMSR Measurements\n",
            "Abstract: Information on snow depth on sea ice and bulk sea ice density is required to convert CryoSat-2 radar freeboard (<inline-formula> <tex-math notation=\"LaTeX\">$F_{r}$ </tex-math></inline-formula>) into sea ice thickness (SIT). It is difficult to obtain their information on an Arctic basin scale; therefore, most CryoSat-2 SIT products largely rely on the distributions of snow depth and bulk sea ice density derived from parameterizations, which are based on sea ice type and climatological values. Several observational studies have found that the distributions of parameterized variables are inaccurate compared to the actual distributions. This study aims to develop a new type of retrieval algorithm for snow depth, SIT and bulk density, and ice freeboard in the Arctic winter by synergizing active CryoSat-2 with passive microwave and infrared measurements. Two parameterizations for the snow–ice thickness ratio and bulk sea ice density were combined with the hydrostatic balance and radar wave speed correction equations. Consequently, solutions for the four target variables were obtained and applied to different CryoSat-<inline-formula> <tex-math notation=\"LaTeX\">$2~F_{r}$ </tex-math></inline-formula>, derived from empirical and waveform-fitting (WF) retracker algorithms. The retrieved thickness-related parameters based on <inline-formula> <tex-math notation=\"LaTeX\">$F_{r}$ </tex-math></inline-formula> from the Lognormal WF retracker algorithm showed good agreement with the airborne snow depth, total freeboard, and mooring ice draft measurements. The retrieved multiyear sea ice bulk density was significantly higher than the value of 882 kg <inline-formula> <tex-math notation=\"LaTeX\">$\\cdot ~\\text{m}^{-3}$ </tex-math></inline-formula>, which was used in the previous density parameterization, showing a higher agreement with values from in situ measurements. The spatial and interannual variabilities of SIT increased when the results from this study were compared with those based on previous parameterizations.\n",
            "\n",
            "Title: BioMedBERT: A Pre-trained Biomedical Language Model for QA and IR\n",
            "Abstract: The SARS-CoV-2 (COVID-19) pandemic spotlighted the importance of moving quickly with biomedical research. However, as the number of biomedical research papers continue to increase, the task of finding relevant articles to answer pressing questions has become significant. In this work, we propose a textual data mining tool that supports literature search to accelerate the work of researchers in the biomedical domain. We achieve this by building a neural-based deep contextual understanding model for Question-Answering (QA) and Information Retrieval (IR) tasks. We also leverage the new BREATHE dataset which is one of the largest available datasets of biomedical research literature, containing abstracts and full-text articles from ten different biomedical literature sources on which we pre-train our BioMedBERT model. Our work achieves state-of-the-art results on the QA fine-tuning task on BioASQ 5b, 6b and 7b datasets. In addition, we observe superior relevant results when BioMedBERT embeddings are used with Elasticsearch for the Information Retrieval task on the intelligently formulated BioASQ dataset. We believe our diverse dataset and our unique model architecture are what led us to achieve the state-of-the-art results for QA and IR tasks.\n",
            "\n",
            "Title: Cross-lingual Search for e-Commerce based on Query Translatability and Mixed-Domain Fine-Tuning\n",
            "Abstract: Online stores in the US offer a unique scenario for Cross-Lingual Information Retrieval (CLIR) due to the mix of Spanish and English in user queries. Machine Translation (MT) provides an opportunity to lift relevance by translating the Spanish queries to English before delivering them to the search engine. However, polysemy-derived problems, high latency and context scarcity in product search, make generic MT an impractical solution. The wide diversity of products in marketplaces injects non-translatable entities, loanwords, ambiguous morphemes, cross-language ambiguity and a variety of Spanish dialects in the communication between buyers and sellers, posing a thread to the accuracy of MT. In this work, we leverage domain adaptation on a simplified architecture of Neural Machine Translation (NMT) to make both latency and accuracy suitable for e-commerce search. Our NMT model is fine-tuned on a mixed-domain corpus based on engagement data expanded with catalog back-translation techniques. Beyond accuracy, and given that translation is not the goal but the means to relevant results, the problem of Query Translatability is addressed by a classifier on whether the translation should be automatic or explicitly requested. We assembled these models into a query translation system that we tested and launched at Walmart.com , with a statistically significant lift in Spanish GMV and an nDCG gain for Spanish queries of +70%.\n",
            "\n",
            "Title: Continuous Self-Attention Models with Neural ODE Networks\n",
            "Abstract: Stacked self-attention models receive widespread attention, due to its ability of capturing global dependency among words. However, the stacking of many layers and components generates huge parameters, leading to low parameter efficiency. In response to this issue, we propose a lightweight architecture named Continuous Self-Attention models with neural ODE networks (CSAODE). In CSAODE, continuous dynamical models (i.e., neural ODEs) are coupled with our proposed self-attention block to form a self-attention ODE solver. This solver continuously calculates and optimizes the hidden states via only one layer of parameters to improve the parameter efficiency. In addition, we design a novel accelerated continuous dynamical model to reduce computing costs, and integrate it in CSAODE. Moreover, since the original self-attention ignores local information, CSAODE makes use of N-gram convolution to encode local representations, and a fusion layer with only two trainable scalars are designed for generating sentence vectors. We perform a series of experiments on text classification, neural language inference (NLI) and text matching tasks. With fewer parameters, CSAODE outperforms state-of-the-art models on text classification tasks (e.g., 1.3% accuracy improved on SUBJ task), and has competitive performances for NLI and text matching tasks as well.\n",
            "\n",
            "Title: Domain Balanced Sampling and Iterative Search for Product Identification\n",
            "Abstract: This paper introduces our solution to the 1st Workshop on Multimodal Product Identification in Livestreaming and Watch and Buy Challenge, a real-world task in a live stream scene and is challenging due to factors such as lighting, occlusion, and cross-domain. We model this task as the object detection and image retrieval problem. In the whole pipeline, we mainly focus on the problem of cross-domain retrieval and propose a domain-balanced sampling method, which enhances the robustness of multi-domain retrieval. Besides, to eliminate the influence of irrelevant clothing, we propose an iterative cross-search strategy, which greatly improves the accuracy of matching. In addition, we also experiment with the exploitation of text information, including multimodal product classification and multimodal intent recognition. With the aforementioned method, we achieved an F1 score of 69.2% and finally achieve first place in the competition.\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{\n",
            "  \"topic\": \"Neural Information Retrieval for Text Analysis\",\n",
            "  \"title\": \"Advancements in Neural Information Retrieval for Text Analysis\",\n",
            "  \"reasoning\": \"The majority of the abstracts focus on the application of neural networks and deep learning techniques for information retrieval in text analysis. These techniques include customized mask region based convolutional neural networks, deep hash with improved dual attention, pre-trained biomedical language models, continuous self-attention models with neural ODE networks, and domain balanced sampling and iterative search. Therefore, a section titled 'Advancements in Neural Information Retrieval for Text Analysis' would encompass these topics and highlight the progress made in utilizing neural networks for text analysis and information retrieval.\"\n",
            "}\n",
            "Results for section 2\n",
            "{'topic': 'Neural Information Retrieval for Text Analysis', 'title': 'Advancements in Neural Information Retrieval for Text Analysis', 'reasoning': \"The majority of the abstracts focus on the application of neural networks and deep learning techniques for information retrieval in text analysis. These techniques include customized mask region based convolutional neural networks, deep hash with improved dual attention, pre-trained biomedical language models, continuous self-attention models with neural ODE networks, and domain balanced sampling and iterative search. Therefore, a section titled 'Advancements in Neural Information Retrieval for Text Analysis' would encompass these topics and highlight the progress made in utilizing neural networks for text analysis and information retrieval.\"}\n",
            "....................................................................................................\n",
            "Extracting subsections...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27ea8e1b05264095a468e705fdfaefa2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster 2: 22\n",
            "Cluster 1: 17\n",
            "Cluster 0: 11\n",
            "Extracting 3 sections in this level:\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: You are a renowned scientist who is writing a survey on 'text neural information retrieval'. You are currently writing a section about 'Advancements in Neural Information Retrieval for Text Analysis'\n",
            "Human: I will send you a list of title and abstract of scientific articles. Most of them cover a specific topic about the section 'Advancements in Neural Information Retrieval for Text Analysis'.\n",
            "Your task is to find out what this topic is and suggest a good title for a section in a scientific survey that addresses it.\n",
            "You should also explain your reasoning.\n",
            "Your answer should be a valid RFC8259 compliant JSON object with three properties:\n",
            "The first property, called \"topic\", describes the main topic that is a subset of 'Advancements in Neural Information Retrieval for Text Analysis' that are present in most abstracts.\n",
            "The second property, called \"title\", is the title of the section that will cover this topic and must be clearly related to the property \"topic\".\n",
            "The third property is called \"reasoning\" and should contains your reasoning to choose this topic as an answer.\n",
            "Use this format: \n",
            "{\n",
            "  \"topic\": {TOPIC},\n",
            "  \"title\": {TITLE},\n",
            "  \"reasoning\": {REASONING}\n",
            "}\n",
            "\n",
            "Do you understand?\n",
            "\n",
            "AI: Sure, send me the list and I will give you what you need.\n",
            "Human: Title: Various Approaches of Machine Translation for Marathi to English Language\n",
            "Abstract: Machine Translation (MT) is a generic term for computerised systems that generate translations from one natural language to another, with or without human intervention. Text may be used to examine knowledge, and turning that information into pictures helps people to communicate and acquire information.There seems to be a lot of work conducted on translating English to Hindi, Tamil, Bangla and other languages. The important parts of translation are to provide translated sentences with correct words and proper grammar. There has been a comprehensive review of 10 primary publications used in research. Two separate approaches are proposed, one uses rule based approach and other uses neural-machine translation approach to translate basic Marathi phrases to English. While designed primarily for Marathi-English language pairs, the design can be applied to other language pairs with a similar structure.\n",
            "\n",
            "Title: A Convolutional Recurrent Neural-Network-Based Machine Learning for Scene Text Recognition Application\n",
            "Abstract: Optical character recognition (OCR) is the process of acquiring text and layout information through analysis and recognition of text data image files. It is also a process to identify the geometric location and orientation of the texts and their symmetrical behavior. It usually consists of two steps: text detection and text recognition. Scene text recognition is a subfield of OCR that focuses on processing text in natural scenes, such as streets, billboards, license plates, etc. Unlike traditional document category photographs, it is a challenging task to use computer technology to locate and read text information in natural scenes. Imaging sequence recognition is a longstanding subject of research in the field of computer vision. Great progress has been made in this field; however, most models struggled to recognize text in images of complex scenes with high accuracy. This paper proposes a new pattern of text recognition based on the convolutional recurrent neural network (CRNN) as a solution to address this issue. It combines real-time scene text detection with differentiable binarization (DBNet) for text detection and segmentation, text direction classifier, and the Retinex algorithm for image enhancement. To evaluate the effectiveness of the proposed method, we performed experimental analysis of the proposed algorithm, and carried out simulation on complex scene image data based on existing literature data and also on several real datasets designed for a variety of nonstationary environments. Experimental results demonstrated that our proposed model performed better than the baseline methods on three benchmark datasets and achieved on-par performance with other approaches on existing datasets. This model can solve the problem that CRNN cannot identify text in complex and multi-oriented text scenes. Furthermore, it outperforms the original CRNN model with higher accuracy across a wider variety of application scenarios.\n",
            "\n",
            "Title: Multimodal Semantic Consistency-Based Fusion Architecture Search for Land Cover Classification\n",
            "Abstract: Multimodal land cover classification (MLCC) using the optical and synthetic aperture radar (SAR) modalities has resulted in outstanding performances over using only unimodal data due to their complementary information on land properties. Previous multimodal deep learning (MDL) methods have relied on handcrafted multibranch convolutional neural networks (CNN) to extract the features of different modalities and merged them for land cover classification. However, natural-image-oriented handcrafted CNN models may not be the optimal strategies to handle remote sensing (RS) image interpretation problems, due to the huge difference in terms of imaging angles and imaging ways. Furthermore, few MDL methods have analyzed optimal combinations of hierarchical features from different modalities. In this article, we propose an efficient multimodal architecture search framework, namely, multimodal semantic consistency-based fusion architecture search ( $\\text{M}^{2}$ SC-FAS) in continuous search space with the gradient-based optimization method, which can not only discover optimal optical- and SAR-specific architectures according to the different characteristics of the optical and SAR images, respectively, but also realizes the search of optimal multimodal dense fusion architecture. Specifically, the semantic consistency constraint is introduced to guarantee dense fusion between hierarchical optical and SAR features with high semantic consistency and then capture the complementary performance on land properties. Finally, the basis of curriculum learning strategy is adopted on $\\text{M}^{2}$ SC-FAS. Extensive experiments show superior performances of our work on three broad coregistered optical and SAR datasets.\n",
            "\n",
            "Title: Neural Network: An Improved FCM for Multimodal Cultural Data Analysis\n",
            "Abstract: With the development of cloud computing, big data, and artificial intelligence (AI) technology, there is a growing interest in “cultural analysis.” Cultural analysis requires different types of data such as texts, pictures, and videos. The richness and differences of resources in the cultural field lead to diverse modalities of cultural data. Traditional text analysis methods can no longer meet the data analysis needs of current multimedia cultural resources. This article starts from cultural data’s feature information to solve the heterogeneity problem faced by massive multimodal cultural data analysis. It analyzes it from geography, time, art, and thematic character, classified and aggregated to form a multimodal cultural feature information matrix. The corresponding correlation measurement methods for different matrices from the above dimensions are proposed, solved in turn, and substituted into the optimized training back propagation (BP) neural network to obtain the final correlation degree. The improved fuzzy C-means (FCM) clustering algorithm is used to aggregate the high correlation cultural data based on the degree. The algorithm proposed in this study is compared with the existing algorithm. The experimental results show that the optimized BP neural network is at least 58% more accurate than the current method for calculating different matrices’ correlation degrees. In terms of accuracy, the improved fuzzy C-means algorithm effectively reduces the random interference in the selection of the initial clustering center, which is significantly higher than other clustering algorithms.\n",
            "\n",
            "Title: Ontology Learning Applications of Knowledge Base Construction for Microelectronic Systems Information\n",
            "Abstract: Knowledge base construction (KBC) using AI has been one of the key goals of this highly popular technology since its emergence, as it helps to comprehend everything, including relations, around us. The construction of knowledge bases can summarize a piece of text in a machine-processable and understandable way. This can prove to be valuable and assistive to knowledge engineers. In this paper, we present the application of natural language processing in the construction of knowledge bases. We demonstrate how a trained bidirectional long short-term memory or bi-LSTM neural network model can be used to construct knowledge bases in accordance with the exact ISO26262 definitions as defined in the GENIAL! Basic Ontology. We provide the system with an electronic text document from the microelectronics domain and the system attempts to create a knowledge base from the available information in textual format. This information is then expressed in the form of graphs when queried by the user. This method of information retrieval presents the user with a much more technical and comprehensive understanding of an expert piece of text. This is achieved by applying the process of named entity recognition (NER) for knowledge extraction. This paper provides a result report of the current status of our knowledge construction process and knowledge base content, as well as describes our challenges and experiences.\n",
            "\n",
            "Title: Long Short-Term Memory Neural Networks for RNA Viruses Mutations Prediction\n",
            "Abstract: Viral progress remains a major deterrent in the viability of antiviral drugs. +e ability to anticipate this development will provide assistance in the early detection of drug-resistant strains and may encourage antiviral drugs to be the most effective plan. In recent years, a deep learning model called the seq2seq neural network has emerged and has been widely used in natural language processing. In this research, we borrow this approach for predicting next generation sequences using the seq2seq LSTM neural network while considering these sequences as text data. We used hot single vectors to represent the sequences as input to the model; subsequently, it maintains the basic information position of each nucleotide in the sequences. Two RNA viruses sequence datasets are used to evaluate the proposed model which achieved encouraging results. +e achieved results illustrate the potential for utilizing the LSTM neural network for DNA and RNA sequences in solving other sequencing issues in bioinformatics.\n",
            "\n",
            "Title: Augmentation of Contextualized Concatenated Word Representation and Dilated Convolution Neural Network for Sentiment Analysis\n",
            "Abstract: Deep learning-based methodologies are significant to perform sentiment analysis on social media data. The valuable insights of social media data through sentiment analysis can be employed to develop intelligent applications. Among many networks, convolution neural networks (CNNs) are widely used in many conventional text classification tasks and perform a significant role. However, to capture long-term contextual information and address the detail loss problem, CNNs require stacking multiple convolutional layers. Also, the stacking of convolutional layers has issues requiring massive computations and the tuning of additional parameters. To solve these problems, in this paper, a contextualized concatenated word representation (CCWRs) is initialized from social media data based on text which is essential to misspelled and out of vocabulary words (OOV). In CCWRs, different word representation models, for example, Word2Vec, its optimized version FastText and Global Vectors, and GloVe, collectively create contextualized representations upon the sequence of input. Second, a three-layered dilated convolutional neural network (3D-CNN) is proposed that places dilated convolution kernels instead of conventional CNN kernels. Incorporating the extension in the receptive field’s size successfully solves the detail loss problem and achieves long-term context information with different dilation rates. Experiments on datasets demonstrate that the proposed framework achieves reliable results with the selection of numerous hyperparameter tuning and configurations for improved optimization leads to reduced computational resources and reliable accuracy.\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{\n",
            "  \"topic\": \"Text Classification using Neural Networks\",\n",
            "  \"title\": \"Advancements in Neural Networks for Text Classification\",\n",
            "  \"reasoning\": \"The majority of the abstracts discuss advancements in neural networks for various text analysis tasks such as machine translation, scene text recognition, land cover classification, cultural data analysis, knowledge base construction, and sentiment analysis. These tasks all fall under the broader topic of text classification using neural networks, where the goal is to automatically categorize or analyze text data using advanced neural network models. Therefore, a suitable title for the section would be 'Advancements in Neural Networks for Text Classification', as it encompasses the common theme across the abstracts.\"\n",
            "}\n",
            "Results for section 0\n",
            "{'topic': 'Text Classification using Neural Networks', 'title': 'Advancements in Neural Networks for Text Classification', 'reasoning': \"The majority of the abstracts discuss advancements in neural networks for various text analysis tasks such as machine translation, scene text recognition, land cover classification, cultural data analysis, knowledge base construction, and sentiment analysis. These tasks all fall under the broader topic of text classification using neural networks, where the goal is to automatically categorize or analyze text data using advanced neural network models. Therefore, a suitable title for the section would be 'Advancements in Neural Networks for Text Classification', as it encompasses the common theme across the abstracts.\"}\n",
            "....................................................................................................\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: You are a renowned scientist who is writing a survey on 'text neural information retrieval'. You are currently writing a section about 'Advancements in Neural Information Retrieval for Text Analysis'\n",
            "Human: I will send you a list of title and abstract of scientific articles. Most of them cover a specific topic about the section 'Advancements in Neural Information Retrieval for Text Analysis'.\n",
            "Your task is to find out what this topic is and suggest a good title for a section in a scientific survey that addresses it.\n",
            "You should also explain your reasoning.\n",
            "Your answer should be a valid RFC8259 compliant JSON object with three properties:\n",
            "The first property, called \"topic\", describes the main topic that is a subset of 'Advancements in Neural Information Retrieval for Text Analysis' that are present in most abstracts.\n",
            "The second property, called \"title\", is the title of the section that will cover this topic and must be clearly related to the property \"topic\".\n",
            "The third property is called \"reasoning\" and should contains your reasoning to choose this topic as an answer.\n",
            "Use this format: \n",
            "{\n",
            "  \"topic\": {TOPIC},\n",
            "  \"title\": {TITLE},\n",
            "  \"reasoning\": {REASONING}\n",
            "}\n",
            "\n",
            "Do you understand?\n",
            "\n",
            "AI: Sure, send me the list and I will give you what you need.\n",
            "Human: Title: TripClick: The Log Files of a Large Health Web Search Engine\n",
            "Abstract: Click logs are valuable resources for a variety of information retrieval (IR) tasks. This includes query understanding/analysis, as well as learning effective IR models particularly when the models require large amounts of training data. We release a large-scale domain-specific dataset of click logs, obtained from user interactions of the Trip Database health web search engine. Our click log dataset comprises approximately 5.2 million user interactions collected between 2013 and 2020. We use this dataset to create a standard IR evaluation benchmark - TripClick - with around 700,000 unique free-text queries and 1.3 million pairs of query-document relevance signals, whose relevance is estimated by two click-through models. As such, the collection is one of the few datasets offering the necessary data richness and scale to train neural IR models with a large amount of parameters, and notably the first in the health domain. Using TripClick, we conduct experiments to evaluate a variety of IR models, showing the benefits of exploiting this data to train neural architectures. In particular, the evaluation results show that the best performing neural IR model significantly improves the performance by a large margin relative to classical IR models, especially for more frequent queries.\n",
            "\n",
            "Title: An Overview of Ontology Learning Process from Arabic Text\n",
            "Abstract: Ontology learning (OL) Plays an important role in many fields such as semantic web, Data Integration and Interoperability, Machine Translation, and Information Retrieval. The success of any of these fields depends on the quality of its ontologies. This paper presents a comprehensive survey of research on the Ontology Learning for Arabic texts. Most of the previous works focused on three main issues: extracting the terms, extracting the semantic relations, and building the ontology from the Arabic text. There are more of techniques and methods can be used for this process. In this paper, first we present the Arabic challenges that were reasons for developing few Arabic Ontology Learning systems. Second we make a research comparison based on the techniques used and their results. Third, we pointed limitations and comments of research works on Arabic Ontology Learning. Finally, we concluded the paper and outlined our future research direction in this area.\n",
            "\n",
            "Title: Using Phoneme Representations to Build Predictive Models Robust to ASR Errors\n",
            "Abstract: Even though Automatic Speech Recognition (ASR) systems significantly improved over the last decade, they still introduce a lot of errors when they transcribe voice to text. One of the most common reasons for these errors is phonetic confusion between similar-sounding expressions. As a result, ASR transcriptions often contain \"quasi-oronyms\", i.e., words or phrases that sound similar to the source ones, but that have completely different semantics (e.g., \"win\" instead of \"when\" or \"accessible on defecting\" instead of \"accessible and affecting\"). These errors significantly affect the performance of downstream Natural Language Understanding (NLU) models (e.g., intent classification, slot filling, etc.) and impair user experience. To make NLU models more robust to such errors, we propose novel phonetic-aware text representations. Specifically, we represent ASR transcriptions at the phoneme level, aiming to capture pronunciation similarities, which are typically neglected in word-level representations (e.g., word embeddings). To train and evaluate our phoneme representations, we generate noisy ASR transcriptions of four existing datasets - Stanford Sentiment Treebank, SQuAD, TREC Question Classification and Subjectivity Analysis - and show that common neural network architectures exploiting the proposed phoneme representations can effectively handle noisy transcriptions and significantly outperform state-of-the-art baselines. Finally, we confirm these results by testing our models on real utterances spoken to the Alexa virtual assistant.\n",
            "\n",
            "Title: Linguistic knowledge-based vocabularies for Neural Machine Translation\n",
            "Abstract: Abstract Neural Networks applied to Machine Translation need a finite vocabulary to express textual information as a sequence of discrete tokens. The currently dominant subword vocabularies exploit statistically-discovered common parts of words to achieve the flexibility of character-based vocabularies without delegating the whole learning of word formation to the neural network. However, they trade this for the inability to apply word-level token associations, which limits their use in semantically-rich areas and prevents some transfer learning approaches e.g. cross-lingual pretrained embeddings, and reduces their interpretability. In this work, we propose new hybrid linguistically-grounded vocabulary definition strategies that keep both the advantages of subword vocabularies and the word-level associations, enabling neural networks to profit from the derived benefits. We test the proposed approaches in both morphologically rich and poor languages, showing that, for the former, the quality in the translation of out-of-domain texts is improved with respect to a strong subword baseline.\n",
            "\n",
            "Title: Research on Quantitative Model of Brand Recognition Based on Sentiment Analysis of Big Data\n",
            "Abstract: This paper takes laptops as an example to carry out research on quantitative model of brand recognition based on sentiment analysis of big data. The basic idea is to use web crawler technology to obtain the most authentic and direct information of different laptop brands from first-line consumers from public spaces such as buyer reviews of major e-commerce platforms, including review time, text reviews, satisfaction ratings and relevant user information, etc., and then analyzes consumers’ sentimental tendencies and recognition status of the product brands. This study extracted a total of 437,815 user reviews of laptops from e-commerce platforms from January 1, 2019 to December 31, 2021, and performed data preprocessing on the obtained review data, followed by sentiment dictionary construction, attribute expansion, text quantification and algorithm evaluation. This paper analyzed the information receiving and processing hierarchy of the quantitative model of brand recognition, discussed the interactive relationship between brand recognition and consumer sentiment, discussed the brand recognition bias, style and demand in the context of big data, and performed the sentiment statistics and dimension analysis in the quantitative model of brand recognition. The study results show that the quantitative model of brand recognition based on sentiment analysis of big data can transform and map the keywords in text to word vectors in the high-dimensional semantic space by performing unsupervised machine learning on the text based on artificial neural network computer bionic metaphors; the model can accumulate each brand-related buyer review in the corresponding brand recognition dimension, so as to obtain the value of each product in each dimension of brand recognition; finally, the model will add the values of each dimension of brand recognition, that is, obtain the relevant value of the sum of each brand recognition. The results of this paper may provide a reference for further research on the quantitative model of brand recognition based on sentiment analysis of big data.\n",
            "\n",
            "Title: A Keyword Detection and Context Filtering Method for Document Level Relation Extraction\n",
            "Abstract: Relation extraction (RE) is the core link of downstream tasks, such as information retrieval, question answering systems, and knowledge graphs. Most of the current mainstream RE technologies focus on the sentence-level corpus, which has great limitations in practical applications. Moreover, the previously proposed models based on graph neural networks or transformers try to obtain context features from the global text, ignoring the importance of local features. In practice, the relation between entity pairs can usually be inferred just through a few keywords. This paper proposes a keyword detection and context filtering method based on the Self-Attention mechanism for document-level RE. In addition, a Self-Attention Memory (SAM) module in ConvLSTM is introduced to process the document context and capture keyword features. By searching for word embeddings with high cross-attention of entity pairs, we update and record critical local features to enhance the performance of the final classification model. The experimental results on three benchmark datasets (DocRED, CDR, and GBA) show that our model achieves advanced performance within open and specialized domain relationship extraction tasks, with up to 0.87% F1 value improvement compared to the state-of-the-art methods. We have also designed experiments to demonstrate that our model can achieve superior results by its stronger contextual filtering capability compared to other methods.\n",
            "\n",
            "Title: Tracking Your Mind's Eye during Recollection: Decoding the Long-Term Recall of Short Audiovisual Clips\n",
            "Abstract: Unlike familiarity, recollection involves the ability to reconstruct mentally previous events that results in a strong sense of reliving. According to the reinstatement hypothesis, this specific feature emerges from the reactivation of cortical patterns involved during information exposure. Over time, the retrieval of specific details becomes more difficult, and memories become increasingly supported by familiarity judgments. The multiple trace theory (MTT) explains the gradual loss of episodic details by a transformation in the memory representation, a view that is not shared by the standard consolidation model. In this study, we tested the MTT in light of the reinstatement hypothesis. The temporal dynamics of mental imagery from long-term memory were investigated and tracked over the passage of time. Participant EEG activity was recorded during the recall of short audiovisual clips that had been watched 3 weeks, 1 day, or a few hours beforehand. The recall of the audiovisual clips was assessed using a Remember/Know/New procedure, and snapshots of clips were used as recall cues. The decoding matrices obtained from the multivariate pattern analyses revealed sustained patterns that occurred at long latencies (>500 msec poststimulus onset) that faded away over the retention intervals and that emerged from the same neural processes. Overall, our data provide further evidence toward the MTT and give new insights into the exploration of our “mind's eye.”\n",
            "\n",
            "Title: Signal Phrase Extraction: A Gateway to Information Retrieval Improvement in Law Texts\n",
            "Abstract: NLP-based techniques can support in improving understanding of legal text documents. In this work we present a semi-automatic framework to extract signal phrases from legislative texts for an arbitrary European language. Through a case study using Dutch legislation, we demonstrate that it is feasible to extract these phrases reliably with a small number of supporting domain experts. Finally, we argue how in future works our framework could be utilized with existing methods to be applied to different languages.\n",
            "\n",
            "Title: Sentiment Polarity Identification of Social Media content using Artificial Neural Networks\n",
            "Abstract: Sentiment of people about consumer goods and government policies for decision making is normally collected through feedback forms, surveys etc. The social network sites and micro blogging sites are considered a very good source of information nowadays because people share and discuss their opinions about a certain topic freely. With the increased use of technology and social media, people proactively express their opinion through social media sites like Twitter, Facebook, Instagram etc. A social media sentiment analysis can help companies to understand how people feel about their products. On the other hand, extracting the sentiment from social media text is a challenging task due to the complexity of natural language processing of social media language. Often these messages reflect the emotion, opinion and sentiment of the public through a mix of text, image, emoticons etc. These statements are often called electronic Word of Mouth (eWOM) and are much prevalent in business and service industry to enable customers to share their point of view.\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{\n",
            "  \"topic\": \"Neural Information Retrieval for Text Analysis\",\n",
            "  \"title\": \"Advancements in Sentiment Analysis for Social Media Text\",\n",
            "  \"reasoning\": \"The abstract discusses the use of artificial neural networks for sentiment polarity identification in social media content. This falls under the broader topic of neural information retrieval for text analysis, as it focuses on extracting sentiment from text data and analyzing it using advanced neural network models.\"\n",
            "}\n",
            "Results for section 1\n",
            "{'topic': 'Neural Information Retrieval for Text Analysis', 'title': 'Advancements in Sentiment Analysis for Social Media Text', 'reasoning': 'The abstract discusses the use of artificial neural networks for sentiment polarity identification in social media content. This falls under the broader topic of neural information retrieval for text analysis, as it focuses on extracting sentiment from text data and analyzing it using advanced neural network models.'}\n",
            "....................................................................................................\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: You are a renowned scientist who is writing a survey on 'text neural information retrieval'. You are currently writing a section about 'Advancements in Neural Information Retrieval for Text Analysis'\n",
            "Human: I will send you a list of title and abstract of scientific articles. Most of them cover a specific topic about the section 'Advancements in Neural Information Retrieval for Text Analysis'.\n",
            "Your task is to find out what this topic is and suggest a good title for a section in a scientific survey that addresses it.\n",
            "You should also explain your reasoning.\n",
            "Your answer should be a valid RFC8259 compliant JSON object with three properties:\n",
            "The first property, called \"topic\", describes the main topic that is a subset of 'Advancements in Neural Information Retrieval for Text Analysis' that are present in most abstracts.\n",
            "The second property, called \"title\", is the title of the section that will cover this topic and must be clearly related to the property \"topic\".\n",
            "The third property is called \"reasoning\" and should contains your reasoning to choose this topic as an answer.\n",
            "Use this format: \n",
            "{\n",
            "  \"topic\": {TOPIC},\n",
            "  \"title\": {TITLE},\n",
            "  \"reasoning\": {REASONING}\n",
            "}\n",
            "\n",
            "Do you understand?\n",
            "\n",
            "AI: Sure, send me the list and I will give you what you need.\n",
            "Human: Title: Современные методы детектирования и классификации токсичных комментариев с использованием нейронных сетей\n",
            "Abstract: The growth of popularity of online platforms which allow users to communicate with each other, share opinions about various events, and leave comments boosted the development of natural language processing algorithms. Tens of millions of messages per day are published by users of a particular social network need to be analyzed in real time for moderation in order to prevent the spread of various illegal or offensive information, threats and other types of toxic comments. Of course, such a large amount of information can be processed quite quickly only automatically. that is why there is a need to and a way to teach computers to “understand” a text written by humans. It is a non-trivial task even if the word “understand” here means only “to classify”. the rapid evolution of machine learning technologies has led to ubiquitous implementation of new algorithms. A lot of tasks, which for many years were considered almost impossible to solve, are now quite successfully solved using deep learning technologies. this article considers algorithms built using deep learning technologies and neural networks which can successfully solve the problem of detection and classification of toxic comments. In addition, the article presents the results of the developed algorithms, as well as the results of the ensemble of all considered algorithms on a large training set collected and tagged by Google and Jigsaw.\n",
            "\n",
            "Title: An efficient approach for textual data classification using deep learning\n",
            "Abstract: Text categorization is an effective activity that can be accomplished using a variety of classification algorithms. In machine learning, the classifier is built by learning the features of categories from a set of preset training data. Similarly, deep learning offers enormous benefits for text classification since they execute highly accurately with lower-level engineering and processing. This paper employs machine and deep learning techniques to classify textual data. Textual data contains much useless information that must be pre-processed. We clean the data, impute missing values, and eliminate the repeated columns. Next, we employ machine learning algorithms: logistic regression, random forest, K-nearest neighbors (KNN), and deep learning algorithms: long short-term memory (LSTM), artificial neural network (ANN), and gated recurrent unit (GRU) for classification. Results reveal that LSTM achieves 92% accuracy outperforming all other model and baseline studies.\n",
            "\n",
            "Title: PROSPECTS OF DATABASES DEVELOPMENT IN THE DIGITAL ECONOMY\n",
            "Abstract: Nowadays each country considers a direction of the digital economy development prioritized. The purpose of the research is to analyze and synthesize the existing prospects of databases development in the digital economy, as well as to identify key technologies and tools that will contribute to this development. Digital technologies are the foundation of a country's prosperity, as they create new opportunities in a rapidly changing world. Today Ukraine's rating on international indices of the global digital competitiveness is unstable. Currently the country is focused on the development of innovations, however, hasn’t used them to improve the economic performance yet. To ensure the operation with databases in modern information systems the database management systems are used. Today there are 378 database management systems in the world. The distinctive feature of the database is that the information within it is necessarily processed by electronic media with a subsequent systematization, which is a subject to certain rules. Designing a database is a complex and time-consuming process. The process of a designing the database means to create a database schema and determine the necessary information integrity constraints. Databases are used as a part of the information system: computer-aided design systems, data banks, information retrieval and expert systems, automated workstations, automated control systems. The process of a proper and timely development of the digital economy can significantly affect the production growth rate, quality of life, level of services. Any process requires special tools, methods and technologies. Regarding tools in the context of the digital economy, such concepts as the Internet of Things, big data, blockchain, neural networks, cloud computing and other tools are generally implied, where the main focus of them is to maximize the digitization of all the information flows for the creation of an information society. It is determined that databases as a powerful component of digital tools allow to combine production with individuality, reduce the development period of innovative ideas till the sale of finished products, provide an opportunity to achieve an effective adaptation to consumer requirements.\n",
            "\n",
            "Title: A Comprehensive Study of Learning Approaches for Author Gender Identification\n",
            "Abstract: In recent years, author gender identification is an important yet challenging task in the fields of information retrieval and computational linguistics. In this paper, different learning approaches are presented to address the problem of author gender identification for Turkish articles. First, several classification algorithms are applied to the list of representations based on different paradigms: fixed-length vector representations such as Stylometric Features (SF), Bag-of-Words (BoW) and distributed word/document embeddings such as Word2vec, fastText and Doc2vec. Secondly, deep learning architectures, Convolution Neural Network (CNN), Recurrent Neural Network (RNN), special kinds of RNN such as Long-Short Term Memory (LSTM) and Gated Recurrent Unit (GRU), C-RNN, Bidirectional LSTM (bi-LSTM), Bidirectional GRU (bi-GRU), Hierarchical Attention Networks and Multi-head Attention (MHA) are designated and their comparable performances are evaluated. We conducted a variety of experiments and achieved outstanding empirical results. To conclude, ML algorithms with BoW have promising results. fast-Text is also probably suitable between embedding models. This comprehensive study contributes to literature utilizing different learning approaches based on several ways of representations. It is also first important attempt to identify author gender applying SF, fastText and DNN architectures to the Turkish language.\n",
            "\n",
            "Title: A Sequential Graph Neural Network for Short Text Classification\n",
            "Abstract: Short text classification is an important problem of natural language processing (NLP), and graph neural networks (GNNs) have been successfully used to solve different NLP problems. However, few studies employ GNN for short text classification, and most of the existing graph-based models ignore sequential information (e.g., word orders) in each document. In this work, we propose an improved sequence-based feature propagation scheme, which fully uses word representation and document-level word interaction and overcomes the limitations of textual features in short texts. On this basis, we utilize this propagation scheme to construct a lightweight model, sequential GNN (SGNN), and its extended model, ESGNN. Specifically, we build individual graphs for each document in the short text corpus based on word co-occurrence and use a bidirectional long short-term memory network (Bi-LSTM) to extract the sequential features of each document; therefore, word nodes in the document graph retain contextual information. Furthermore, two different simplified graph convolutional networks (GCNs) are used to learn word representations based on their local structures. Finally, word nodes combined with sequential information and local information are incorporated as the document representation. Extensive experiments on seven benchmark datasets demonstrate the effectiveness of our method.\n",
            "\n",
            "Title: Using Character-Level and Entity-Level Representations to Enhance Bidirectional Encoder Representation From Transformers-Based Clinical Semantic Textual Similarity Model: ClinicalSTS Modeling Study (Preprint)\n",
            "Abstract: \n",
            " BACKGROUND\n",
            " With the popularity of electronic health records (EHRs), the quality of health care has been improved. However, there are also some problems caused by EHRs, such as the growing use of copy-and-paste and templates, resulting in EHRs of low quality in content. In order to minimize data redundancy in different documents, Harvard Medical School and Mayo Clinic organized a national natural language processing (NLP) clinical challenge (n2c2) on clinical semantic textual similarity (ClinicalSTS) in 2019. The task of this challenge is to compute the semantic similarity among clinical text snippets.\n",
            " \n",
            " \n",
            " OBJECTIVE\n",
            " In this study, we aim to investigate novel methods to model ClinicalSTS and analyze the results.\n",
            " \n",
            " \n",
            " METHODS\n",
            " We propose a semantically enhanced text matching model for the 2019 n2c2/Open Health NLP (OHNLP) challenge on ClinicalSTS. The model includes 3 representation modules to encode clinical text snippet pairs at different levels: (1) character-level representation module based on convolutional neural network (CNN) to tackle the out-of-vocabulary problem in NLP; (2) sentence-level representation module that adopts a pretrained language model bidirectional encoder representation from transformers (BERT) to encode clinical text snippet pairs; and (3) entity-level representation module to model clinical entity information in clinical text snippets. In the case of entity-level representation, we compare 2 methods. One encodes entities by the entity-type label sequence corresponding to text snippet (called entity I), whereas the other encodes entities by their representation in MeSH, a knowledge graph in the medical domain (called entity II).\n",
            " \n",
            " \n",
            " RESULTS\n",
            " We conduct experiments on the ClinicalSTS corpus of the 2019 n2c2/OHNLP challenge for model performance evaluation. The model only using BERT for text snippet pair encoding achieved a Pearson correlation coefficient (PCC) of 0.848. When character-level representation and entity-level representation are individually added into our model, the PCC increased to 0.857 and 0.854 (entity I)/0.859 (entity II), respectively. When both character-level representation and entity-level representation are added into our model, the PCC further increased to 0.861 (entity I) and 0.868 (entity II).\n",
            " \n",
            " \n",
            " CONCLUSIONS\n",
            " Experimental results show that both character-level information and entity-level information can effectively enhance the BERT-based STS model.\n",
            "\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{\n",
            "  \"topic\": \"Text Classification using Deep Learning\",\n",
            "  \"title\": \"Advancements in Deep Learning for Text Classification\",\n",
            "  \"reasoning\": \"The majority of the abstracts discuss the advancements in deep learning techniques for text classification tasks. These techniques include the use of neural networks such as LSTM, GRU, CNN, and GNN, as well as the utilization of different types of representations like word embeddings and character-level representations. The abstracts highlight the improved performance achieved by these deep learning models compared to traditional machine learning algorithms. Therefore, a section titled 'Advancements in Deep Learning for Text Classification' would encompass the main topic discussed in these abstracts.\"\n",
            "}\n",
            "Results for section 2\n",
            "{'topic': 'Text Classification using Deep Learning', 'title': 'Advancements in Deep Learning for Text Classification', 'reasoning': \"The majority of the abstracts discuss the advancements in deep learning techniques for text classification tasks. These techniques include the use of neural networks such as LSTM, GRU, CNN, and GNN, as well as the utilization of different types of representations like word embeddings and character-level representations. The abstracts highlight the improved performance achieved by these deep learning models compared to traditional machine learning algorithms. Therefore, a section titled 'Advancements in Deep Learning for Text Classification' would encompass the main topic discussed in these abstracts.\"}\n",
            "....................................................................................................\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: You are a renowned scientist who is writing a survey on 'text neural information retrieval'. You are currently writing a section about 'text neural information retrieval'\n",
            "Human: I will send you a list of title and abstract of scientific articles. Most of them cover a specific topic about the section 'text neural information retrieval'.\n",
            "Your task is to find out what this topic is and suggest a good title for a section in a scientific survey that addresses it.\n",
            "You should also explain your reasoning.\n",
            "Your answer should be a valid RFC8259 compliant JSON object with three properties:\n",
            "The first property, called \"topic\", describes the main topic that is a subset of 'text neural information retrieval' that are present in most abstracts.\n",
            "The second property, called \"title\", is the title of the section that will cover this topic and must be clearly related to the property \"topic\".\n",
            "The third property is called \"reasoning\" and should contains your reasoning to choose this topic as an answer.\n",
            "Use this format: \n",
            "{\n",
            "  \"topic\": {TOPIC},\n",
            "  \"title\": {TITLE},\n",
            "  \"reasoning\": {REASONING}\n",
            "}\n",
            "\n",
            "Do you understand?\n",
            "\n",
            "AI: Sure, send me the list and I will give you what you need.\n",
            "Human: Title: Fine-grained Interest Matching for Neural News Recommendation\n",
            "Abstract: Personalized news recommendation is a critical technology to improve users’ online news reading experience. The core of news recommendation is accurate matching between user’s interests and candidate news. The same user usually has diverse interests that are reflected in different news she has browsed. Meanwhile, important semantic features of news are implied in text segments of different granularities. Existing studies generally represent each user as a single vector and then match the candidate news vector, which may lose fine-grained information for recommendation. In this paper, we propose FIM, a Fine-grained Interest Matching method for neural news recommendation. Instead of aggregating user’s all historical browsed news into a unified vector, we hierarchically construct multi-level representations for each news via stacked dilated convolutions. Then we perform fine-grained matching between segment pairs of each browsed news and the candidate news at each semantic level. High-order salient signals are then identified by resembling the hierarchy of image recognition for final click prediction. Extensive experiments on a real-world dataset from MSN news validate the effectiveness of our model on news recommendation.\n",
            "\n",
            "Title: Interpreting Node Embedding with Text-labeled Graphs\n",
            "Abstract: Graph neural networks have recently received increasing attention. These methods often map nodes into latent spaces and learn vector representations of the nodes for a variety of downstream tasks. To gain trust and to promote collaboration between AIs and humans, it would be better if those representations were interpretable for humans. However, most explainable AIs focus on a supervised learning setting and aim to answer the following question: “Why does the model predict y for an input x?”. For an unsupervised learning setting as node embedding, interpretation can be more complicated since the embedding vectors are usually not understandable for humans. On the other hand, nodes and edges in a graph are often associated with texts in many real-world applications. A question naturally arises: could we integrate the human-understandable textural data into graph learning to facilitate interpretable node embedding? In this paper we present interpretable graph neural networks (iGNN), a model to learn textual explanations for node representations modeling the extra information contained in the associated textual data. To validate the performance of the proposed method, we investigate the learned interpretability of the embedding vectors and use functional interpretability to measure it. Experimental results on multiple text-labeled graphs show the effectiveness of the iGNN model on learning textual explanations of node embedding while performing well in downstream tasks.\n",
            "\n",
            "Title: CommAID: Visual Analytics for Communication Analysis through Interactive Dynamics Modeling\n",
            "Abstract: Communication consists of both meta‐information as well as content. Currently, the automated analysis of such data often focuses either on the network aspects via social network analysis or on the content, utilizing methods from text‐mining. However, the first category of approaches does not leverage the rich content information, while the latter ignores the conversation environment and the temporal evolution, as evident in the meta‐information. In contradiction to communication research, which stresses the importance of a holistic approach, both aspects are rarely applied simultaneously, and consequently, their combination has not yet received enough attention in automated analysis systems. In this work, we aim to address this challenge by discussing the difficulties and design decisions of such a path as well as contribute CommAID, a blueprint for a holistic strategy to communication analysis. It features an integrated visual analytics design to analyze communication networks through dynamics modeling, semantic pattern retrieval, and a user‐adaptable and problem‐specific machine learning‐based retrieval system. An interactive multi‐level matrix‐based visualization facilitates a focused analysis of both network and content using inline visuals supporting cross‐checks and reducing context switches. We evaluate our approach in both a case study and through formative evaluation with eight law enforcement experts using a real‐world communication corpus. Results show that our solution surpasses existing techniques in terms of integration level and applicability. With this contribution, we aim to pave the path for a more holistic approach to communication analysis.\n",
            "\n",
            "Title: Joint Generative Image Deblurring Aided by Edge Attention Prior and Dynamic Kernel Selection\n",
            "Abstract: Image deblurring is a classic and important problem in industrial fields, such as aviation photo restoration, object recognition in robotics, and autonomous vehicles. Blurry images in real-world scenarios consist of mixed blurring types, such as a natural motion blurring owing to shaking of the camera. Fast deblurring does not deblur the entire image because it is not the best option. Considering the computational costs, it is also better to have an alternative kernel to deblur different objects at a high-semantic level. To achieve better image restoration quality, it is also beneficial to combine the blurring category location and important structural information in terms of specific artifacts and degree of blurring. The goal of blind image deblurring is to restore sharpness from the unknown blurring kernel of an image. Recent deblurring methods tend to reconstruct prior knowledge, neglecting the influence of blur estimation and visual fidelity on image details and structure. Generative adversarial networks(GANs) have recently been attracting considerable attention from both academia and industry because GAN can perfectly generate new data with the same statistics as the training set. Therefore, this study proposes a generative neural architecture and an edge attention algorithm developed to restore vivid multimedia patches. Joint edge generation and image restoration techniques are designed to solve the low-level multimedia retrieval. This multipath refinement fusion network (MRFNet) can not only perform deblurring of images directly but also individual the frames separately from videos. Ablation experiments validate that our generative adversarial network MRFNet performs better in joint training than in multimodel. Compared to other GAN methods, our two-phase method exhibited state-ofthe-art performance in terms of speed and accuracy as well as has a significant visual improvement.\n",
            "\n",
            "Title: Analysis of Internet Public Opinion Popularity Trend Based on a Deep Neural Network\n",
            "Abstract: In the context dominated by Internet communication, people's various emotions can be clearly reflected through network public opinion, whether it is the view of political affairs, the preference for entertainment, or the demand for life. This also allows management or providers to meet their needs more specifically. Based on today's need to understand the trend of Internet public opinion, this paper describes a deep neural network (DNN). A deep neural network is a machine learning model that is the foundation of deep learning and has a strong ability to mine potential information in data. By improving the loss function of the neural network, this paper reduces the influence of unbalanced data on the classification results and improves the classification effect of the model on a small number of categories. Aiming at the different lengths of Internet text, a more robust model of text sentiment classification is proposed, which makes the HCB-Att model better extract the local information and contextual information of the text. Finally, through comparative experiments, the optimization model used in this paper is proved to be effective for the analysis of network public opinion sentiment.\n",
            "\n",
            "Title: Symmetric Network with Spatial Relationship Modeling for Natural Language-based Vehicle Retrieval\n",
            "Abstract: Natural language (NL) based vehicle retrieval aims to search specific vehicle given text description. Different from the image-based vehicle retrieval, NL-based vehicle retrieval requires considering not only vehicle appearance, but also surrounding environment and temporal relations. In this paper, we propose a Symmetric Network with Spatial Relationship Modeling (SSM) method for NL-based vehicle retrieval. Specifically, we design a symmetric network to learn the unified cross-modal representations between text descriptions and vehicle images, where vehicle appearance details and vehicle trajectory global information are pre-served. Besides, to make better use of location information, we propose a spatial relationship modeling methods to take surrounding environment and mutual relationship between vehicles into consideration. The qualitative and quantitative experiments verify the effectiveness of the proposed method. We achieve 43.92% MRR accuracy on the test set of the 6th AI City Challenge on natural language-based vehicle retrieval track, yielding the 4th place on the public leaderboard. The code will be available at https://github.com/hbchen121/AICITY2022_Track2_SSM.\n",
            "\n",
            "Title: Variational neural decoder for abstractive text summarization\n",
            "Abstract: In the conventional sequence-to-sequence (seq2seq) model for abstractive summarization, the internal transformation structure of recurrent neural networks (RNNs) is completely determined. Therefore, the learned semantic information is far from enough to represent all semantic details and context dependencies, resulting in a redundant summary and poor consistency. In this paper, we propose a variational neural decoder text summarization model (VND). The model introduces a series of implicit variables by combining variational RNN and variational autoencoder, which is used to capture complex semantic representation at each step of decoding. It includes a standard RNN layer and a variational RNN layer [5]. These two network layers respectively generate a deterministic hidden state and a random hidden state. We use these two RNN layers to establish the dependence between implicit variables between adjacent time steps. In this way, the model structure can better capture the complex semantics and the strong dependence between the adjacent time steps when outputting the summary, thereby improving the performance of generating the summary. The experimental results show that, on the text summary LCSTS and English Gigaword dataset, our model has a significant improvement over the baseline model.\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{\n",
            "  \"topic\": \"Text-based Neural Information Retrieval\",\n",
            "  \"title\": \"Advancements in Text-based Neural Information Retrieval\",\n",
            "  \"reasoning\": \"The majority of the abstracts focus on utilizing neural networks and deep learning techniques to improve information retrieval from text data. These techniques include fine-grained interest matching for news recommendation, interpreting node embedding with text-labeled graphs, visual analytics for communication analysis, joint generative image deblurring aided by edge attention prior and dynamic kernel selection, analysis of internet public opinion popularity trend, and natural language-based vehicle retrieval. Therefore, the main topic that emerges from these abstracts is 'Text-based Neural Information Retrieval'. The suggested title 'Advancements in Text-based Neural Information Retrieval' accurately reflects the focus of the abstracts and highlights the advancements made in this field.\"\n",
            "}\n",
            "Results for section 3\n",
            "{'topic': 'Text-based Neural Information Retrieval', 'title': 'Advancements in Text-based Neural Information Retrieval', 'reasoning': \"The majority of the abstracts focus on utilizing neural networks and deep learning techniques to improve information retrieval from text data. These techniques include fine-grained interest matching for news recommendation, interpreting node embedding with text-labeled graphs, visual analytics for communication analysis, joint generative image deblurring aided by edge attention prior and dynamic kernel selection, analysis of internet public opinion popularity trend, and natural language-based vehicle retrieval. Therefore, the main topic that emerges from these abstracts is 'Text-based Neural Information Retrieval'. The suggested title 'Advancements in Text-based Neural Information Retrieval' accurately reflects the focus of the abstracts and highlights the advancements made in this field.\"}\n",
            "....................................................................................................\n",
            "Extracting subsections...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fe0ba0cd32174fc5a6faba6b5e981060"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster 1: 9\n",
            "Cluster 2: 10\n",
            "Cluster 0: 31\n",
            "Extracting 3 sections in this level:\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: You are a renowned scientist who is writing a survey on 'text neural information retrieval'. You are currently writing a section about 'Advancements in Text-based Neural Information Retrieval'\n",
            "Human: I will send you a list of title and abstract of scientific articles. Most of them cover a specific topic about the section 'Advancements in Text-based Neural Information Retrieval'.\n",
            "Your task is to find out what this topic is and suggest a good title for a section in a scientific survey that addresses it.\n",
            "You should also explain your reasoning.\n",
            "Your answer should be a valid RFC8259 compliant JSON object with three properties:\n",
            "The first property, called \"topic\", describes the main topic that is a subset of 'Advancements in Text-based Neural Information Retrieval' that are present in most abstracts.\n",
            "The second property, called \"title\", is the title of the section that will cover this topic and must be clearly related to the property \"topic\".\n",
            "The third property is called \"reasoning\" and should contains your reasoning to choose this topic as an answer.\n",
            "Use this format: \n",
            "{\n",
            "  \"topic\": {TOPIC},\n",
            "  \"title\": {TITLE},\n",
            "  \"reasoning\": {REASONING}\n",
            "}\n",
            "\n",
            "Do you understand?\n",
            "\n",
            "AI: Sure, send me the list and I will give you what you need.\n",
            "Human: Title: KW-ATTN: Knowledge Infused Attention for Accurate and Interpretable Text Classification\n",
            "Abstract: Text classification has wide-ranging applications in various domains. While neural network approaches have drastically advanced performance in text classification, they tend to be powered by a large amount of training data, and interpretability is often an issue. As a step towards better accuracy and interpretability especially on small data, in this paper we present a new knowledge-infused attention mechanism, called KW-ATTN (KnoWledge-infused ATTentioN) to incorporate high-level concepts from external knowledge bases into Neural Network models. We show that KW-ATTN outperforms baseline models using only words as well as other approaches using concepts by classification accuracy, which indicates that high-level concepts help model prediction. Furthermore, crowdsourced human evaluation suggests that additional concept information helps interpretability of the model.\n",
            "\n",
            "Title: Signal Phrase Extraction: A Gateway to Information Retrieval Improvement in Law Texts\n",
            "Abstract: NLP-based techniques can support in improving understanding of legal text documents. In this work we present a semi-automatic framework to extract signal phrases from legislative texts for an arbitrary European language. Through a case study using Dutch legislation, we demonstrate that it is feasible to extract these phrases reliably with a small number of supporting domain experts. Finally, we argue how in future works our framework could be utilized with existing methods to be applied to different languages.\n",
            "\n",
            "Title: An Effective Hybrid Deep Neural Network for Arabic Fake News Detection\n",
            "Abstract: Recently, the phenomenon of the spread of fake news or misinformation in most fields has taken on a wide resonance in societies. Combating this phenomenon and detecting misleading information manually is rather boring, takes a long time, and impractical. It is therefore necessary to rely on the fields of artificial intelligence to solve this problem. As such, this study aims to use deep learning techniques to detect Arabic fake news based on Arabic dataset called the AraNews dataset. This dataset contains news articles covering multiple fields such as politics, economy, culture, sports and others. A Hybrid Deep Neural Network has been proposed to improve accuracy. This network focuses on the properties of both the Text-Convolution Neural Network (Text-CNN) and Long Short-Term Memory (LSTM) architecture to produce efficient hybrid model. Text-CNN is used to identify the relevant features, whereas the LSTM is applied to deal with the long-term dependency of sequence. The results showed that when trained individually, the proposed model outperformed both the Text-CNN and the LSTM. Accuracy was used as a measure of model quality, whereby the accuracy of the Hybrid Deep Neural Network is (0.914), while the accuracy of both Text-CNN and LSTM is (0.859) and (0.878), respectively. Moreover, the results of our proposed model are better compared to previous work that used the same dataset (AraNews dataset).\n",
            "\n",
            "Title: PENELOPIE: Enabling Open Information Extraction for the Greek Language through Machine Translation\n",
            "Abstract: In this work, we present a methodology that aims at bridging the gap between high and low-resource languages in the context of Open Information Extraction, showcasing it on the Greek language. The goals of this paper are twofold: First, we build Neural Machine Translation (NMT) models for English-to-Greek and Greek-to-English based on the Transformer architecture. Second, we leverage these NMT models to produce English translations of Greek text as input for our NLP pipeline, to which we apply a series of pre-processing and triple extraction tasks. Finally, we back-translate the extracted triples to Greek. We conduct an evaluation of both our NMT and OIE methods on benchmark datasets and demonstrate that our approach outperforms the current state-of-the-art for the Greek natural language.\n",
            "\n",
            "Title: Multimodal Deep Learning Models for Detecting Dementia From Speech and Transcripts\n",
            "Abstract: Alzheimer's dementia (AD) entails negative psychological, social, and economic consequences not only for the patients but also for their families, relatives, and society in general. Despite the significance of this phenomenon and the importance for an early diagnosis, there are still limitations. Specifically, the main limitation is pertinent to the way the modalities of speech and transcripts are combined in a single neural network. Existing research works add/concatenate the image and text representations, employ majority voting approaches or average the predictions after training many textual and speech models separately. To address these limitations, in this article we present some new methods to detect AD patients and predict the Mini-Mental State Examination (MMSE) scores in an end-to-end trainable manner consisting of a combination of BERT, Vision Transformer, Co-Attention, Multimodal Shifting Gate, and a variant of the self-attention mechanism. Specifically, we convert audio to Log-Mel spectrograms, their delta, and delta-delta (acceleration values). First, we pass each transcript and image through a BERT model and Vision Transformer, respectively, adding a co-attention layer at the top, which generates image and word attention simultaneously. Secondly, we propose an architecture, which integrates multimodal information to a BERT model via a Multimodal Shifting Gate. Finally, we introduce an approach to capture both the inter- and intra-modal interactions by concatenating the textual and visual representations and utilizing a self-attention mechanism, which includes a gate model. Experiments conducted on the ADReSS Challenge dataset indicate that our introduced models demonstrate valuable advantages over existing research initiatives achieving competitive results in both the AD classification and MMSE regression task. Specifically, our best performing model attains an accuracy of 90.00% and a Root Mean Squared Error (RMSE) of 3.61 in the AD classification task and MMSE regression task, respectively, achieving a new state-of-the-art performance in the MMSE regression task.\n",
            "\n",
            "Title: 3D Model Classification Based on Bayesian Classifier with AdaBoost\n",
            "Abstract: With the exponential increasement of 3D models, 3D model classification is crucial to the effective management and retrieval of model database. Feature descriptor has important influence on 3D model classification. Voxel descriptor expresses surface and internal information of 3D model. However, it does not contain topological structure information. Shape distribution descriptor expresses geometry relationship of random points on model surface and has rotation invariance. They can all be used to classify 3D models, but accuracy is low due to insufficient description of 3D model. This paper proposes a 3D model classification algorithm that fuses voxel descriptor and shape distribution descriptor. 3D convolutional neural network (CNN) is used to extract voxel features, and 1D CNN is adopted to extract shape distribution features. AdaBoost algorithm is applied to combine several Bayesian classifiers to get a strong classifier for classifying 3D models. Experiments are conducted on ModelNet10, and results show that accuracy of the proposed method is improved.\n",
            "\n",
            "Title: Distilling Knowledge from Well-Informed Soft Labels for Neural Relation Extraction\n",
            "Abstract: Extracting relations from plain text is an important task with wide application. Most existing methods formulate it as a supervised problem and utilize one-hot hard labels as the sole target in training, neglecting the rich semantic information among relations. In this paper, we aim to explore the supervision with soft labels in relation extraction, which makes it possible to integrate prior knowledge. Specifically, a bipartite graph is first devised to discover type constraints between entities and relations based on the entire corpus. Then, we combine such type constraints with neural networks to achieve a knowledgeable model. Furthermore, this model is regarded as teacher to generate well-informed soft labels and guide the optimization of a student network via knowledge distillation. Besides, a multi-aspect attention mechanism is introduced to help student mine latent information from text. In this way, the enhanced student inherits the dark knowledge (e.g., type constraints and relevance among relations) from teacher, and directly serves the testing scenarios without any extra constraints. We conduct extensive experiments on the TACRED and SemEval datasets, the experimental results justify the effectiveness of our approach.\n",
            "\n",
            "Title: DEVELOPMENT OF A MATHEMATICAL MODEL FOR A NEURAL NETWORK FOR RECOGNIZING TEXT INFORMATION\n",
            "Abstract: The article considers a mathematical model of a system that provides recognition of images that represent text or use similar information in the generation process.\n",
            "\n",
            "Title: Linguistic knowledge-based vocabularies for Neural Machine Translation\n",
            "Abstract: Abstract Neural Networks applied to Machine Translation need a finite vocabulary to express textual information as a sequence of discrete tokens. The currently dominant subword vocabularies exploit statistically-discovered common parts of words to achieve the flexibility of character-based vocabularies without delegating the whole learning of word formation to the neural network. However, they trade this for the inability to apply word-level token associations, which limits their use in semantically-rich areas and prevents some transfer learning approaches e.g. cross-lingual pretrained embeddings, and reduces their interpretability. In this work, we propose new hybrid linguistically-grounded vocabulary definition strategies that keep both the advantages of subword vocabularies and the word-level associations, enabling neural networks to profit from the derived benefits. We test the proposed approaches in both morphologically rich and poor languages, showing that, for the former, the quality in the translation of out-of-domain texts is improved with respect to a strong subword baseline.\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{\n",
            "  \"topic\": \"Knowledge-infused Attention Mechanism for Text Classification\",\n",
            "  \"title\": \"Advancements in Knowledge-infused Attention Mechanism for Text Classification\",\n",
            "  \"reasoning\": \"The abstract of the first article highlights the development of a knowledge-infused attention mechanism called KW-ATTN for text classification. This mechanism incorporates high-level concepts from external knowledge bases into neural network models, resulting in improved accuracy and interpretability. This topic is a significant advancement in text-based neural information retrieval as it addresses the challenges of accuracy and interpretability in text classification tasks.\"\n",
            "}\n",
            "Results for section 0\n",
            "{'topic': 'Knowledge-infused Attention Mechanism for Text Classification', 'title': 'Advancements in Knowledge-infused Attention Mechanism for Text Classification', 'reasoning': 'The abstract of the first article highlights the development of a knowledge-infused attention mechanism called KW-ATTN for text classification. This mechanism incorporates high-level concepts from external knowledge bases into neural network models, resulting in improved accuracy and interpretability. This topic is a significant advancement in text-based neural information retrieval as it addresses the challenges of accuracy and interpretability in text classification tasks.'}\n",
            "....................................................................................................\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: You are a renowned scientist who is writing a survey on 'text neural information retrieval'. You are currently writing a section about 'Advancements in Text-based Neural Information Retrieval'\n",
            "Human: I will send you a list of title and abstract of scientific articles. Most of them cover a specific topic about the section 'Advancements in Text-based Neural Information Retrieval'.\n",
            "Your task is to find out what this topic is and suggest a good title for a section in a scientific survey that addresses it.\n",
            "You should also explain your reasoning.\n",
            "Your answer should be a valid RFC8259 compliant JSON object with three properties:\n",
            "The first property, called \"topic\", describes the main topic that is a subset of 'Advancements in Text-based Neural Information Retrieval' that are present in most abstracts.\n",
            "The second property, called \"title\", is the title of the section that will cover this topic and must be clearly related to the property \"topic\".\n",
            "The third property is called \"reasoning\" and should contains your reasoning to choose this topic as an answer.\n",
            "Use this format: \n",
            "{\n",
            "  \"topic\": {TOPIC},\n",
            "  \"title\": {TITLE},\n",
            "  \"reasoning\": {REASONING}\n",
            "}\n",
            "\n",
            "Do you understand?\n",
            "\n",
            "AI: Sure, send me the list and I will give you what you need.\n",
            "Human: Title: An efficient approach for textual data classification using deep learning\n",
            "Abstract: Text categorization is an effective activity that can be accomplished using a variety of classification algorithms. In machine learning, the classifier is built by learning the features of categories from a set of preset training data. Similarly, deep learning offers enormous benefits for text classification since they execute highly accurately with lower-level engineering and processing. This paper employs machine and deep learning techniques to classify textual data. Textual data contains much useless information that must be pre-processed. We clean the data, impute missing values, and eliminate the repeated columns. Next, we employ machine learning algorithms: logistic regression, random forest, K-nearest neighbors (KNN), and deep learning algorithms: long short-term memory (LSTM), artificial neural network (ANN), and gated recurrent unit (GRU) for classification. Results reveal that LSTM achieves 92% accuracy outperforming all other model and baseline studies.\n",
            "\n",
            "Title: Incorporating Code Structure and Quality in Deep Code Search\n",
            "Abstract: Developers usually search for reusable code snippets to improve software development efficiency. Existing code search methods, including methods based on full-text or deep learning, have two disadvantages: (1) ignoring structural information of code snippets, such as conditional statements and loop statements, and (2) ignoring quality information of code snippets, such as naming clarity and logical correctness. These disadvantages limit the performance of existing code search methods. In this paper, we propose a novel code search method named Structure and Quality based Deep Code Search (SQ-DeepCS). SQ-DeepCS introduces a code representation method called program slice to represent structual information as well as API usage of code snippets. Meanwhile, SQ-DeepCS introduces a novel deep neural network named Method-Description-Joint Embedding Neural Network (MD-JEnn) to weight the quality of code snippets. To evaluate the proposed methods, we train MD-JEnn and evaluate SQ-DeepCS by searching for code snippets with respect to the top-rated questions from Stack Overflow. We use four evaluation indicators to measure the effectiveness of SQ-DeepCS: FRank, SuccessRate@k, PrecisionRate@k, and Mean Reciprocal Rank (MRR). The experimental results show that our approach can provide better results than existing techniques when searching for relevant code snippets.\n",
            "\n",
            "Title: PROSPECTS OF DATABASES DEVELOPMENT IN THE DIGITAL ECONOMY\n",
            "Abstract: Nowadays each country considers a direction of the digital economy development prioritized. The purpose of the research is to analyze and synthesize the existing prospects of databases development in the digital economy, as well as to identify key technologies and tools that will contribute to this development. Digital technologies are the foundation of a country's prosperity, as they create new opportunities in a rapidly changing world. Today Ukraine's rating on international indices of the global digital competitiveness is unstable. Currently the country is focused on the development of innovations, however, hasn’t used them to improve the economic performance yet. To ensure the operation with databases in modern information systems the database management systems are used. Today there are 378 database management systems in the world. The distinctive feature of the database is that the information within it is necessarily processed by electronic media with a subsequent systematization, which is a subject to certain rules. Designing a database is a complex and time-consuming process. The process of a designing the database means to create a database schema and determine the necessary information integrity constraints. Databases are used as a part of the information system: computer-aided design systems, data banks, information retrieval and expert systems, automated workstations, automated control systems. The process of a proper and timely development of the digital economy can significantly affect the production growth rate, quality of life, level of services. Any process requires special tools, methods and technologies. Regarding tools in the context of the digital economy, such concepts as the Internet of Things, big data, blockchain, neural networks, cloud computing and other tools are generally implied, where the main focus of them is to maximize the digitization of all the information flows for the creation of an information society. It is determined that databases as a powerful component of digital tools allow to combine production with individuality, reduce the development period of innovative ideas till the sale of finished products, provide an opportunity to achieve an effective adaptation to consumer requirements.\n",
            "\n",
            "Title: Aesthetic and Implication Analysis of the Traditional Poetic Environment Based on Natural Language Emotion Analysis\n",
            "Abstract: The uniqueness of aesthetic implication in Zhou Dynasty poetics lies in that it is the basic forming stage of the concept of formal beauty of the whole Chinese nation, and the aesthetic implication of the Zhou Dynasty poetics art has fundamental significance for the whole ancient Chinese aesthetic implication theory. In the discipline of natural language processing, text emotion analysis is a crucial topic. Artificial neural network research is where the idea of “deep learning” (DL) first emerged. In view of the problems that semantic information is easy to be lost and emotional information may be ignored in the traditional Chinese short text emotion analysis model, this paper introduces the AM (attention mechanism) and proposes a CNN-LSTM (convolutional neural network-long short-term memory) poetic aesthetic implication analysis method based on self-attention. For the IL (input layer), word vectors trained by Word2Vec are used and then input into the CNN-LSTM joint model. Then, the output of the joint model is weighted and summed by self-attention and finally input into the Softmax classifier, so as to realize the emotion classification of the text. By creating and putting into practise pertinent comparative experiments, the usefulness of the proposed model is confirmed. The outcomes demonstrate that this model outperforms the other three comparison models for the quantification of evaluation indices in terms of overall performance. The accuracy and F1 of this paper are 93.362% and 90.886%, respectively, which are higher than other models.\n",
            "\n",
            "Title: Sentiment Polarity Identification of Social Media content using Artificial Neural Networks\n",
            "Abstract: Sentiment of people about consumer goods and government policies for decision making is normally collected through feedback forms, surveys etc. The social network sites and micro blogging sites are considered a very good source of information nowadays because people share and discuss their opinions about a certain topic freely. With the increased use of technology and social media, people proactively express their opinion through social media sites like Twitter, Facebook, Instagram etc. A social media sentiment analysis can help companies to understand how people feel about their products. On the other hand, extracting the sentiment from social media text is a challenging task due to the complexity of natural language processing of social media language. Often these messages reflect the emotion, opinion and sentiment of the public through a mix of text, image, emoticons etc. These statements are often called electronic Word of Mouth (eWOM) and are much prevalent in business and service industry to enable customers to share their point of view.\n",
            "\n",
            "Title: The Limitations of Stylometry for Detecting Machine-Generated Fake News\n",
            "Abstract: Recent developments in neural language models (LMs) have raised concerns about their potential misuse for automatically spreading misinformation. In light of these concerns, several studies have proposed to detect machine-generated fake news by capturing their stylistic differences from human-written text. These approaches, broadly termed stylometry, have found success in source attribution and misinformation detection in human-written texts. However, in this work, we show that stylometry is limited against machine-generated misinformation. Whereas humans speak differently when trying to deceive, LMs generate stylistically consistent text, regardless of underlying motive. Thus, though stylometry can successfully prevent impersonation by identifying text provenance, it fails to distinguish legitimate LM applications from those that introduce false information. We create two benchmarks demonstrating the stylistic similarity between malicious and legitimate uses of LMs, utilized in auto-completion and editing-assistance settings.1 Our findings highlight the need for non-stylometry approaches in detecting machine-generated misinformation, and open up the discussion on the desired evaluation benchmarks.\n",
            "\n",
            "Title: Tracking Your Mind's Eye during Recollection: Decoding the Long-Term Recall of Short Audiovisual Clips\n",
            "Abstract: Unlike familiarity, recollection involves the ability to reconstruct mentally previous events that results in a strong sense of reliving. According to the reinstatement hypothesis, this specific feature emerges from the reactivation of cortical patterns involved during information exposure. Over time, the retrieval of specific details becomes more difficult, and memories become increasingly supported by familiarity judgments. The multiple trace theory (MTT) explains the gradual loss of episodic details by a transformation in the memory representation, a view that is not shared by the standard consolidation model. In this study, we tested the MTT in light of the reinstatement hypothesis. The temporal dynamics of mental imagery from long-term memory were investigated and tracked over the passage of time. Participant EEG activity was recorded during the recall of short audiovisual clips that had been watched 3 weeks, 1 day, or a few hours beforehand. The recall of the audiovisual clips was assessed using a Remember/Know/New procedure, and snapshots of clips were used as recall cues. The decoding matrices obtained from the multivariate pattern analyses revealed sustained patterns that occurred at long latencies (>500 msec poststimulus onset) that faded away over the retention intervals and that emerged from the same neural processes. Overall, our data provide further evidence toward the MTT and give new insights into the exploration of our “mind's eye.”\n",
            "\n",
            "Title: Generalized context-dependent graph-theoretic model of folklore and literary texts\n",
            "Abstract: One of the problems of automatic text processing is their attribution. This term is understood as the establishment of the attributes of a text work (determination of authorship, time of creation, place of recording, etc.). The article presents a generalized context-dependent graph-theoretic model designed for the analysis of folklore and literary texts. The minimal structural unit of the model (primitive) is a word. Sets of words are combined into vertices, and the same word can be related to different vertices. Edges and graph substructures reflect the lexical, syntactic and semantic links of the text. The characteristics of the model are its fuzziness, hierarchy and temporality. As examples, a hierarchical graph-theoretical model of components (on the example of literary works by A. S. Pushkin), a temporal graph-theoretic model of a fairy tale plot (on the example of Russian fairy tales by A. M. Afanasyev) and a fuzzy graph-theoretic model of «strong» connections of grammatical classes (on the example of anonymous articles from the pre-revolutionary magazines «Time», «Epoch» and the weekly «Citizen», edited by F. M. Dostoevsky). The model is built in such a way that it can be further explored using artificial intelligence methods (for example, decision trees or neural networks). For this purpose, a format for storing such data was implemented in the information system «Folklore», as well as procedures for entering, editing and analyzing texts and their graph-theoretic models.\n",
            "\n",
            "Title: Successive Image Generation from a Single Sentence\n",
            "Abstract: Through various examples in history such as the early man’s carving on caves, dependence on diagrammatic representations, the immense popularity of comic books we have seen that vision has a higher reach in communication than written words. In this paper, we analyse and propose a new task of transfer of information from text to image synthesis. Through this paper we aim to generate a story from a single sentence and convert our generated story into a sequence of images. We plan to use state of the art technology to implement this task. With the advent of Generative Adversarial Networks text to image synthesis have found a new awakening. We plan to take this task a step further, in order to automate the entire process. Our system generates a multi-lined story given a single sentence using a deep neural network. This story is then fed into our networks of multiple stage GANs inorder to produce a photorealistic image sequence.\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{\n",
            "  \"topic\": \"Text-to-Image Synthesis\",\n",
            "  \"title\": \"Advancements in Text-to-Image Synthesis: Generating Images from Text\",\n",
            "  \"reasoning\": \"Based on the abstract, several articles discuss the task of generating images from text using deep learning techniques. These articles explore different approaches, such as using generative adversarial networks (GANs) and deep neural networks, to convert textual information into visual representations. The focus is on advancing the field of text-to-image synthesis and improving the quality and realism of the generated images.\"\n",
            "}\n",
            "Results for section 1\n",
            "{'topic': 'Text-to-Image Synthesis', 'title': 'Advancements in Text-to-Image Synthesis: Generating Images from Text', 'reasoning': 'Based on the abstract, several articles discuss the task of generating images from text using deep learning techniques. These articles explore different approaches, such as using generative adversarial networks (GANs) and deep neural networks, to convert textual information into visual representations. The focus is on advancing the field of text-to-image synthesis and improving the quality and realism of the generated images.'}\n",
            "....................................................................................................\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: You are a renowned scientist who is writing a survey on 'text neural information retrieval'. You are currently writing a section about 'Advancements in Text-based Neural Information Retrieval'\n",
            "Human: I will send you a list of title and abstract of scientific articles. Most of them cover a specific topic about the section 'Advancements in Text-based Neural Information Retrieval'.\n",
            "Your task is to find out what this topic is and suggest a good title for a section in a scientific survey that addresses it.\n",
            "You should also explain your reasoning.\n",
            "Your answer should be a valid RFC8259 compliant JSON object with three properties:\n",
            "The first property, called \"topic\", describes the main topic that is a subset of 'Advancements in Text-based Neural Information Retrieval' that are present in most abstracts.\n",
            "The second property, called \"title\", is the title of the section that will cover this topic and must be clearly related to the property \"topic\".\n",
            "The third property is called \"reasoning\" and should contains your reasoning to choose this topic as an answer.\n",
            "Use this format: \n",
            "{\n",
            "  \"topic\": {TOPIC},\n",
            "  \"title\": {TITLE},\n",
            "  \"reasoning\": {REASONING}\n",
            "}\n",
            "\n",
            "Do you understand?\n",
            "\n",
            "AI: Sure, send me the list and I will give you what you need.\n",
            "Human: Title: TripClick: The Log Files of a Large Health Web Search Engine\n",
            "Abstract: Click logs are valuable resources for a variety of information retrieval (IR) tasks. This includes query understanding/analysis, as well as learning effective IR models particularly when the models require large amounts of training data. We release a large-scale domain-specific dataset of click logs, obtained from user interactions of the Trip Database health web search engine. Our click log dataset comprises approximately 5.2 million user interactions collected between 2013 and 2020. We use this dataset to create a standard IR evaluation benchmark - TripClick - with around 700,000 unique free-text queries and 1.3 million pairs of query-document relevance signals, whose relevance is estimated by two click-through models. As such, the collection is one of the few datasets offering the necessary data richness and scale to train neural IR models with a large amount of parameters, and notably the first in the health domain. Using TripClick, we conduct experiments to evaluate a variety of IR models, showing the benefits of exploiting this data to train neural architectures. In particular, the evaluation results show that the best performing neural IR model significantly improves the performance by a large margin relative to classical IR models, especially for more frequent queries.\n",
            "\n",
            "Title: An Overview of Ontology Learning Process from Arabic Text\n",
            "Abstract: Ontology learning (OL) Plays an important role in many fields such as semantic web, Data Integration and Interoperability, Machine Translation, and Information Retrieval. The success of any of these fields depends on the quality of its ontologies. This paper presents a comprehensive survey of research on the Ontology Learning for Arabic texts. Most of the previous works focused on three main issues: extracting the terms, extracting the semantic relations, and building the ontology from the Arabic text. There are more of techniques and methods can be used for this process. In this paper, first we present the Arabic challenges that were reasons for developing few Arabic Ontology Learning systems. Second we make a research comparison based on the techniques used and their results. Third, we pointed limitations and comments of research works on Arabic Ontology Learning. Finally, we concluded the paper and outlined our future research direction in this area.\n",
            "\n",
            "Title: Augmentation of Contextualized Concatenated Word Representation and Dilated Convolution Neural Network for Sentiment Analysis\n",
            "Abstract: Deep learning-based methodologies are significant to perform sentiment analysis on social media data. The valuable insights of social media data through sentiment analysis can be employed to develop intelligent applications. Among many networks, convolution neural networks (CNNs) are widely used in many conventional text classification tasks and perform a significant role. However, to capture long-term contextual information and address the detail loss problem, CNNs require stacking multiple convolutional layers. Also, the stacking of convolutional layers has issues requiring massive computations and the tuning of additional parameters. To solve these problems, in this paper, a contextualized concatenated word representation (CCWRs) is initialized from social media data based on text which is essential to misspelled and out of vocabulary words (OOV). In CCWRs, different word representation models, for example, Word2Vec, its optimized version FastText and Global Vectors, and GloVe, collectively create contextualized representations upon the sequence of input. Second, a three-layered dilated convolutional neural network (3D-CNN) is proposed that places dilated convolution kernels instead of conventional CNN kernels. Incorporating the extension in the receptive field’s size successfully solves the detail loss problem and achieves long-term context information with different dilation rates. Experiments on datasets demonstrate that the proposed framework achieves reliable results with the selection of numerous hyperparameter tuning and configurations for improved optimization leads to reduced computational resources and reliable accuracy.\n",
            "\n",
            "Title: Classification of Problem and Solution Strings in Scientific Texts: Evaluation of the Effectiveness of Machine Learning Classifiers and Deep Neural Networks\n",
            "Abstract: One of the central aspects of science is systematic problem-solving. Therefore, problem and solution statements are an integral component of the scientific discourse. The scientific analysis would be more successful if the problem–solution claims in scientific texts were automatically classified. It would help in knowledge mining, idea generation, and information classification from scientific texts. It would also help to compare scientific papers and automatically generate review articles in a given field. However, computational research on problem–solution patterns has been scarce. The linguistic analysis, instructional-design research, theory, and empirical methods have not paid enough attention to the study of problem–solution patterns. This paper tries to solve this issue by applying the computational techniques of machine learning classifiers and neural networks to a set of features to intelligently classify a problem phrase from a non-problem phrase and a solution phrase from a non-solution phrase. Our analysis shows that deep learning networks outperform machine learning classifiers. Our best model was able to classify a problem phrase from a non-problem phrase with an accuracy of 90.0% and a solution phrase from a non-solution phrase with an accuracy of 86.0%.\n",
            "\n",
            "Title: (Quasi-)Real-Time Inversion of Airborne Time-Domain Electromagnetic Data via Artificial Neural Network\n",
            "Abstract: The possibility to have results very quickly after, or even during, the collection of electromagnetic data would be important, not only for quality check purposes, but also for adjusting the location of the proposed flight lines during an airborne time-domain acquisition. This kind of readiness could have a large impact in terms of optimization of the Value of Information of the measurements to be acquired. In addition, the importance of having fast tools for retrieving resistivity models from airborne time-domain data is demonstrated by the fact that Conductivity-Depth Imaging methodologies are still the standard in mineral exploration. In fact, they are extremely computationally efficient, and, at the same time, they preserve a very high lateral resolution. For these reasons, they are often preferred to inversion strategies even if the latter approaches are generally more accurate in terms of proper reconstruction of the depth of the targets and of reliable retrieval of true resistivity values of the subsurface. In this research, we discuss a novel approach, based on neural network techniques, capable of retrieving resistivity models with a quality comparable with the inversion strategy, but in a fraction of the time. We demonstrate the advantages of the proposed novel approach on synthetic and field datasets.\n",
            "\n",
            "Title: Neural Network: An Improved FCM for Multimodal Cultural Data Analysis\n",
            "Abstract: With the development of cloud computing, big data, and artificial intelligence (AI) technology, there is a growing interest in “cultural analysis.” Cultural analysis requires different types of data such as texts, pictures, and videos. The richness and differences of resources in the cultural field lead to diverse modalities of cultural data. Traditional text analysis methods can no longer meet the data analysis needs of current multimedia cultural resources. This article starts from cultural data’s feature information to solve the heterogeneity problem faced by massive multimodal cultural data analysis. It analyzes it from geography, time, art, and thematic character, classified and aggregated to form a multimodal cultural feature information matrix. The corresponding correlation measurement methods for different matrices from the above dimensions are proposed, solved in turn, and substituted into the optimized training back propagation (BP) neural network to obtain the final correlation degree. The improved fuzzy C-means (FCM) clustering algorithm is used to aggregate the high correlation cultural data based on the degree. The algorithm proposed in this study is compared with the existing algorithm. The experimental results show that the optimized BP neural network is at least 58% more accurate than the current method for calculating different matrices’ correlation degrees. In terms of accuracy, the improved fuzzy C-means algorithm effectively reduces the random interference in the selection of the initial clustering center, which is significantly higher than other clustering algorithms.\n",
            "\n",
            "Title: A Sequential Graph Neural Network for Short Text Classification\n",
            "Abstract: Short text classification is an important problem of natural language processing (NLP), and graph neural networks (GNNs) have been successfully used to solve different NLP problems. However, few studies employ GNN for short text classification, and most of the existing graph-based models ignore sequential information (e.g., word orders) in each document. In this work, we propose an improved sequence-based feature propagation scheme, which fully uses word representation and document-level word interaction and overcomes the limitations of textual features in short texts. On this basis, we utilize this propagation scheme to construct a lightweight model, sequential GNN (SGNN), and its extended model, ESGNN. Specifically, we build individual graphs for each document in the short text corpus based on word co-occurrence and use a bidirectional long short-term memory network (Bi-LSTM) to extract the sequential features of each document; therefore, word nodes in the document graph retain contextual information. Furthermore, two different simplified graph convolutional networks (GCNs) are used to learn word representations based on their local structures. Finally, word nodes combined with sequential information and local information are incorporated as the document representation. Extensive experiments on seven benchmark datasets demonstrate the effectiveness of our method.\n",
            "\n",
            "Title: Questions and Answers on Legal Texts Based on BERT-BiGRU\n",
            "Abstract: Using text question and answer technology to improve the efficiency of judicial personnel in the process of case handling can greatly reduce labour costs. This paper proposes a hybrid neural network model that combines pre-training model BERT and Bi-GRU. The model first uses the pre-training model to learn powerful semantic capabilities, then combines Bi-GRU to learn the semantic information between the text and the question, finally gets the answer corresponding to the legal text. The experimental results on the CJRC data set show that compared with the basic baseline model, the algorithm in this paper can effectively improve the accuracy and F1 value.\n",
            "\n",
            "Title: Современные методы детектирования и классификации токсичных комментариев с использованием нейронных сетей\n",
            "Abstract: The growth of popularity of online platforms which allow users to communicate with each other, share opinions about various events, and leave comments boosted the development of natural language processing algorithms. Tens of millions of messages per day are published by users of a particular social network need to be analyzed in real time for moderation in order to prevent the spread of various illegal or offensive information, threats and other types of toxic comments. Of course, such a large amount of information can be processed quite quickly only automatically. that is why there is a need to and a way to teach computers to “understand” a text written by humans. It is a non-trivial task even if the word “understand” here means only “to classify”. the rapid evolution of machine learning technologies has led to ubiquitous implementation of new algorithms. A lot of tasks, which for many years were considered almost impossible to solve, are now quite successfully solved using deep learning technologies. this article considers algorithms built using deep learning technologies and neural networks which can successfully solve the problem of detection and classification of toxic comments. In addition, the article presents the results of the developed algorithms, as well as the results of the ensemble of all considered algorithms on a large training set collected and tagged by Google and Jigsaw.\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{\n",
            "  \"topic\": \"Neural Networks for Text Classification\",\n",
            "  \"title\": \"Advancements in Neural Networks for Text Classification\",\n",
            "  \"reasoning\": \"The majority of the abstracts focus on the advancements in neural networks for text classification. These advancements include the use of contextualized word representations, dilated convolutional neural networks, graph neural networks, and hybrid models combining pre-training models with recurrent neural networks. The abstracts highlight the benefits of these neural network models in improving the performance of text classification tasks, such as sentiment analysis, problem-solution classification, short text classification, and legal text question-answering. Therefore, a section titled 'Advancements in Neural Networks for Text Classification' would encompass the main topic discussed in these abstracts.\"\n",
            "}\n",
            "Results for section 2\n",
            "{'topic': 'Neural Networks for Text Classification', 'title': 'Advancements in Neural Networks for Text Classification', 'reasoning': \"The majority of the abstracts focus on the advancements in neural networks for text classification. These advancements include the use of contextualized word representations, dilated convolutional neural networks, graph neural networks, and hybrid models combining pre-training models with recurrent neural networks. The abstracts highlight the benefits of these neural network models in improving the performance of text classification tasks, such as sentiment analysis, problem-solution classification, short text classification, and legal text question-answering. Therefore, a section titled 'Advancements in Neural Networks for Text Classification' would encompass the main topic discussed in these abstracts.\"}\n",
            "....................................................................................................\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: You are a renowned scientist who is writing a survey on 'text neural information retrieval'. You are currently writing a section about 'text neural information retrieval'\n",
            "Human: I will send you a list of title and abstract of scientific articles. Most of them cover a specific topic about the section 'text neural information retrieval'.\n",
            "Your task is to find out what this topic is and suggest a good title for a section in a scientific survey that addresses it.\n",
            "You should also explain your reasoning.\n",
            "Your answer should be a valid RFC8259 compliant JSON object with three properties:\n",
            "The first property, called \"topic\", describes the main topic that is a subset of 'text neural information retrieval' that are present in most abstracts.\n",
            "The second property, called \"title\", is the title of the section that will cover this topic and must be clearly related to the property \"topic\".\n",
            "The third property is called \"reasoning\" and should contains your reasoning to choose this topic as an answer.\n",
            "Use this format: \n",
            "{\n",
            "  \"topic\": {TOPIC},\n",
            "  \"title\": {TITLE},\n",
            "  \"reasoning\": {REASONING}\n",
            "}\n",
            "\n",
            "Do you understand?\n",
            "\n",
            "AI: Sure, send me the list and I will give you what you need.\n",
            "Human: Title: Is Image Encoding Beneficial for Deep Learning in Finance?\n",
            "Abstract: In 2012, Securities and Exchange Commission (SEC) mandated all corporate filings for any company doing business in the U.S. be entered into the electronic data gathering, analysis, and retrieval (EDGAR) system. In this work, we are investigating ways to analyze the data available through the EDGAR database. This may serve portfolio managers (pension funds, mutual funds, insurance, and hedge funds) to get automated insights into companies they invest in, to better manage their portfolios. The analysis is based on artificial neural networks applied to the data. In particular, one of the most popular machine learning methods, the convolutional neural network (CNN) architecture, originally developed to interpret and classify images, is now being used to interpret financial data. This work investigates the best way to input data collected from the SEC filings into a CNN architecture. We incorporate accounting principles and mathematical methods into the design of three image encoding methods. Specifically, two methods are derived from accounting principles (sequential arrangement, category chunk arrangement) and one is using a purely mathematical technique [the Hilbert vector arrangement (HVA)]. In this work, we analyze fundamental financial data as well as financial ratio data and study companies from the financial, healthcare, and information technology sectors in the United States. We find that using imaging techniques to input data for CNN works better for financial ratio data but is not significantly better than simply using the 1-D input directly for fundamental data. We do not find the HVA technique to be significantly better than other imaging techniques.\n",
            "\n",
            "Title: Comparative Analysis of Deep Learning Approaches for Twitter Text Classification\n",
            "Abstract: Abstract—Sentiment analysis (also known as opinion mining or emotion AI) is the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. Sentiment analysis is widely applied to voice of the customer materials such as reviews and survey responses, online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine. With the rise of deep language models, such as RoBERTa, also more difficult data domains can be analyzed, e.g., news texts where authors typically express their opinion/sentiment less explicitly. Sentiment analysis aims to extract opinion automatically from data and classify them as positive and negative. Twitter widely used social media tools, been seen as an important source of information for acquiring people’s attitudes, emotions, views, and feedbacks. Within this context, Twitter sentiment analysis techniques were developed to decide whether textual tweets express a positive or negative opinion. In contrast to lower classification performance of traditional algorithms, deep learning models, including Convolution Neural Network (CNN) and Bidirectional Long Short-Term Memory (Bi-LSTM), have achieved a significant result in sentiment analysis. Keras is a Deep Learning (DL) framework that provides an embedding layer to produce the vector representation of words present in the document. The objective of this work is to analyze the performance of deep learning models namely Convolutional Neural Network (CNN), Simple Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM), bidirectional Long Short-Term Memory (Bi-LSTM), BERT and RoBERTa for classifying the twitter reviews. From the experiments conducted, it is found that RoBERTa model performs better than CNN and simple RNN for sentiment classification. Keywords—Convolution Neural Network (CNN), Recurrent Neural Network (RNN), Long Short-Term Memory (LSTM), Deep Learning, Bidirectional Long Short-Term Memory (BiLSTM), Bidirectional Encoder Representations from Transformers (BERT), Robustly Optimized BERT Pre-training Approach (RoBERTa).\n",
            "\n",
            "Title: It Takes Two to Tango: Combining Visual and Textual Information for Detecting Duplicate Video-Based Bug Reports\n",
            "Abstract: When a bug manifests in a user-facing application, it is likely to be exposed through the graphical user interface (GUI). Given the importance of visual information to the process of identifying and understanding such bugs, users are increasingly making use of screenshots and screen-recordings as a means to report issues to developers. However, when such information is reported en masse, such as during crowd-sourced testing, managing these artifacts can be a time-consuming process. As the reporting of screen-recordings in particular becomes more popular, developers are likely to face challenges related to manually identifying videos that depict duplicate bugs. Due to their graphical nature, screen-recordings present challenges for automated analysis that preclude the use of current duplicate bug report detection techniques. To overcome these challenges and aid developers in this task, this paper presents Tango, a duplicate detection technique that operates purely on video-based bug reports by leveraging both visual and textual information. Tango combines tailored computer vision techniques, optical character recognition, and text retrieval. We evaluated multiple configurations of Tango in a comprehensive empirical evaluation on 4,860 duplicate detection tasks that involved a total of 180 screen-recordings from six Android apps. Additionally, we conducted a user study investigating the effort required for developers to manually detect duplicate video-based bug reports and compared this to the effort required to use Tango. The results reveal that Tango's optimal configuration is highly effective at detecting duplicate video-based bug reports, accurately ranking target duplicate videos in the top-2 returned results in 83% of the tasks. Additionally, our user study shows that, on average, Tango can reduce developer effort by over 60%, illustrating its practicality.\n",
            "\n",
            "Title: Second Harmonic Imaging Enhanced by Deep Learning Decipher\n",
            "Abstract: Wavefront sensing and reconstruction are widely used for adaptive optics, aberration correction, and high-resolution optical phase imaging. Traditionally, interference and/or microlens arrays are used to convert the optical phase into intensity variation. Direct imaging of distorted wavefront usually results in complicated phase retrieval with low contrast and low sensitivity. Here, a novel approach has been developed and experimentally demonstrated based on the phase-sensitive information encoded into second harmonic signals, which are intrinsically sensitive to wavefront modulations. By designing and implementing a deep neural network, we demonstrate the second harmonic imaging enhanced by deep learning decipher (SHIELD) for efficient and resilient phase retrieval. Inheriting the advantages of two-photon microscopy, SHIELD demonstrates single-shot, reference-free, and video-rate phase imaging with sensitivity better than {\\lambda}/100 and high robustness against noises, facilitating numerous applications from biological imaging to wavefront sensing.\n",
            "\n",
            "Title: Distantly-Supervised Neural Relation Extraction with Side Information using BERT\n",
            "Abstract: Relation extraction (RE) consists in categorizing the relationship between entities in a sentence. A recent paradigm to develop relation extractors is Distant Supervision (DS), which allows the automatic creation of new datasets by taking an alignment between a text corpus and a Knowledge Base (KB). KBs can sometimes also provide additional information to the RE task. One of the methods that adopt this strategy is the RESIDE model, which proposes a distantly-supervised neural relation extraction using side information from KBs. Considering that this method outperformed state-of-the-art baselines, in this paper, we propose a related approach to RESIDE also using additional side information, but simplifying the sentence encoding with BERT embeddings. Through experiments, we show the effectiveness of the proposed method in Google Distant Supervision and Riedel datasets concerning the BGWA and RESIDE baseline methods. Although Area Under the Curve is decreased because of unbalanced datasets, P@N results have shown that the use of BERT as sentence encoding allows superior performance to baseline methods.\n",
            "\n",
            "Title: A Non-Exclusive Multi-Class Convolutional Neural Network for the Classification of Functional Requirements in AUTOSAR Software Requirement Specification Text\n",
            "Abstract: Software Requirement Specification (SRS) describes a software system to be developed that captures the functional, non-functional, and technical aspects of the stakeholder’s requirements. Retrieval and extraction of software information from SRS are essential to the development of software product line (SPL). Albeit Natural Language Processing (NLP) techniques, such as information retrieval and standard machine learning, have been advocated in the recent past as a semi-automatic means of optimising requirements specifications, they have not been widely embraced. The complexity in the organization’s information makes requirement analysis intricately a challenging task. The interdependence of subsystems and within an organisation drives this complexity. A plain multi-class classification framework may not address this issue. Hence, this paper propounds an automated non-exclusive approach for classification of functional requirements from SRS, using a deep learning framework. Specifically, Word2Vec and FastText word embeddings are utilised for document representation for training a convolutional neural network (CNN). The study was carried out by the compilation of manually categorised relevant enterprise data (AUTomotive Open System ARchitecture (AUTOSAR)), which were also employed for model training. Over a convolutional neural network, the impact of data trained with Word2Vec and FastText word embeddings from SRS documentation were compared to pre-trained word embeddings models, available online.\n",
            "\n",
            "Title: A Survey of Query Expansion Methods to Improve Relevant Search Engine Results\n",
            "Abstract: Due to large volumes of documents available for retrieval in a search database, an intelligent method is required to retrieve relevant search results. Query expansion is one of such methods widely used in retrieving pertinent results of various search domains. The increased amount of information stored in a search engine database requires the use of query expansion. A query expansion deals with expanding the query by adding additional information to the query for effective retrieving relevant results. Recently, many query expansion techniques have been proposed to addresses the vocabulary mismatch problem that may arise in the information retrieval system. However, these techniques still have low precision results. This paper presents a systematic review of query expansion research from 1999 to 2018. The paper reviewed and discussed 573 research papers on query expansion methods and their application areas. It focuses only on the query expansion in text retrieval of search engines. This review's primary goal is to provide a broad overview of query expansion research and view how research approaches changed. The research paper analyzed and presented the contributions of each query expansion study. It also identifies major application areas of query expansions and their future opportunities. The finding of this study indicates a trend towards using semantic-ontology and pseudo-relevant feedbacks methods. This work will be beneficial to query expansion researchers in extending future work on query expansion research.\n",
            "\n",
            "Title: Exploring biomedical records through text mining-driven complex data visualisation\n",
            "Abstract: The recent events in health call for the prioritization of insightful and meaningful information retrieval from the fastly growing pool of biomedical knowledge. This information has its own challenges both in the data itself and in its appropriate representation, enhancing its usability by health professionals. In this paper we present a framework leveraging the MEDLINE dataset and its controlled vocabulary, the MeSH Headings, to annotate and explore health-related documents. The MEDijs system ingests and automatically annotates text documents, extending their legacy metadata with MeSH Headings. It then uses text mining algorithms that enable interactive data visualisations. These allow the user to the exploration of the enriched data made available by the MEDijs system.\n",
            "\n",
            "Title: Multi-Modal Graph Neural Network for Joint Reasoning on Vision and Scene Text\n",
            "Abstract: Answering questions that require reading texts in an image is challenging for current models. One key difficulty of this task is that rare, polysemous, and ambiguous words frequently appear in images, e.g., names of places, products, and sports teams. To overcome this difficulty, only resorting to pre-trained word embedding models is far from enough. A desired model should utilize the rich information in multiple modalities of the image to help understand the meaning of scene texts, e.g., the prominent text on a bottle is most likely to be the brand. Following this idea, we propose a novel VQA approach, Multi-Modal Graph Neural Network (MM-GNN). It first represents an image as a graph consisting of three sub-graphs, depicting visual, semantic, and numeric modalities respectively. Then, we introduce three aggregators which guide the message passing from one graph to another to utilize the contexts in various modalities, so as to refine the features of nodes. The updated nodes have better features for the downstream question answering module. Experimental evaluations show that our MM-GNN represents the scene texts better and obviously facilitates the performances on two VQA tasks that require reading scene texts.\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{\n",
            "  \"topic\": \"Deep Learning Approaches for Text Classification\",\n",
            "  \"title\": \"Deep Learning Approaches for Text Classification in Scientific Surveys\",\n",
            "  \"reasoning\": \"Based on the abstracts provided, multiple articles discuss the application of deep learning approaches, such as convolutional neural networks (CNN), recurrent neural networks (RNN), and transformers, for text classification tasks. These tasks include sentiment analysis, relation extraction, bug report detection, requirement classification, and query expansion. Therefore, a section in a scientific survey that covers these topics and explores the effectiveness of deep learning approaches for text classification would be relevant and informative.\"\n",
            "}\n",
            "Results for section 4\n",
            "{'topic': 'Deep Learning Approaches for Text Classification', 'title': 'Deep Learning Approaches for Text Classification in Scientific Surveys', 'reasoning': 'Based on the abstracts provided, multiple articles discuss the application of deep learning approaches, such as convolutional neural networks (CNN), recurrent neural networks (RNN), and transformers, for text classification tasks. These tasks include sentiment analysis, relation extraction, bug report detection, requirement classification, and query expansion. Therefore, a section in a scientific survey that covers these topics and explores the effectiveness of deep learning approaches for text classification would be relevant and informative.'}\n",
            "....................................................................................................\n",
            "Extracting subsections...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "51a2b4396f4d40189890f3f454035d0f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster 0: 33\n",
            "Cluster 2: 7\n",
            "Cluster 1: 10\n",
            "Extracting 3 sections in this level:\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: You are a renowned scientist who is writing a survey on 'text neural information retrieval'. You are currently writing a section about 'Deep Learning Approaches for Text Classification in Scientific Surveys'\n",
            "Human: I will send you a list of title and abstract of scientific articles. Most of them cover a specific topic about the section 'Deep Learning Approaches for Text Classification in Scientific Surveys'.\n",
            "Your task is to find out what this topic is and suggest a good title for a section in a scientific survey that addresses it.\n",
            "You should also explain your reasoning.\n",
            "Your answer should be a valid RFC8259 compliant JSON object with three properties:\n",
            "The first property, called \"topic\", describes the main topic that is a subset of 'Deep Learning Approaches for Text Classification in Scientific Surveys' that are present in most abstracts.\n",
            "The second property, called \"title\", is the title of the section that will cover this topic and must be clearly related to the property \"topic\".\n",
            "The third property is called \"reasoning\" and should contains your reasoning to choose this topic as an answer.\n",
            "Use this format: \n",
            "{\n",
            "  \"topic\": {TOPIC},\n",
            "  \"title\": {TITLE},\n",
            "  \"reasoning\": {REASONING}\n",
            "}\n",
            "\n",
            "Do you understand?\n",
            "\n",
            "AI: Sure, send me the list and I will give you what you need.\n",
            "Human: Title: Sparse, Dense, and Attentional Representations for Text Retrieval\n",
            "Abstract: Abstract Dual encoders perform retrieval by encoding documents and queries into dense low-dimensional vectors, scoring each document by its inner product with the query. We investigate the capacity of this architecture relative to sparse bag-of-words models and attentional neural networks. Using both theoretical and empirical analysis, we establish connections between the encoding dimension, the margin between gold and lower-ranked documents, and the document length, suggesting limitations in the capacity of fixed-length encodings to support precise retrieval of long documents. Building on these insights, we propose a simple neural model that combines the efficiency of dual encoders with some of the expressiveness of more costly attentional architectures, and explore sparse-dense hybrids to capitalize on the precision of sparse retrieval. These models outperform strong alternatives in large-scale retrieval.\n",
            "\n",
            "Title: Knowing False Negatives: An Adversarial Training Method for Distantly Supervised Relation Extraction\n",
            "Abstract: Distantly supervised relation extraction (RE) automatically aligns unstructured text with relation instances in a knowledge base (KB). Due to the incompleteness of current KBs, sentences implying certain relations may be annotated as N/A instances, which causes the so-called false negative (FN) problem. Current RE methods usually overlook this problem, inducing improper biases in both training and testing procedures. To address this issue, we propose a two-stage approach. First, it finds out possible FN samples by heuristically leveraging the memory mechanism of deep neural networks. Then, it aligns those unlabeled data with the training data into a unified feature space by adversarial training to assign pseudo labels and further utilize the information contained in them. Experiments on two wildly-used benchmark datasets demonstrate the effectiveness of our approach.\n",
            "\n",
            "Title: Knowledge Mapping of Medicinal Plants Based on Artificial Neural Network\n",
            "Abstract: Knowledge mapping of medicinal plants enable ordinary people to differentiate between medicinal plants and learn their pharmacological effects, provide assistances and instructions to medical workers during the use of medicinal plants, and support intelligent queries of the properties of traditional medicinal plants. This paper innovatively introduces artificial neural network to the knowledge mapping of medicinal plants, and provides a practical and valuable reference for scientific development and reasonable use of medicinal plants. Firstly, the entity relationships were designed for medical knowledge map, and the definitions, scales, and examples were given for each type of data in the proposed knowledge map of medicinal plants. Next, the authors detailed the ideas of multi-source knowledge fusion, and the acquisition and storage strategies for entity information of medicinal plants. Then, the attention-based bidirectional gated recurrent network was combined with convolutional neural network to detect the genetic relationships between medicinal plants from the angles of semantics and texts. Finally, this paper explains the semantic retrieval algorithm for medicinal plants, and visualizes the knowledge map. The proposed model and semantic retrieval algorithm were proved effective and superior through experiments. It is concluded that: The smaller the batch size, the higher the recognition accuracy of plant entities, and the better the recognition effect. The research findings provide a reference for knowledge mapping in other fields.\n",
            "\n",
            "Title: Automatic Integrated Scoring Model for English Composition Oriented to Part-Of-Speech Tagging\n",
            "Abstract: Part-of-speech tagging for English composition is the basis for automatic correction of English composition. The performance of the part-of-speech tagging system directly affects the performance of the marking and analysis of the correction system. Therefore, this paper proposes an automatic scoring model for English composition based on article part-of-speech tagging. First, use the convolutional neural network to extract the word information from the character level and use this part of the information in the coarse-grained learning layer. Secondly, the word-level vector is introduced, and the residual network is used to establish an information path to integrate the coarse-grained annotation and word vector information. Then, the model relies on the recurrent neural network to extract the overall information of the sequence data to obtain accurate annotation results. Then, the features of the text content are extracted, and the automatic scoring model of English composition is constructed by means of model fusion. Finally, this paper uses the English composition scoring competition data set on the international data mining competition platform Kaggle to verify the effect of the model.\n",
            "\n",
            "Title: A Semantic-Preserving Deep Hashing Model for Multi-Label Remote Sensing Image Retrieval\n",
            "Abstract: Conventional remote sensing image retrieval (RSIR) systems perform single-label retrieval with a single label to represent the most dominant semantic content for an image. Improved spatial resolution dramatically boosts the remote sensing image scene complexity, as a remote sensing image always contains multiple categories of surface features. In this case, a single label cannot comprehensively describe the semantic content of a complex remote sensing image scene and therefore results in poor retrieval performance in practical applications. As a result, researchers have begun to pay attention to multi-label image retrieval. However, in the era of massive remote sensing data, how to increase retrieval efficiency and reduce feature storage while preserving semantic information remains unsolved. Considering the powerful capability of hashing learning in overcoming the curse of dimensionality caused by high-dimensional image representation in Approximate Nearest Neighbor (ANN) search problems, we propose a new semantic-preserving deep hashing model for multi-label remote sensing image retrieval. Our model consists of three main components: (1) a convolutional neural network to extract image features; (2) a hash layer to generate binary codes; (3) a new loss function to better maintain the multi-label semantic information of hash learning contained in context remote sensing image scene. As far as we know, this is the first attempt to apply deep hashing into the multi-label remote sensing image retrieval. Experimental results indicate the effectiveness and promising of the introduction of hashing methods in the multi-label remote sensing image retrieval.\n",
            "\n",
            "Title: A Sequential Graph Neural Network for Short Text Classification\n",
            "Abstract: Short text classification is an important problem of natural language processing (NLP), and graph neural networks (GNNs) have been successfully used to solve different NLP problems. However, few studies employ GNN for short text classification, and most of the existing graph-based models ignore sequential information (e.g., word orders) in each document. In this work, we propose an improved sequence-based feature propagation scheme, which fully uses word representation and document-level word interaction and overcomes the limitations of textual features in short texts. On this basis, we utilize this propagation scheme to construct a lightweight model, sequential GNN (SGNN), and its extended model, ESGNN. Specifically, we build individual graphs for each document in the short text corpus based on word co-occurrence and use a bidirectional long short-term memory network (Bi-LSTM) to extract the sequential features of each document; therefore, word nodes in the document graph retain contextual information. Furthermore, two different simplified graph convolutional networks (GCNs) are used to learn word representations based on their local structures. Finally, word nodes combined with sequential information and local information are incorporated as the document representation. Extensive experiments on seven benchmark datasets demonstrate the effectiveness of our method.\n",
            "\n",
            "Title: TSSRGCN: Temporal Spectral Spatial Retrieval Graph Convolutional Network for Traffic Flow Forecasting\n",
            "Abstract: Traffic flow forecasting is of great significance for improving the efficiency of transportation systems and preventing emergencies. Due to the highly non-linearity and intricate evolutionary patterns of short-term and long-term traffic flow, existing methods often fail to take full advantage of spatial-temporal information, especially the various temporal patterns with different period shifting and the characteristics of road segments. Besides, the globality representing the absolute value of traffic status indicators and the locality representing the relative value have not been considered simultaneously. This paper proposes a neural network model that focuses on the globality and locality of traffic networks as well as the temporal patterns of traffic data. The cycle-based dilated deformable convolution block is designed to capture different time-varying trends on each node accurately. Our model can extract both global and local spatial information since we combine two graph convolutional network methods to learn the representations of nodes and edges. Experiments on two real-world datasets show that the model can scrutinize the spatial-temporal correlation of traffic data, and its performance is better than the compared state-of-the-art methods. Further analysis indicates that the locality and globality of the traffic networks are critical to traffic flow prediction and the proposed TSSRGCN model can adapt to the various temporal traffic patterns.\n",
            "\n",
            "Title: Real-world sentence boundary detection using multitask learning: A case study on French\n",
            "Abstract: \n",
            " We propose a novel approach for sentence boundary detection in text datasets in which boundaries are not evident (e.g., sentence fragments). Although detecting sentence boundaries without punctuation marks has rarely been explored in written text, current real-world textual data suffer from widespread lack of proper start/stop signaling. Herein, we annotate a dataset with linguistic information, such as parts of speech and named entity labels, to boost the sentence boundary detection task. Via experiments, we obtained F1 scores up to 98.07% using the proposed multitask neural model, including a score of 89.41% for sentences completely lacking punctuation marks. We also present an ablation study and provide a detailed analysis to demonstrate the effectiveness of the proposed multitask learning method.\n",
            "\n",
            "Title: PuzzLing Machines: A Challenge on Learning From Small Data\n",
            "Abstract: Deep neural models have repeatedly proved excellent at memorizing surface patterns from large datasets for various ML and NLP benchmarks. They struggle to achieve human-like thinking, however, because they lack the skill of iterative reasoning upon knowledge. To expose this problem in a new light, we introduce a challenge on learning from small data, PuzzLing Machines, which consists of Rosetta Stone puzzles from Linguistic Olympiads for high school students. These puzzles are carefully designed to contain only the minimal amount of parallel text necessary to deduce the form of unseen expressions. Solving them does not require external information (e.g., knowledge bases, visual signals) or linguistic expertise, but meta-linguistic awareness and deductive skills. Our challenge contains around 100 puzzles covering a wide range of linguistic phenomena from 81 languages. We show that both simple statistical algorithms and state-of-the-art deep neural models perform inadequately on this challenge, as expected. We hope that this benchmark, available at https://ukplab.github.io/PuzzLing-Machines/, inspires further efforts towards a new paradigm in NLP—one that is grounded in human-like reasoning and understanding.\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{\n",
            "  \"topic\": \"Text Classification for Scientific Surveys\",\n",
            "  \"title\": \"Deep Learning Approaches for Text Classification in Scientific Surveys\",\n",
            "  \"reasoning\": \"The abstracts of the provided articles discuss various deep learning approaches for text classification in scientific surveys. These approaches include sparse, dense, and attentional representations for text retrieval, adversarial training for relation extraction, artificial neural networks for knowledge mapping, automatic scoring models for English composition, deep hashing models for multi-label remote sensing image retrieval, sequential graph neural networks for short text classification, and temporal spectral spatial retrieval graph convolutional networks for traffic flow forecasting. Therefore, the main topic that is a subset of 'Deep Learning Approaches for Text Classification in Scientific Surveys' present in most abstracts is 'Text Classification for Scientific Surveys'. The suggested title for the section covering this topic is 'Text Classification for Scientific Surveys: Deep Learning Approaches'.\"\n",
            "}\n",
            "Results for section 0\n",
            "{'topic': 'Text Classification for Scientific Surveys', 'title': 'Deep Learning Approaches for Text Classification in Scientific Surveys', 'reasoning': \"The abstracts of the provided articles discuss various deep learning approaches for text classification in scientific surveys. These approaches include sparse, dense, and attentional representations for text retrieval, adversarial training for relation extraction, artificial neural networks for knowledge mapping, automatic scoring models for English composition, deep hashing models for multi-label remote sensing image retrieval, sequential graph neural networks for short text classification, and temporal spectral spatial retrieval graph convolutional networks for traffic flow forecasting. Therefore, the main topic that is a subset of 'Deep Learning Approaches for Text Classification in Scientific Surveys' present in most abstracts is 'Text Classification for Scientific Surveys'. The suggested title for the section covering this topic is 'Text Classification for Scientific Surveys: Deep Learning Approaches'.\"}\n",
            "....................................................................................................\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: You are a renowned scientist who is writing a survey on 'text neural information retrieval'. You are currently writing a section about 'Deep Learning Approaches for Text Classification in Scientific Surveys'\n",
            "Human: I will send you a list of title and abstract of scientific articles. Most of them cover a specific topic about the section 'Deep Learning Approaches for Text Classification in Scientific Surveys'.\n",
            "Your task is to find out what this topic is and suggest a good title for a section in a scientific survey that addresses it.\n",
            "You should also explain your reasoning.\n",
            "Your answer should be a valid RFC8259 compliant JSON object with three properties:\n",
            "The first property, called \"topic\", describes the main topic that is a subset of 'Deep Learning Approaches for Text Classification in Scientific Surveys' that are present in most abstracts.\n",
            "The second property, called \"title\", is the title of the section that will cover this topic and must be clearly related to the property \"topic\".\n",
            "The third property is called \"reasoning\" and should contains your reasoning to choose this topic as an answer.\n",
            "Use this format: \n",
            "{\n",
            "  \"topic\": {TOPIC},\n",
            "  \"title\": {TITLE},\n",
            "  \"reasoning\": {REASONING}\n",
            "}\n",
            "\n",
            "Do you understand?\n",
            "\n",
            "AI: Sure, send me the list and I will give you what you need.\n",
            "Human: Title: Various Approaches of Machine Translation for Marathi to English Language\n",
            "Abstract: Machine Translation (MT) is a generic term for computerised systems that generate translations from one natural language to another, with or without human intervention. Text may be used to examine knowledge, and turning that information into pictures helps people to communicate and acquire information.There seems to be a lot of work conducted on translating English to Hindi, Tamil, Bangla and other languages. The important parts of translation are to provide translated sentences with correct words and proper grammar. There has been a comprehensive review of 10 primary publications used in research. Two separate approaches are proposed, one uses rule based approach and other uses neural-machine translation approach to translate basic Marathi phrases to English. While designed primarily for Marathi-English language pairs, the design can be applied to other language pairs with a similar structure.\n",
            "\n",
            "Title: Twitter Sentiment Analysis Using Recurrent Neural Network\n",
            "Abstract: Being one of the largest social media platforms, Twitter has a diverse community collaborating on a multitude of ideas. With large amounts of data being generated and collected on Twitter every day, it is the perfect platform to implement machine learning algorithms to analyze information in different tweets. A recurrent Neural Network (RNN) is a specific algorithm that is used to solve problems involving sequential data such as texts or time series. In this paper, we use RNNs to classify a given tweet as either a positive or a negative sentiment. The RNN model we describe below was able to reach an accuracy of 76%. \n",
            "\n",
            "Title: An Overview of Ontology Learning Process from Arabic Text\n",
            "Abstract: Ontology learning (OL) Plays an important role in many fields such as semantic web, Data Integration and Interoperability, Machine Translation, and Information Retrieval. The success of any of these fields depends on the quality of its ontologies. This paper presents a comprehensive survey of research on the Ontology Learning for Arabic texts. Most of the previous works focused on three main issues: extracting the terms, extracting the semantic relations, and building the ontology from the Arabic text. There are more of techniques and methods can be used for this process. In this paper, first we present the Arabic challenges that were reasons for developing few Arabic Ontology Learning systems. Second we make a research comparison based on the techniques used and their results. Third, we pointed limitations and comments of research works on Arabic Ontology Learning. Finally, we concluded the paper and outlined our future research direction in this area.\n",
            "\n",
            "Title: Improving Sentence Representations via Component Focusing\n",
            "Abstract: The efficiency of natural language processing (NLP) tasks, such as text classification and information retrieval, can be significantly improved with proper sentence representations. Neural networks such as convolutional neural network (CNN) and recurrent neural network (RNN) are gradually applied to learn the representations of sentences and are suitable for processing sequences. Recently, bidirectional encoder representations from transformers (BERT) has attracted much attention because it achieves state-of-the-art performance on various NLP tasks. However, these standard models do not adequately address a general linguistic fact, that is, different sentence components serve diverse roles in the meaning of a sentence. In general, the subject, predicate, and object serve the most crucial roles as they represent the primary meaning of a sentence. Additionally, words in a sentence are also related to each other by syntactic relations. To emphasize on these issues, we propose a sentence representation model, a modification of the pre-trained bidirectional encoder representations from transformers (BERT) network via component focusing (CF-BERT). The sentence representation consists of a basic part which refers to the complete sentence, and a component-enhanced part, which focuses on subject, predicate, object, and their relations. For the best performance, a weight factor is introduced to adjust the ratio of both parts. We evaluate CF-BERT on two different tasks: semantic textual similarity and entailment classification. Results show that CF-BERT yields a significant performance gain compared to other sentence representation methods.\n",
            "\n",
            "Title: N-Gram, Semantic-Based Neural Network for Mobile Malware Network Traffic Detection\n",
            "Abstract: Mobile malware poses a great challenge to mobile devices and mobile communication. With the explosive growth of mobile networks, it is significant to detect mobile malware for mobile security. Since most mobile malware relies on the networks to coordinate operations, steal information, or launch attacks, evading network monitor is difficult for the mobile malware. In this paper, we present an N-gram, semantic-based neural modeling method to detect the network traffic generated by the mobile malware. In the proposed scheme, we segment the network traffic into flows and extract the application layer payload from each packet. Then, the generated flow payload data are converted into the text form as the input of the proposed model. Each flow text consists of several domains with 20 words. The proposed scheme models the domain representation using convolutional neural network with multiwidth kernels from each domain. Afterward, relationships of domains are adaptively encoded in flow representation using gated recurrent network and then the classification result is obtained from an attention layer. A series of experiments have been conducted to verify the effectiveness of our proposed scheme. In addition, to compare with the state-of-the-art methods, several comparative experiments also are conducted. The experiment results depict that our proposed scheme is better in terms of accuracy.\n",
            "\n",
            "Title: MUSE: Textual Attributes Guided Portrait Painting Generation\n",
            "Abstract: We propose a novel approach, MUSE, to automatically generate portrait paintings guided by textual attributes. MUSE takes a set of attributes written in text, in addition to facial features extracted from a photo of the subject as input. We propose 11 attribute types to represent inspirations from a subject's profile, emotion, story, and environment. Then we design a novel stacked neural network architecture by extending an image-to-image generative model to accept textual attributes. Experiments show that our approach significantly outperforms several state-of-the-art methods without using textual attributes, with Inception Score score increased by 6% and Frechet Inception Distance (FID) score decreased by 11%, respectively. We also propose a new attribute reconstruction metric to evaluate whether the generated portraits preserve the subject's attributes. Experiments show that our approach can accurately illustrate 78% textual attributes, which also help MUSE capture the subject in a more creative and expressive way.1\n",
            "\n",
            "Title: Using Character-Level and Entity-Level Representations to Enhance Bidirectional Encoder Representation From Transformers-Based Clinical Semantic Textual Similarity Model: ClinicalSTS Modeling Study (Preprint)\n",
            "Abstract: \n",
            " BACKGROUND\n",
            " With the popularity of electronic health records (EHRs), the quality of health care has been improved. However, there are also some problems caused by EHRs, such as the growing use of copy-and-paste and templates, resulting in EHRs of low quality in content. In order to minimize data redundancy in different documents, Harvard Medical School and Mayo Clinic organized a national natural language processing (NLP) clinical challenge (n2c2) on clinical semantic textual similarity (ClinicalSTS) in 2019. The task of this challenge is to compute the semantic similarity among clinical text snippets.\n",
            " \n",
            " \n",
            " OBJECTIVE\n",
            " In this study, we aim to investigate novel methods to model ClinicalSTS and analyze the results.\n",
            " \n",
            " \n",
            " METHODS\n",
            " We propose a semantically enhanced text matching model for the 2019 n2c2/Open Health NLP (OHNLP) challenge on ClinicalSTS. The model includes 3 representation modules to encode clinical text snippet pairs at different levels: (1) character-level representation module based on convolutional neural network (CNN) to tackle the out-of-vocabulary problem in NLP; (2) sentence-level representation module that adopts a pretrained language model bidirectional encoder representation from transformers (BERT) to encode clinical text snippet pairs; and (3) entity-level representation module to model clinical entity information in clinical text snippets. In the case of entity-level representation, we compare 2 methods. One encodes entities by the entity-type label sequence corresponding to text snippet (called entity I), whereas the other encodes entities by their representation in MeSH, a knowledge graph in the medical domain (called entity II).\n",
            " \n",
            " \n",
            " RESULTS\n",
            " We conduct experiments on the ClinicalSTS corpus of the 2019 n2c2/OHNLP challenge for model performance evaluation. The model only using BERT for text snippet pair encoding achieved a Pearson correlation coefficient (PCC) of 0.848. When character-level representation and entity-level representation are individually added into our model, the PCC increased to 0.857 and 0.854 (entity I)/0.859 (entity II), respectively. When both character-level representation and entity-level representation are added into our model, the PCC further increased to 0.861 (entity I) and 0.868 (entity II).\n",
            " \n",
            " \n",
            " CONCLUSIONS\n",
            " Experimental results show that both character-level information and entity-level information can effectively enhance the BERT-based STS model.\n",
            "\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{\n",
            "  \"topic\": \"Text Classification for Semantic Similarity in Clinical Texts\",\n",
            "  \"title\": \"Enhancing Clinical Semantic Textual Similarity with Character-Level and Entity-Level Representations\",\n",
            "  \"reasoning\": \"The abstract discusses the use of different representation modules, including character-level and entity-level representations, to enhance the performance of a model for clinical semantic textual similarity (ClinicalSTS). The paper focuses on improving the accuracy of computing semantic similarity among clinical text snippets, which falls under the broader topic of text classification in scientific surveys. The suggested title reflects the use of character-level and entity-level representations and their impact on enhancing the clinical semantic textual similarity model.\"\n",
            "}\n",
            "Results for section 1\n",
            "{'topic': 'Text Classification for Semantic Similarity in Clinical Texts', 'title': 'Enhancing Clinical Semantic Textual Similarity with Character-Level and Entity-Level Representations', 'reasoning': 'The abstract discusses the use of different representation modules, including character-level and entity-level representations, to enhance the performance of a model for clinical semantic textual similarity (ClinicalSTS). The paper focuses on improving the accuracy of computing semantic similarity among clinical text snippets, which falls under the broader topic of text classification in scientific surveys. The suggested title reflects the use of character-level and entity-level representations and their impact on enhancing the clinical semantic textual similarity model.'}\n",
            "....................................................................................................\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: You are a renowned scientist who is writing a survey on 'text neural information retrieval'. You are currently writing a section about 'Deep Learning Approaches for Text Classification in Scientific Surveys'\n",
            "Human: I will send you a list of title and abstract of scientific articles. Most of them cover a specific topic about the section 'Deep Learning Approaches for Text Classification in Scientific Surveys'.\n",
            "Your task is to find out what this topic is and suggest a good title for a section in a scientific survey that addresses it.\n",
            "You should also explain your reasoning.\n",
            "Your answer should be a valid RFC8259 compliant JSON object with three properties:\n",
            "The first property, called \"topic\", describes the main topic that is a subset of 'Deep Learning Approaches for Text Classification in Scientific Surveys' that are present in most abstracts.\n",
            "The second property, called \"title\", is the title of the section that will cover this topic and must be clearly related to the property \"topic\".\n",
            "The third property is called \"reasoning\" and should contains your reasoning to choose this topic as an answer.\n",
            "Use this format: \n",
            "{\n",
            "  \"topic\": {TOPIC},\n",
            "  \"title\": {TITLE},\n",
            "  \"reasoning\": {REASONING}\n",
            "}\n",
            "\n",
            "Do you understand?\n",
            "\n",
            "AI: Sure, send me the list and I will give you what you need.\n",
            "Human: Title: Learning to Evaluate Performance of Multimodal Semantic Localization\n",
            "Abstract: Semantic localization (SeLo) refers to the task of obtaining the most relevant locations in large-scale remote sensing (RS) images using semantic information, such as text. As an emerging task based on cross-modal retrieval, SeLo achieves semantic-level retrieval with only caption-level annotation, which demonstrates its great potential in unifying downstream tasks. Although SeLo has been carried out successively, there is currently no work that systematically explores and analyzes this urgent direction. In this article, we thoroughly study this field and provide a complete benchmark in terms of metrics and test data to advance the SeLo task. First, based on the characteristics of this task, we propose multiple discriminative evaluation metrics to quantify the performance of the SeLo task. The devised significant area proportion, attention shift distance, and discrete attention distance are utilized to evaluate the generated SeLo map from the pixel level and the region level. Next, to provide standard evaluation data for the SeLo task, we contribute a diverse, multisemantic, multiobjective SeLo testset (AIR-SLT). AIR-SLT consists of 22 large-scale RS images and 59 test cases with different semantics, which aims to provide comprehensive evaluations for retrieval models. Finally, we analyze the SeLo performance of RS cross-modal retrieval models in detail, explore the impact of different variables on this task, and provide a complete benchmark for the SeLo task. We have also established a new paradigm for RS referring expression comprehension (REC) and demonstrated the great advantage of SeLo in semantics by combining it with tasks such as detection and road extraction. The proposed evaluation metrics, SeLo testsets, and corresponding scripts have been open to access at https://github.com/xiaoyuan1996/SemanticLocalizationMetrics.\n",
            "\n",
            "Title: A Convolutional Recurrent Neural-Network-Based Machine Learning for Scene Text Recognition Application\n",
            "Abstract: Optical character recognition (OCR) is the process of acquiring text and layout information through analysis and recognition of text data image files. It is also a process to identify the geometric location and orientation of the texts and their symmetrical behavior. It usually consists of two steps: text detection and text recognition. Scene text recognition is a subfield of OCR that focuses on processing text in natural scenes, such as streets, billboards, license plates, etc. Unlike traditional document category photographs, it is a challenging task to use computer technology to locate and read text information in natural scenes. Imaging sequence recognition is a longstanding subject of research in the field of computer vision. Great progress has been made in this field; however, most models struggled to recognize text in images of complex scenes with high accuracy. This paper proposes a new pattern of text recognition based on the convolutional recurrent neural network (CRNN) as a solution to address this issue. It combines real-time scene text detection with differentiable binarization (DBNet) for text detection and segmentation, text direction classifier, and the Retinex algorithm for image enhancement. To evaluate the effectiveness of the proposed method, we performed experimental analysis of the proposed algorithm, and carried out simulation on complex scene image data based on existing literature data and also on several real datasets designed for a variety of nonstationary environments. Experimental results demonstrated that our proposed model performed better than the baseline methods on three benchmark datasets and achieved on-par performance with other approaches on existing datasets. This model can solve the problem that CRNN cannot identify text in complex and multi-oriented text scenes. Furthermore, it outperforms the original CRNN model with higher accuracy across a wider variety of application scenarios.\n",
            "\n",
            "Title: Conversational Agent in Romanian for Storing User Information in a Knowledge Graph\n",
            "Abstract: Information surrounds us and keeping track of relevant details can be challenging. Although there are multiple applications to take notes, organize ideas, or set reminders, existing solutions are semantic-agnostic and rely on the user to manually search for desired information by keywords. We propose a novel method to help people store and retrieve such details with ease in Romanian language. Our conversational agent built on top of the RASA framework is capable to extract relevant information from the user’s utterances, store them in a persistent knowledge graph, and ultimately, access them when requested. A set of specific intents regarding locations, timestamps, and properties were created and learned by the agent using manually built examples. In addition, an interaction logic based on a knowledge graph was added to enable the storage and retrieval of information, based on the identified semantic components from the input sentences. The performed tests showed a good accuracy for intent detection, and promising results for the sentence parser. Our conversational agent is accessible as a web application which can process text or speech inputs, and responds with a textual representation of the user’s memorized facts.\n",
            "\n",
            "Title: Robust Neural Text Classification and Entailment via Mixup Regularized Adversarial Training\n",
            "Abstract: Recent studies show that neural models for natural language processing are usually fragile under adversarial attacks (e.g., character-level insertion and word-level synonym substitution), which exposes the lack of robustness. Most defense techniques are tailored to specific semantic level attacks and do not possess the ability to mitigate multi-level attack simultaneously. Adversarial training has been shown the effectiveness of increasing model robustness. However, it often suffers from degradation on normal data, especially when the proportion of adversarial examples increase. To address this, we propose mixup regularized adversarial training (MRAT) against multi-level attack. Our method can utilize multiple adversarial examples to increase model intrinsic robustness without sacrificing the performance on normal data. We evaluate our method on text classification and entailment tasks. Experimental results on different text encoders (BERT, LSTM and CNN) with multi-level attack show that our method outperforms adversarial training consistently.\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{\n",
            "  \"topic\": \"Text Classification for Scene Text Recognition\",\n",
            "  \"title\": \"Advancements in Convolutional Recurrent Neural Networks for Scene Text Recognition\",\n",
            "  \"reasoning\": \"The abstract discusses the challenges of recognizing text in natural scenes and proposes a new approach using convolutional recurrent neural networks (CRNN) for scene text recognition. The paper presents experimental results demonstrating the effectiveness of the proposed method and its superiority over baseline methods. This topic focuses on deep learning approaches for text classification in the specific context of scene text recognition.\"\n",
            "}\n",
            "Results for section 2\n",
            "{'topic': 'Text Classification for Scene Text Recognition', 'title': 'Advancements in Convolutional Recurrent Neural Networks for Scene Text Recognition', 'reasoning': 'The abstract discusses the challenges of recognizing text in natural scenes and proposes a new approach using convolutional recurrent neural networks (CRNN) for scene text recognition. The paper presents experimental results demonstrating the effectiveness of the proposed method and its superiority over baseline methods. This topic focuses on deep learning approaches for text classification in the specific context of scene text recognition.'}\n",
            "....................................................................................................\n",
            "CPU times: user 9.45 s, sys: 657 ms, total: 10.1 s\n",
            "Wall time: 2min 11s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pretty print\n",
        "def print_sections(sections, level=0):\n",
        "  for section in sections:\n",
        "    section_title = '\\t'*(level) + section['section_title']\n",
        "    print(section_title)\n",
        "    if len(section['subsections']) > 0:\n",
        "      print_sections(section['subsections'], level+1)\n",
        "\n",
        "print_sections(sections)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3sGjz3Ohl6Y",
        "outputId": "52b6c799-afbb-4e59-c7c9-fd6f4a48b65b"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Semantic Fusion Strategies for Text Neural Information Retrieval\n",
            "\tDeep Learning Approaches for Misinformation Detection in Text\n",
            "\tGraph Neural Networks for Text Classification\n",
            "\tEnhancing Text Classification with Semantic Fusion Strategies\n",
            "Multi-modal Retrieval in Text Neural Information Retrieval\n",
            "\tEnhancing Text Neural Information Retrieval with Named Entities\n",
            "\tEnhancing Cross-Modal Retrieval in Text Neural Information Retrieval\n",
            "\tPattern Separation and Completion in Memory Retrieval\n",
            "Advancements in Neural Information Retrieval for Text Analysis\n",
            "\tAdvancements in Neural Networks for Text Classification\n",
            "\tAdvancements in Sentiment Analysis for Social Media Text\n",
            "\tAdvancements in Deep Learning for Text Classification\n",
            "Advancements in Text-based Neural Information Retrieval\n",
            "\tAdvancements in Knowledge-infused Attention Mechanism for Text Classification\n",
            "\tAdvancements in Text-to-Image Synthesis: Generating Images from Text\n",
            "\tAdvancements in Neural Networks for Text Classification\n",
            "Deep Learning Approaches for Text Classification in Scientific Surveys\n",
            "\tDeep Learning Approaches for Text Classification in Scientific Surveys\n",
            "\tEnhancing Clinical Semantic Textual Similarity with Character-Level and Entity-Level Representations\n",
            "\tAdvancements in Convolutional Recurrent Neural Networks for Scene Text Recognition\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_file(sections, f'sections-{SURVEY_TOPIC_QUERY}-{SURVEY_FILTERS[\"year\"]}-{PARAMS[\"gpt_model_name\"]}.pkl')"
      ],
      "metadata": {
        "id": "_4o5YrOfOVxH"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sections[0]['result_for_section']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Q7VJ3VXjUCu",
        "outputId": "88fefc26-b94b-41a9-cd54-0fa6cef361a0"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'topic': 'Semantic Information Fusion for Text Neural Information Retrieval',\n",
              " 'title': 'Semantic Fusion Strategies for Text Neural Information Retrieval',\n",
              " 'reasoning': \"Based on the abstracts provided, the common theme among the articles is the fusion of different sources of information to improve text neural information retrieval. This includes the fusion of Japanese text and voice recognition, visual saliency sharing for 3D model recognition, deep fusion CNN for image retrieval, hierarchical attentive heterogeneous graph network for extractive summarization, self-supervised spatial recurrent network for content-based image retrieval, contextualized query embeddings for conversational search, leveraging equivariant features for absolute pose regression, semantically enriched word embeddings for text classification, cognitive reasoning for complex information retrieval, and recurrent neural ordinary differential equations for social media text classification. Therefore, the main topic is the fusion of semantic information for text neural information retrieval, which involves integrating different sources of information to enhance the retrieval process. The suggested title, 'Semantic Fusion Strategies for Text Neural Information Retrieval', accurately reflects this topic.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    }
  ]
}