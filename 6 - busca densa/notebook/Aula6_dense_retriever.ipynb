{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcqFuVMQ94KA"
      },
      "source": [
        "# Aula 6 - Solução dos exercícios\n",
        "Leandro Carísio Fernandes\n",
        "\n",
        "<br>\n",
        "\n",
        "Iremos fazer finetuning de um buscador denso\n",
        "\n",
        "Usar como treino o dataset \"tiny\" do MS MARCO\n",
        "https://storage.googleapis.com/unicamp-dl/ia368dd_2023s1/msmarco/msmarco_triples.train.tiny.tsv\n",
        "\n",
        "Avaliar o modelo no TREC-COVID, e comparar os resultados com o BM25 e doc2query\n",
        "\n",
        "Comparar busca \"exaustiva\" (semelhança do vetor query com todos os vetores do corpus) com a busca aproximada (Approximate Nearest Neighbor - ANN)\n",
        "\n",
        "Para a busca aproximada, usar os algoritmos existentes na biblioteca sentence-transformers (ex: hnswlib) OU implemente um você mesmo (Bonus!)\n",
        "\n",
        "Dicas:\n",
        "\n",
        "- Usar a média dos vetores da última camada (conhecido como mean pooling) do transformer para representar queries e passagens; Alternativamente, usar apenas o vetor do [CLS] da última cada.\n",
        "\n",
        "- Tente inicialmente uma loss facil de implementar, como a entropia-cruzada\n",
        "\n",
        "- Começar o treino a partir do microsoft/MiniLM-L12-H384-uncased\n",
        "\n",
        "- Avaliar o pipeline usando um modelo já bem treinado: sentence-transformers/all-mpnet-base-v2\n",
        "\n",
        "- Comparar resultados usando semelhança de coseno e produto escalar como funções de similaridade\n",
        "\n",
        "- Para checar se seu codigo de avaliação está correto, comparar o seu desempenho com o do modelo já treinado no MS MARCO: https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2; O nDCG@10 no TREC-COVID deve ser ~0.47\n",
        "\n",
        "- Usar a biblioteca do sentence-transformers para avaliar o modelo\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz4tGFdv-VEJ"
      },
      "source": [
        "## Preparação do ambiente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckAfaNIp-Xks"
      },
      "source": [
        "### Variáveis para controlar o fluxo do caderno\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "TMQ9Lro1GMKi"
      },
      "outputs": [],
      "source": [
        "treinar_e_salvar_modelos = False\n",
        "\n",
        "gerar_e_salvar_matriz_docs_trec_covid = False\n",
        "\n",
        "# Local onde fica o arquivo que contém a matriz de todos os documentos do trec_covid. Se gerar_e_salvar_matriz_docs_trec_covid = True, esse arquivo será sobrescrito com a nova geração\n",
        "arquivo_matriz_docs_trec_covid = '/content/drive/My Drive/IA368-DD_deep_learning_busca/Aula6-buscador-denso/matriz_docs_trec_covid.pt'\n",
        "\n",
        "# Diretório onde vai salvando o modelo a cada época\n",
        "dir_modelos = '/content/drive/My Drive/IA368-DD_deep_learning_busca/Aula6-buscador-denso/modelos/'\n",
        "\n",
        "# Nome dos modelos e tokenizador. São esses modelos que serão carregados no início com o from_pretrained.\n",
        "# Se quiser iniciar um treinamento do 0, é necessário substituir por \"microsoft/MiniLM-L12-H384-uncased\"\n",
        "nome_modelo_query = '/content/drive/My Drive/IA368-DD_deep_learning_busca/Aula6-buscador-denso/modelos/final/query/' #nome_modelo_query = \"microsoft/MiniLM-L12-H384-uncased\"\n",
        "nome_modelo_doc = '/content/drive/My Drive/IA368-DD_deep_learning_busca/Aula6-buscador-denso/modelos/final/doc/' #nome_modelo_doc = \"microsoft/MiniLM-L12-H384-uncased\"\n",
        "nome_tokenizador = \"microsoft/MiniLM-L12-H384-uncased\"\n",
        "\n",
        "# Gerar e salvar arquivo de embeddings para os documentos usando os modelos da biblioteca Sequence Transformers:\n",
        "gerar_arquivo_doc_embeddings_all_mpnet_base_v2 = False\n",
        "arq_doc_embeddings_all_mpnet_base_v2 = '/content/drive/My Drive/IA368-DD_deep_learning_busca/Aula6-buscador-denso/docs_embeddings-all-mpnet-base-v2.pt'\n",
        "\n",
        "gerar_arquivo_doc_embeddings_all_MiniLM_L12_v2 = False\n",
        "arq_doc_embeddings_all_MiniLM_L12_v2 = '/content/drive/My Drive/IA368-DD_deep_learning_busca/Aula6-buscador-denso/docs_embeddings-all-MiniLM-L12-v2.pt'\n",
        "\n",
        "\n",
        "url_ms_marco_treinamento = \"https://storage.googleapis.com/unicamp-dl/ia368dd_2023s1/msmarco/msmarco_triples.train.tiny.tsv\"\n",
        "url_trec_covid = 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/trec-covid.zip'\n",
        "\n",
        "max_length = 256 \n",
        "batch_size = 32\n",
        "epochs = 20\n",
        "lr = 2e-5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM6qeJFz-3Nl"
      },
      "source": [
        "### Instalação de libs e montagem do Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_dJy9vSFF4l",
        "outputId": "0ba1be86-6f33-4d98-a70b-a68d95a1328e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Já monta o drive, pois vamos usar o índice invertido da Aula 1 para usar o BM25 implementado também na aula 1\n",
        "# Além disso, é necessário para salvar/recuperar o modelo tunado\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install transformers datasets -q\n",
        "!pip install sentence-transformers -q\n",
        "!pip install pyserini -q\n",
        "!pip install faiss-gpu -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aq9LmOj4Fi4x"
      },
      "source": [
        "## Fine-tuning dos encoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJM5c2a5Fi_R"
      },
      "source": [
        "Cada linha do dataset de treino (MSMARCO-tiny) possui 3 campos: query, exemplo positivo, exemplo negativo. Vamos desconsiderar os exemplos negativos e usar apenas os positivos. Para uma dada query, usamos os exemplos positivos de outras queries como negativo para a query avaliada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-dEj2t4Fh6J",
        "outputId": "e7a75888-c466-4b29-84dc-be7aad5d3b18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total treinamento 9900 9900\n",
            "Total validação 1100 1100\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Só faz o download se ainda não tiver feito\n",
        "if not Path('./collections/msmarco_triples.train.tiny.tsv').is_file():\n",
        "  !wget {url_ms_marco_treinamento} -P collections # type: ignore\n",
        "\n",
        "# Lê usando pandas\n",
        "msmarco_df = pd.read_csv(\"collections/msmarco_triples.train.tiny.tsv\", sep='\\t', names=['query', 'relevante', 'nao_relevante'], header=None)\n",
        "msmarco_train_df, msmarco_val_df = train_test_split(msmarco_df, test_size=0.1, random_state=42)\n",
        "\n",
        "# Separa os conjuntos de treinamento e validação\n",
        "queries_train = msmarco_train_df['query'].tolist()\n",
        "docs_train = msmarco_train_df['relevante'].tolist()\n",
        "queries_val = msmarco_val_df['query'].tolist()\n",
        "docs_val = msmarco_val_df['relevante'].tolist()\n",
        "\n",
        "print('Total treinamento', len(queries_train), len(docs_train))\n",
        "print('Total validação', len(queries_val), len(docs_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7oebY_0_y-8"
      },
      "source": [
        "Define os datasets e dataloaders:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "xzu-esgqFOv2"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from torch.utils import data\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BatchEncoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "b94xkxC-F5k2"
      },
      "outputs": [],
      "source": [
        "# Definição do Dataset\n",
        "class Dataset(data.Dataset):\n",
        "    # Recebe apenas um vetor de textos\n",
        "    def __init__(self, tokenizer, textos, max_seq_length = max_length):\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.tokenizer = tokenizer\n",
        "        self.textos = textos\n",
        "        self.cache = {}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.textos)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # Guarda os itens tokenizados num dict e apenas recupera de lá, pra não\n",
        "        # ter que ficar tokenizando a cada época\n",
        "        # Como estamos guardando no dict e idx é um slice, é necessário converter ele pra algo\n",
        "        # mapeável\n",
        "        self.cache[str(idx)] = self.cache.get(str(idx), \n",
        "                   self.tokenizer(self.textos[idx],\n",
        "                                  padding=True,\n",
        "                                  truncation=True,\n",
        "                                  max_length=self.max_seq_length\n",
        "                                  )\n",
        "                   )\n",
        "        return self.cache[str(idx)]\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "pITFVK5LF-Ov"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(nome_tokenizador)\n",
        "\n",
        "# Temos 2 datasets de cada tipo (train/val). Um pro encoder do documentos e outro pro encoder das queries\n",
        "# Datasets de treinamento\n",
        "dataset_queries_train = Dataset(tokenizer, queries_train)\n",
        "dataset_docs_train = Dataset(tokenizer, docs_train)\n",
        "\n",
        "# Datasets de validação\n",
        "dataset_queries_val = Dataset(tokenizer, queries_val)\n",
        "dataset_docs_val = Dataset(tokenizer, docs_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "fC1YppfOGXur"
      },
      "outputs": [],
      "source": [
        "# Dataloaders para os datasets\n",
        "\n",
        "#collate_fn = lambda batch: BatchEncoding(tokenizer.pad(batch, return_tensors='pt'))\n",
        "def collate_fn(batch):\n",
        "    #print('Dentro de collate_fn')\n",
        "    #print(BatchEncoding(tokenizer.pad(batch, return_tensors='pt')))\n",
        "    return BatchEncoding(tokenizer.pad(batch, return_tensors='pt'))\n",
        "\n",
        "dataloader_queries_train = DataLoader(dataset_queries_train, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "dataloader_docs_train = DataLoader(dataset_docs_train, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "dataloader_queries_val = DataLoader(dataset_queries_val, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "dataloader_docs_val = DataLoader(dataset_docs_val, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGouSrQgF-sn"
      },
      "source": [
        "Carrega os modelos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "FpWhDxhFGh2G"
      },
      "outputs": [],
      "source": [
        "# Agora vamos carregar dois modelos:\n",
        "import torch\n",
        "from transformers import AutoModel\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Se tiver que treinar os modelos, abre\n",
        "model_query = AutoModel.from_pretrained(nome_modelo_query).to(device)\n",
        "model_doc = AutoModel.from_pretrained(nome_modelo_doc).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xinVUAMdGnoa"
      },
      "source": [
        "Define função pro cálculo da loss:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "nE_Kk3ZeGp2L"
      },
      "outputs": [],
      "source": [
        "# Essa função já considera o resultado via batchs:\n",
        "def compute_loss_com_gradiente(model_query, tokenized_queries, model_doc, tokenized_docs):\n",
        "    outputs_queries = model_query(**tokenized_queries.to(device))\n",
        "    outputs_docs    = model_doc(**tokenized_docs.to(device))\n",
        "    \n",
        "    # Extrai a última camada oculta associada ao token [CLS]\n",
        "    tcls_queries = outputs_queries.last_hidden_state[:, 0, :]\n",
        "    tcls_docs    = outputs_docs.last_hidden_state[:, 0, :]\n",
        "    \n",
        "    # Normaliza os tensores\n",
        "    #tcls_queries = tcls_queries / torch.norm(tcls_queries, dim=1, keepdim=True)\n",
        "    #tcls_docs = tcls_docs / torch.norm(tcls_docs, dim=1, keepdim=True)\n",
        "    \n",
        "    # Agora é necessário calcular a loss. Para isso, o primeiro passo é\n",
        "    # calcular a similaridade entre uma query e documento (sim(q, d))\n",
        "    similaridade = torch.matmul(tcls_queries, torch.transpose(tcls_docs, 0, 1))\n",
        "\n",
        "    # Calcula a exponencial da similaridade\n",
        "    exp_sim = torch.exp(similaridade)\n",
        "    \n",
        "    # Calcula a loss\n",
        "    soma_linhas = exp_sim.sum(dim=1) # Isso é pro denominador, inclui os exemplos positivos e negativos\n",
        "    diagonal = torch.diag(exp_sim)\n",
        "    log_loss = -1* torch.log(diagonal/soma_linhas)\n",
        "    \n",
        "    loss = torch.mean(log_loss)\n",
        "    return loss\n",
        "\n",
        "def compute_loss_sem_gradiente(model_query, tokenized_queries, model_doc, tokenized_docs):\n",
        "    with torch.no_grad():\n",
        "        return compute_loss_com_gradiente(model_query, tokenized_queries, model_doc, tokenized_docs)\n",
        "\n",
        "def compute_loss_dataloaders(model_query, dataloader_query, model_doc, dataloader_docs):\n",
        "    loss = 0\n",
        "    n_batches = 0\n",
        "    for batch_query, batch_docs in zip(dataloader_query, dataloader_docs):\n",
        "        loss = loss + compute_loss_sem_gradiente(model_query, batch_query, model_doc, batch_docs)\n",
        "        n_batches += 1\n",
        "    return loss/n_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xTW39DGTuXY",
        "outputId": "fdac0113-0d0f-4ea1-b44a-a725cf8db11c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss de treinamento: 0.0008150112116709352\n",
            "Loss de validação: 0.04865604639053345\n",
            "CPU times: user 41.7 s, sys: 119 ms, total: 41.8 s\n",
            "Wall time: 42.1 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Só pra medir o tempo que ele demora para calcular a loss em todo o dataset de treinamento\n",
        "model_query.eval()\n",
        "model_doc.eval()\n",
        "print(f'Loss de treinamento: {compute_loss_dataloaders(model_query, dataloader_queries_train, model_doc, dataloader_docs_train)}')\n",
        "print(f'Loss de validação: {compute_loss_dataloaders(model_query, dataloader_queries_val, model_doc, dataloader_docs_val)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KST5dRqWGxff"
      },
      "source": [
        "Agora treina os dois encoders simulatenamente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrzMILg4GxDz",
        "outputId": "f6d1e64a-9455-417c-d24f-0903683fffd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 22 µs, sys: 1e+03 ns, total: 23 µs\n",
            "Wall time: 26.7 µs\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import get_linear_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup, AdamW\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "if treinar_e_salvar_modelos:\n",
        "  # Training loop\n",
        "  optimizer_query = AdamW(model_query.parameters(), lr=lr)\n",
        "  optimizer_doc = AdamW(model_doc.parameters(), lr=lr)\n",
        "\n",
        "  num_training_steps = epochs * len(dataloader_queries_train)\n",
        "  num_warmup_steps = int(num_training_steps * 0.1)\n",
        "\n",
        "  # get_linear_schedule_with_warmup get_cosine_with_hard_restarts_schedule_with_warmup\n",
        "  scheduler_query = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer_query, num_warmup_steps, num_training_steps)   \n",
        "  scheduler_doc = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer_doc, num_warmup_steps, num_training_steps)   \n",
        "\n",
        "  for epoch in tqdm(range(epochs), desc='Epochs'):\n",
        "      model_query.train()\n",
        "      model_doc.train()\n",
        "      \n",
        "      train_losses = []\n",
        "      for batch_query, batch_docs in tqdm(list(zip(dataloader_queries_train, dataloader_docs_train)), mininterval=0.5, desc='Train', disable=False):\n",
        "          optimizer_query.zero_grad()\n",
        "          optimizer_doc.zero_grad()\n",
        "          \n",
        "          loss = compute_loss_com_gradiente(model_query, batch_query, model_doc, batch_docs)\n",
        "          loss.backward()\n",
        "          \n",
        "          optimizer_query.step()\n",
        "          optimizer_doc.step()\n",
        "\n",
        "          scheduler_query.step()\n",
        "          scheduler_doc.step()\n",
        "      \n",
        "      model_query.save_pretrained(f'{dir_modelos}{epoch}/query/')\n",
        "      model_doc.save_pretrained(f'{dir_modelos}{epoch}/doc/')\n",
        "\n",
        "      model_query.eval()\n",
        "      model_doc.eval()\n",
        "      \n",
        "      print(f'Loss de treinamento {epoch}: {compute_loss_dataloaders(model_query, dataloader_queries_train, model_doc, dataloader_docs_train)}')\n",
        "      print(f'Loss de validação {epoch}: {compute_loss_dataloaders(model_query, dataloader_queries_val, model_doc, dataloader_docs_val)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sun4lP8KCAvs"
      },
      "source": [
        "## Pesquisa completa no TREC-COVID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kLOVQsjj25U"
      },
      "source": [
        "Treinado o modelo, agora vamos aplicá-lo ao TREC-COVID:.\n",
        "\n",
        "Coloca os modelos em modo eval:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "CNXUml6aRki7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77434a83-1a74-42ff-d34b-19813543c68d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 384, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 384)\n",
              "    (token_type_embeddings): Embedding(2, 384)\n",
              "    (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
              "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
              "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
              "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "model_query.eval()\n",
        "model_doc.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baixa o trec-covid:"
      ],
      "metadata": {
        "id": "qSzSdHkpqA7E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "x3t9H9Dyj8F0"
      },
      "outputs": [],
      "source": [
        "if not Path('./collections/trec-covid.zip').is_file():\n",
        "  !wget {url_trec_covid} -P collections # type: ignore\n",
        "  !unzip -o collections/trec-covid.zip -d ./collections # type: ignore\n",
        "\n",
        "# Converte o qrels que veio no trec-covid.zip pra o formato esperado:\n",
        "with open('./collections/trec-covid/qrels/test.tsv', 'r') as fin:\n",
        "  data = fin.read().splitlines(True)\n",
        "with open('./collections/trec-covid/qrels/test_corrigido.tsv', 'w') as fout:\n",
        "  for linha in data[1:]:\n",
        "    campos = linha.split()\n",
        "    fout.write(f'{campos[0]}\\t0\\t{campos[1]}\\t{campos[2]}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carrega as queries e os documentos:"
      ],
      "metadata": {
        "id": "D-d2PEMXqEWA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "755HjJD1k_x-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d92c1e8e-1f80-496a-ff19-3902c5ace133"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processado 0 documentos\n",
            "Processado 10000 documentos\n",
            "Processado 20000 documentos\n",
            "Processado 30000 documentos\n",
            "Processado 40000 documentos\n",
            "Processado 50000 documentos\n",
            "Processado 60000 documentos\n",
            "Processado 70000 documentos\n",
            "Processado 80000 documentos\n",
            "Processado 90000 documentos\n",
            "Processado 100000 documentos\n",
            "Processado 110000 documentos\n",
            "Processado 120000 documentos\n",
            "Processado 130000 documentos\n",
            "Processado 140000 documentos\n",
            "Processado 150000 documentos\n",
            "Processado 160000 documentos\n",
            "Processado 170000 documentos\n",
            "CPU times: user 1.83 s, sys: 102 ms, total: 1.93 s\n",
            "Wall time: 1.9 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import json\n",
        "\n",
        "def carrega_corpus_trec_covid():\n",
        "  retorno = []\n",
        "  with open('./collections/trec-covid/corpus.jsonl') as corpus:\n",
        "    for i, line in enumerate(corpus):\n",
        "      doc = json.loads(line)\n",
        "      #retorno.append({\n",
        "      #    'id': doc['_id'],\n",
        "      #    'doc': f\"{doc['title']} {doc['text']}\"\n",
        "      #})\n",
        "      retorno.append(\n",
        "          (doc['_id'], f\"{doc['title']} {doc['text']}\")\n",
        "      )\n",
        "      if (i % 10000 == 0):\n",
        "        print(f'Processado {i} documentos')\n",
        "    return retorno\n",
        "\n",
        "def carrega_queries_trec_covid():\n",
        "  retorno = []\n",
        "  with open('./collections/trec-covid/queries.jsonl') as queries:\n",
        "    for line in queries:\n",
        "      query = json.loads(line)\n",
        "      # Faz apenas uma pequena tradução de _id para id e text para texto\n",
        "      retorno.append({'id': query['_id'], 'texto': query['text']})\n",
        "  return retorno\n",
        "\n",
        "queries_trec_covid = carrega_queries_trec_covid()\n",
        "corpus_trec_covid = carrega_corpus_trec_covid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLTscG-JCLy7"
      },
      "source": [
        "A variável corpus_trec_covid contém os ids e os textos. Agora é necessário carregar a representação vetorial desses textos. Isso será feito gerando a matriz matriz_docs_trec_covid:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "pNsy7URQm4ws",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1e87ecc-be2b-47b3-fcb3-0bec1ffeb920"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([171332, 384])\n",
            "CPU times: user 215 ms, sys: 333 ms, total: 549 ms\n",
            "Wall time: 459 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "ids_trec_covid, textos_trec_covid = zip(*corpus_trec_covid)\n",
        "\n",
        "matriz_docs_trec_covid = None\n",
        "#textos_trec_covid = textos_trec_covid[0:20000]\n",
        "dataset_docs_trec_covid = Dataset(tokenizer, textos_trec_covid)\n",
        "dataloader_docs_trec_covid = DataLoader(dataset_docs_trec_covid, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "if gerar_e_salvar_matriz_docs_trec_covid:\n",
        "  with torch.no_grad():\n",
        "    for batch in tqdm(dataloader_docs_trec_covid, mininterval=0.5, desc='Convertendo documentos trec-covid', disable=False):\n",
        "      outputs_docs = model_doc(**batch.to(device))\n",
        "      tcls_docs    = outputs_docs.last_hidden_state[:, 0, :]\n",
        "      #tcls_docs    = tcls_docs / torch.norm(tcls_docs, dim=1, keepdim=True)\n",
        "      # Monta a matriz de documentos na CPU\n",
        "      tcls_docs = tcls_docs.to(\"cpu\")\n",
        "\n",
        "      if matriz_docs_trec_covid is None:\n",
        "        matriz_docs_trec_covid = tcls_docs\n",
        "      else:\n",
        "        matriz_docs_trec_covid = torch.cat( (matriz_docs_trec_covid, tcls_docs), dim=0)\n",
        "\n",
        "  # Agora volta a matriz pra GPU pq ela cabe lá, não sei pq estava estourando a memória antes...\n",
        "  matriz_docs_trec_covid = matriz_docs_trec_covid.to(device)\n",
        "  torch.save(matriz_docs_trec_covid, arquivo_matriz_docs_trec_covid)\n",
        "else:\n",
        "  matriz_docs_trec_covid = torch.load(arquivo_matriz_docs_trec_covid).to(device)\n",
        "\n",
        "print(matriz_docs_trec_covid.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBFvzp3BrD0X"
      },
      "source": [
        "Agora vamos definir um método pra calcular a representação vetorial da query e para fazer a pesquisa na base de dados. O método de pesquisa só retorna o vetor de score pareado com as ids contidas na variável ids_trec_covid:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "OIQ2VNrdq_kH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02c7c679-e70b-4b5d-87c7-4e8bc790a093"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([384])\n"
          ]
        }
      ],
      "source": [
        "def get_vetor_query(query):\n",
        "  query_tokenizada = tokenizer(query, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
        "\n",
        "  with torch.no_grad():\n",
        "    output_query = model_query(**query_tokenizada.to(device))\n",
        "    tcls_query    = output_query.last_hidden_state[:, 0, :]\n",
        "    # tcls_query    = tcls_query / torch.norm(tcls_query, dim=1, keepdim=True)\n",
        "\n",
        "  return tcls_query[0]\n",
        "\n",
        "vetor_query = get_vetor_query('what is this?')\n",
        "\n",
        "print(vetor_query.size())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "1F9iF_6KsB-O"
      },
      "outputs": [],
      "source": [
        "def calcula_score_documentos_para_a_query(matriz_docs, query):\n",
        "  vetor_query = get_vetor_query(query)\n",
        "  score = torch.matmul(matriz_docs, vetor_query)\n",
        "\n",
        "  return score\n",
        "\n",
        "def pesquisa_query_e_retorna_n_primeiros_docs(matriz_docs, ids_docs_na_matriz, query, n=1000):\n",
        "  # Calcula o score\n",
        "  score = calcula_score_documentos_para_a_query(matriz_docs, query)\n",
        "  # Ordena\n",
        "  sorted_score, indices_score = torch.sort(score, descending=True)\n",
        "  # Pega só os n primeiros\n",
        "  sorted_score = sorted_score[0:n]\n",
        "  indices_score = indices_score[0:n]\n",
        "  # Extrai os ids dos documentos\n",
        "  ids_docs = [ids_docs_na_matriz[i] for i in indices_score]\n",
        "\n",
        "  return zip(ids_docs, sorted_score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora roda todas as queries para avaliação..."
      ],
      "metadata": {
        "id": "8B56kNR_wfCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Roda todas as queries\n",
        "def run_all_queries(file):\n",
        "  print('Carregando as queries do arquivo queries.jsonl...\\n')\n",
        "  queries_trec_covid = carrega_queries_trec_covid()\n",
        "\n",
        "  print(f'Total de queries que serão avaliadas: {len(queries_trec_covid)}')\n",
        "  cnt = 0\n",
        "  with open(file, 'w') as runfile:\n",
        "    for query in queries_trec_covid:\n",
        "      id_query = query['id']\n",
        "      texto = query['texto']\n",
        "\n",
        "      if cnt % 5 == 0:\n",
        "        print(f'{cnt} queries completadas')\n",
        "\n",
        "      # Pega os primeiros 1000 resultados\n",
        "      docs_score = pesquisa_query_e_retorna_n_primeiros_docs(matriz_docs_trec_covid, ids_trec_covid, texto, n=1000)\n",
        "      \n",
        "      i = 0\n",
        "      for id_doc, score in docs_score:\n",
        "        _ = runfile.write('{} Q0 {} {} {:.6f} Pesquisa_densa\\n'.format(id_query, id_doc, i+1, float(score)))\n",
        "        i += 1\n",
        "\n",
        "      cnt += 1\n",
        "      # break # Pra testar, gera só a primeira query\n",
        "    print(f'{cnt} queries completadas')\n",
        "    "
      ],
      "metadata": {
        "id": "C6PFulWMsgX9"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "E calcula o nDCG@10:"
      ],
      "metadata": {
        "id": "5lVWkSeBwiNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_all_queries('run-pesquisa-densa.txt')\n",
        "\n",
        "!python -m pyserini.eval.trec_eval -c -m ndcg_cut.10 collections/trec-covid/qrels/test_corrigido.tsv run-pesquisa-densa.txt #type: ignore"
      ],
      "metadata": {
        "id": "wuDvWPs8wYCX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b8b432e-d669-400d-b0ba-6c1e79543175"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando as queries do arquivo queries.jsonl...\n",
            "\n",
            "Total de queries que serão avaliadas: 50\n",
            "0 queries completadas\n",
            "5 queries completadas\n",
            "10 queries completadas\n",
            "15 queries completadas\n",
            "20 queries completadas\n",
            "25 queries completadas\n",
            "30 queries completadas\n",
            "35 queries completadas\n",
            "40 queries completadas\n",
            "45 queries completadas\n",
            "50 queries completadas\n",
            "Downloading https://search.maven.org/remotecontent?filepath=uk/ac/gla/dcs/terrierteam/jtreceval/0.0.5/jtreceval-0.0.5-jar-with-dependencies.jar to /root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar...\n",
            "/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar already exists!\n",
            "Skipping download.\n",
            "Running command: ['java', '-jar', '/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar', '-c', '-m', 'ndcg_cut.10', 'collections/trec-covid/qrels/test_corrigido.tsv', 'run-pesquisa-densa.txt']\n",
            "Results:\n",
            "ndcg_cut_10           \tall\t0.3322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pesquisa aproximada (usando k-means)"
      ],
      "metadata": {
        "id": "zX2vUHn-KKQx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos pegar a matriz de documentos e criar 10 clusters para eles. E treinar um kmeans:"
      ],
      "metadata": {
        "id": "1mNTBy1rjAlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "# Primeiro vamos transformar a matriz de documentos para uma matriz numpy:\n",
        "matriz_docs_np = matriz_docs_trec_covid.to('cpu').numpy()\n",
        "matriz_docs_trec_covid.to(device)\n",
        "\n",
        "# Cálculo dos centróides:\n",
        "n_clusters = 10\n",
        "kmeans = KMeans(n_clusters=n_clusters, init='k-means++', max_iter=300, n_init=10, random_state=42)\n",
        "classe_doc = kmeans.fit_predict(matriz_docs_np)\n"
      ],
      "metadata": {
        "id": "KgxGXwvvLOtH"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora vamos separar os índices dos documentos nesses 10 clusters e deixar em um dict só pra facilitar as coisas depois:"
      ],
      "metadata": {
        "id": "MlNGoj0Uj7Hs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indices_por_cluster = {}\n",
        "\n",
        "for classe in range(0, n_clusters):\n",
        "  indices_por_cluster[classe] = {\n",
        "      'indice': [indice for indice, classe_doc in enumerate(classe_doc) if classe == classe_doc],\n",
        "      'ids_doc': [ids_trec_covid[indice] for indice, classe_doc in enumerate(classe_doc) if classe == classe_doc]\n",
        "  }\n",
        "  print(f'Total docs na classe {classe} = {len(indices_por_cluster[classe][\"indice\"])}')\n",
        "\n",
        "print(indices_por_cluster[0]['indice'][0:3])\n",
        "print(indices_por_cluster[0]['ids_doc'][0:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZ9NFEmdiveS",
        "outputId": "b09781bf-0ed7-4be7-a1da-f341d2bb5e3e"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total docs na classe 0 = 28914\n",
            "Total docs na classe 1 = 12221\n",
            "Total docs na classe 2 = 24366\n",
            "Total docs na classe 3 = 22542\n",
            "Total docs na classe 4 = 11836\n",
            "Total docs na classe 5 = 11859\n",
            "Total docs na classe 6 = 22447\n",
            "Total docs na classe 7 = 8259\n",
            "Total docs na classe 8 = 15198\n",
            "Total docs na classe 9 = 13690\n",
            "[2, 4, 5]\n",
            "['ejv2xln0', '9785vg6d', 'zjufx4fo']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos definir um método pra, dada uma query (um texto), retornar qual é a classe de documentos que está mais próxima dessa query. Ou seja, esse método extrai o vetor que representa a query e calcula a distância entre esse vetor e o vetor do centro de cada cluster:"
      ],
      "metadata": {
        "id": "75hiWt7Wk8Mf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_classe_mais_proxima(query):\n",
        "  vetor_query = get_vetor_query(query).to('cpu').numpy()\n",
        "  \n",
        "  classe_mais_proxima = -1\n",
        "  dist_mais_proxima = 1e10\n",
        "  for classe, vetor_classe in enumerate(kmeans.cluster_centers_):\n",
        "    dist = np.linalg.norm(vetor_query - vetor_classe)\n",
        "    if dist < dist_mais_proxima:\n",
        "      dist_mais_proxima = dist\n",
        "      classe_mais_proxima = classe\n",
        "  \n",
        "  return classe_mais_proxima\n",
        "\n",
        "get_classe_mais_proxima('What is this?')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJLK2TH2lEvO",
        "outputId": "63eeced8-8c49-464e-900a-be4fff5afd99"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora um método que faz a pesquisa apenas nos documentos que estão no cluster mais próximo.\n",
        "\n",
        "Como já separamos os ids dos documentos por cluster dentro de indices_por_cluster[classe_mais_proxima]['ids_doc'], o que precisamos agora é só filtrar a matriz matriz_docs_trec_covid pra considerar apenas os documentos equivalentes a indices_por_cluster[classe_mais_proxima]['ids_doc']. Feito isso, basta passar pro método de pesquisa geral:"
      ],
      "metadata": {
        "id": "OsEQAkP_y3XF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pesquisa_query_na_classe_mais_proxima_e_retorna_n_primeiros_docs(query, n=1000):\n",
        "  classe_mais_proxima = get_classe_mais_proxima(query)\n",
        "  indices_na_classe = indices_por_cluster[classe_mais_proxima]['indice']\n",
        "\n",
        "  matriz_filtrada = matriz_docs_trec_covid[torch.LongTensor(indices_na_classe).to(torch.int)]\n",
        "  ids_docs_filtrados = indices_por_cluster[classe_mais_proxima]['ids_doc']\n",
        " \n",
        "  return pesquisa_query_e_retorna_n_primeiros_docs(matriz_filtrada, ids_docs_filtrados, query, n)\n"
      ],
      "metadata": {
        "id": "ixN60X_DmsUl"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "for i in pesquisa_query_na_classe_mais_proxima_e_retorna_n_primeiros_docs('teste', 10):\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-QVU9NWn2hH",
        "outputId": "16d40b64-9f92-4f38-f5f9-2dfcf7ee5779"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('v6nnnzhm', tensor(61.4928, device='cuda:0'))\n",
            "('crtvxdn6', tensor(58.7026, device='cuda:0'))\n",
            "('iryg7xw0', tensor(57.4697, device='cuda:0'))\n",
            "('kyae3q05', tensor(57.1972, device='cuda:0'))\n",
            "('1qhbucb0', tensor(57.0405, device='cuda:0'))\n",
            "('zlx4etkg', tensor(55.8098, device='cuda:0'))\n",
            "('1x3da6ko', tensor(55.6850, device='cuda:0'))\n",
            "('4oabmb1a', tensor(55.6653, device='cuda:0'))\n",
            "('4cpiemq5', tensor(55.3849, device='cuda:0'))\n",
            "('g6sfaa2m', tensor(55.3795, device='cuda:0'))\n",
            "CPU times: user 43.3 ms, sys: 3.05 ms, total: 46.4 ms\n",
            "Wall time: 44.6 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como o run_all_queries tinha sido definido pesquisando apenas em todos os documentos, vamos replicar ele aqui mas agora mandando pesquisar apenas na classe mais próxima:"
      ],
      "metadata": {
        "id": "Q1aKFYzNzOqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Roda todas as queries\n",
        "def run_all_queries_classe_proxima(file):\n",
        "  print('Carregando as queries do arquivo queries.jsonl...\\n')\n",
        "  queries_trec_covid = carrega_queries_trec_covid()\n",
        "\n",
        "  print(f'Total de queries que serão avaliadas: {len(queries_trec_covid)}')\n",
        "  cnt = 0\n",
        "  with open(file, 'w') as runfile:\n",
        "    for query in queries_trec_covid:\n",
        "      id_query = query['id']\n",
        "      texto = query['texto']\n",
        "\n",
        "      if cnt % 5 == 0:\n",
        "        print(f'{cnt} queries completadas')\n",
        "\n",
        "      # Pega os primeiros 1000 resultados\n",
        "      docs_score = pesquisa_query_na_classe_mais_proxima_e_retorna_n_primeiros_docs(texto, n=1000)\n",
        "      \n",
        "      i = 0\n",
        "      for id_doc, score in docs_score:\n",
        "        _ = runfile.write('{} Q0 {} {} {:.6f} Pesquisa_densa\\n'.format(id_query, id_doc, i+1, float(score)))\n",
        "        i += 1\n",
        "\n",
        "      cnt += 1\n",
        "      # break # Pra testar, gera só a primeira query\n",
        "    print(f'{cnt} queries completadas')\n",
        "    "
      ],
      "metadata": {
        "id": "8pyumrgunpo3"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por fim, roda a pesquisa e checa o nDCG@10:"
      ],
      "metadata": {
        "id": "r_zCDCbBz2Ik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_all_queries_classe_proxima('run-pesquisa-densa-classe-proxima.txt')\n",
        "\n",
        "!python -m pyserini.eval.trec_eval -c -m ndcg_cut.10 collections/trec-covid/qrels/test_corrigido.tsv run-pesquisa-densa-classe-proxima.txt #type: ignore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJItv_npnwYT",
        "outputId": "6a3f2998-3733-417f-944d-59367605bf15"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando as queries do arquivo queries.jsonl...\n",
            "\n",
            "Total de queries que serão avaliadas: 50\n",
            "0 queries completadas\n",
            "5 queries completadas\n",
            "10 queries completadas\n",
            "15 queries completadas\n",
            "20 queries completadas\n",
            "25 queries completadas\n",
            "30 queries completadas\n",
            "35 queries completadas\n",
            "40 queries completadas\n",
            "45 queries completadas\n",
            "50 queries completadas\n",
            "Downloading https://search.maven.org/remotecontent?filepath=uk/ac/gla/dcs/terrierteam/jtreceval/0.0.5/jtreceval-0.0.5-jar-with-dependencies.jar to /root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar...\n",
            "/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar already exists!\n",
            "Skipping download.\n",
            "Running command: ['java', '-jar', '/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar', '-c', '-m', 'ndcg_cut.10', 'collections/trec-covid/qrels/test_corrigido.tsv', 'run-pesquisa-densa-classe-proxima.txt']\n",
            "Results:\n",
            "ndcg_cut_10           \tall\t0.2991\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usando sentence transformers"
      ],
      "metadata": {
        "id": "JwUf5ldnwemk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "model = SentenceTransformer('all-mpnet-base-v2').to(device)\n",
        "\n",
        "if gerar_arquivo_doc_embeddings_all_mpnet_base_v2:\n",
        "  docs_embeddings = model.encode(textos_trec_covid)\n",
        "  torch.save(docs_embeddings, arq_doc_embeddings_all_mpnet_base_v2)\n",
        "else:\n",
        "  docs_embeddings = torch.load(arq_doc_embeddings_all_mpnet_base_v2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sUaIc6Twg_V",
        "outputId": "d2cc2c93-5577-4ea6-d8c7-4b8569db621b"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 5.13 s, sys: 2.45 s, total: 7.58 s\n",
            "Wall time: 7.59 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calcula_score_documentos_para_a_query_sentence_transformers(model, docs_embeddings, query):\n",
        "  query_embedding = model.encode(query)\n",
        "  return util.dot_score(query_embedding, docs_embeddings)\n",
        "\n",
        "def pesquisa_query_e_retorna_n_primeiros_docs_sentence_transformers(model, docs_embeddings, query, n=1000):\n",
        "  # Calcula o score\n",
        "  score = calcula_score_documentos_para_a_query_sentence_transformers(model, docs_embeddings, query)\n",
        "  # Ordena\n",
        "  sorted_score, indices_score = torch.sort(score, descending=True)\n",
        "  # Pega só os n primeiros\n",
        "  sorted_score = sorted_score[0, 0:n]\n",
        "  indices_score = indices_score[0, 0:n]\n",
        "  # Extrai os ids dos documentos\n",
        "  ids_docs = [ids_trec_covid[i] for i in indices_score]\n",
        "\n",
        "  return zip(ids_docs, sorted_score)\n",
        "\n",
        "# Roda todas as queries\n",
        "def run_all_queries_sentence_transformer(model, docs_embeddings, file):\n",
        "  print('Carregando as queries do arquivo queries.jsonl...\\n')\n",
        "  queries_trec_covid = carrega_queries_trec_covid()\n",
        "\n",
        "  print(f'Total de queries que serão avaliadas: {len(queries_trec_covid)}')\n",
        "  cnt = 0\n",
        "  with open(file, 'w') as runfile:\n",
        "    for query in queries_trec_covid:\n",
        "      id_query = query['id']\n",
        "      texto = query['texto']\n",
        "\n",
        "      if cnt % 5 == 0:\n",
        "        print(f'{cnt} queries completadas')\n",
        "\n",
        "      # Pega os primeiros 1000 resultados\n",
        "      docs_score = pesquisa_query_e_retorna_n_primeiros_docs_sentence_transformers(model, docs_embeddings, texto, n=1000)\n",
        "      \n",
        "      i = 0\n",
        "      for id_doc, score in docs_score:\n",
        "        _ = runfile.write('{} Q0 {} {} {:.6f} Pesquisa_densa\\n'.format(id_query, id_doc, i+1, float(score)))\n",
        "        i += 1\n",
        "\n",
        "      cnt += 1\n",
        "      # break # Pra testar, gera só a primeira query\n",
        "    print(f'{cnt} queries completadas')\n",
        "    \n",
        "run_all_queries_sentence_transformer(model, docs_embeddings, 'run-pesquisa-densa-sentence-transformer-all-mpnet-base-v2.txt')\n",
        "!python -m pyserini.eval.trec_eval -c -m ndcg_cut.10 collections/trec-covid/qrels/test_corrigido.tsv run-pesquisa-densa-sentence-transformer-all-mpnet-base-v2.txt #type: ignore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2t_mKrUxs3_",
        "outputId": "b58e72be-54cc-4780-c06e-4a9df60b82e3"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando as queries do arquivo queries.jsonl...\n",
            "\n",
            "Total de queries que serão avaliadas: 50\n",
            "0 queries completadas\n",
            "5 queries completadas\n",
            "10 queries completadas\n",
            "15 queries completadas\n",
            "20 queries completadas\n",
            "25 queries completadas\n",
            "30 queries completadas\n",
            "35 queries completadas\n",
            "40 queries completadas\n",
            "45 queries completadas\n",
            "50 queries completadas\n",
            "Downloading https://search.maven.org/remotecontent?filepath=uk/ac/gla/dcs/terrierteam/jtreceval/0.0.5/jtreceval-0.0.5-jar-with-dependencies.jar to /root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar...\n",
            "/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar already exists!\n",
            "Skipping download.\n",
            "Running command: ['java', '-jar', '/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar', '-c', '-m', 'ndcg_cut.10', 'collections/trec-covid/qrels/test_corrigido.tsv', 'run-pesquisa-densa-sentence-transformer-all-mpnet-base-v2.txt']\n",
            "Results:\n",
            "ndcg_cut_10           \tall\t0.5133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "model = SentenceTransformer('all-MiniLM-L12-v2').to(device)\n",
        "\n",
        "if gerar_arquivo_doc_embeddings_all_MiniLM_L12_v2:\n",
        "  docs_embeddings = model.encode(textos_trec_covid)\n",
        "  torch.save(docs_embeddings, arq_doc_embeddings_all_MiniLM_L12_v2)\n",
        "else:\n",
        "  docs_embeddings = torch.load(arq_doc_embeddings_all_MiniLM_L12_v2)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUpCtFUckYex",
        "outputId": "a923d05a-edf3-4206-e2b9-a123cd852e8b"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2.38 s, sys: 1.05 s, total: 3.43 s\n",
            "Wall time: 3.4 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "run_all_queries_sentence_transformer(model, docs_embeddings, 'run-pesquisa-densa-sentence-transformer-all-MiniLM-L12-v2.txt')\n",
        "!python -m pyserini.eval.trec_eval -c -m ndcg_cut.10 collections/trec-covid/qrels/test_corrigido.tsv run-pesquisa-densa-sentence-transformer-all-MiniLM-L12-v2.txt #type: ignore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-8ses99k0wd",
        "outputId": "3cb53009-158a-484a-cbe6-f188ef8b75f5"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando as queries do arquivo queries.jsonl...\n",
            "\n",
            "Total de queries que serão avaliadas: 50\n",
            "0 queries completadas\n",
            "5 queries completadas\n",
            "10 queries completadas\n",
            "15 queries completadas\n",
            "20 queries completadas\n",
            "25 queries completadas\n",
            "30 queries completadas\n",
            "35 queries completadas\n",
            "40 queries completadas\n",
            "45 queries completadas\n",
            "50 queries completadas\n",
            "Downloading https://search.maven.org/remotecontent?filepath=uk/ac/gla/dcs/terrierteam/jtreceval/0.0.5/jtreceval-0.0.5-jar-with-dependencies.jar to /root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar...\n",
            "/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar already exists!\n",
            "Skipping download.\n",
            "Running command: ['java', '-jar', '/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar', '-c', '-m', 'ndcg_cut.10', 'collections/trec-covid/qrels/test_corrigido.tsv', 'run-pesquisa-densa-sentence-transformer-all-MiniLM-L12-v2.txt']\n",
            "Results:\n",
            "ndcg_cut_10           \tall\t0.5082\n",
            "CPU times: user 7.42 s, sys: 7.18 s, total: 14.6 s\n",
            "Wall time: 13.4 s\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}