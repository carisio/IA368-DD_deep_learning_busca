{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcqFuVMQ94KA"
      },
      "source": [
        "# Aula 6 - Solução dos exercícios\n",
        "Leandro Carísio Fernandes\n",
        "\n",
        "<br>\n",
        "\n",
        "Iremos fazer finetuning de um buscador denso\n",
        "\n",
        "Usar como treino o dataset \"tiny\" do MS MARCO\n",
        "https://storage.googleapis.com/unicamp-dl/ia368dd_2023s1/msmarco/msmarco_triples.train.tiny.tsv\n",
        "\n",
        "Avaliar o modelo no TREC-COVID, e comparar os resultados com o BM25 e doc2query\n",
        "\n",
        "Comparar busca \"exaustiva\" (semelhança do vetor query com todos os vetores do corpus) com a busca aproximada (Approximate Nearest Neighbor - ANN)\n",
        "\n",
        "Para a busca aproximada, usar os algoritmos existentes na biblioteca sentence-transformers (ex: hnswlib) OU implemente um você mesmo (Bonus!)\n",
        "\n",
        "Dicas:\n",
        "\n",
        "- Usar a média dos vetores da última camada (conhecido como mean pooling) do transformer para representar queries e passagens; Alternativamente, usar apenas o vetor do [CLS] da última cada.\n",
        "\n",
        "- Tente inicialmente uma loss facil de implementar, como a entropia-cruzada\n",
        "\n",
        "- Começar o treino a partir do microsoft/MiniLM-L12-H384-uncased\n",
        "\n",
        "- Avaliar o pipeline usando um modelo já bem treinado: sentence-transformers/all-mpnet-base-v2\n",
        "\n",
        "- Comparar resultados usando semelhança de coseno e produto escalar como funções de similaridade\n",
        "\n",
        "- Para checar se seu codigo de avaliação está correto, comparar o seu desempenho com o do modelo já treinado no MS MARCO: https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2; O nDCG@10 no TREC-COVID deve ser ~0.47\n",
        "\n",
        "- Usar a biblioteca do sentence-transformers para avaliar o modelo\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz4tGFdv-VEJ"
      },
      "source": [
        "## Preparação do ambiente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckAfaNIp-Xks"
      },
      "source": [
        "### Variáveis para controlar o fluxo do caderno\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "TMQ9Lro1GMKi"
      },
      "outputs": [],
      "source": [
        "treinar_e_salvar_modelos = False\n",
        "\n",
        "gerar_e_salvar_matriz_docs_trec_covid = False\n",
        "\n",
        "# Local onde fica o arquivo que contém a matriz de todos os documentos do trec_covid. Se gerar_e_salvar_matriz_docs_trec_covid = True, esse arquivo será sobrescrito com a nova geração\n",
        "arquivo_matriz_docs_trec_covid = '/content/drive/My Drive/IA368-DD_deep_learning_busca/Aula6-buscador-denso/matriz_docs_trec_covid_unitario.pt'\n",
        "\n",
        "# Diretório onde vai salvando o modelo a cada época\n",
        "dir_modelos = '/content/drive/My Drive/IA368-DD_deep_learning_busca/Aula6-buscador-denso/modelos_unitario/'\n",
        "\n",
        "# Nome dos modelos e tokenizador. São esses modelos que serão carregados no início com o from_pretrained.\n",
        "# Se quiser iniciar um treinamento do 0, é necessário substituir por \"microsoft/MiniLM-L12-H384-uncased\"\n",
        "nome_modelo_query = '/content/drive/My Drive/IA368-DD_deep_learning_busca/Aula6-buscador-denso/modelos_unitario/final/query/' #nome_modelo_query = \"microsoft/MiniLM-L12-H384-uncased\"\n",
        "nome_modelo_doc = '/content/drive/My Drive/IA368-DD_deep_learning_busca/Aula6-buscador-denso/modelos_unitario/final/doc/' #nome_modelo_doc = \"microsoft/MiniLM-L12-H384-uncased\"\n",
        "nome_tokenizador = \"microsoft/MiniLM-L12-H384-uncased\"\n",
        "\n",
        "\n",
        "url_ms_marco_treinamento = \"https://storage.googleapis.com/unicamp-dl/ia368dd_2023s1/msmarco/msmarco_triples.train.tiny.tsv\"\n",
        "url_trec_covid = 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/trec-covid.zip'\n",
        "\n",
        "max_length = 256 \n",
        "batch_size = 32\n",
        "epochs = 20\n",
        "lr = 1e-5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM6qeJFz-3Nl"
      },
      "source": [
        "### Instalação de libs e montagem do Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_dJy9vSFF4l",
        "outputId": "86e64866-bf82-4084-aeee-6b5cfd4e81c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Já monta o drive, pois vamos usar o índice invertido da Aula 1 para usar o BM25 implementado também na aula 1\n",
        "# Além disso, é necessário para salvar/recuperar o modelo tunado\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install transformers datasets -q\n",
        "!pip install sentence-transformers -q\n",
        "!pip install pyserini -q\n",
        "!pip install faiss-gpu -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aq9LmOj4Fi4x"
      },
      "source": [
        "## Fine-tuning dos encoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJM5c2a5Fi_R"
      },
      "source": [
        "Cada linha do dataset de treino (MSMARCO-tiny) possui 3 campos: query, exemplo positivo, exemplo negativo. Vamos desconsiderar os exemplos negativos e usar apenas os positivos. Para uma dada query, usamos os exemplos positivos de outras queries como negativo para a query avaliada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-dEj2t4Fh6J",
        "outputId": "0329c3e0-adf1-4b06-cc84-7bdd4a6cfb3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total treinamento 9900 9900\n",
            "Total validação 1100 1100\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Só faz o download se ainda não tiver feito\n",
        "if not Path('./collections/msmarco_triples.train.tiny.tsv').is_file():\n",
        "  !wget {url_ms_marco_treinamento} -P collections # type: ignore\n",
        "\n",
        "# Lê usando pandas\n",
        "msmarco_df = pd.read_csv(\"collections/msmarco_triples.train.tiny.tsv\", sep='\\t', names=['query', 'relevante', 'nao_relevante'], header=None)\n",
        "msmarco_train_df, msmarco_val_df = train_test_split(msmarco_df, test_size=0.1, random_state=42)\n",
        "\n",
        "# Separa os conjuntos de treinamento e validação\n",
        "queries_train = msmarco_train_df['query'].tolist()\n",
        "docs_train = msmarco_train_df['relevante'].tolist()\n",
        "queries_val = msmarco_val_df['query'].tolist()\n",
        "docs_val = msmarco_val_df['relevante'].tolist()\n",
        "\n",
        "print('Total treinamento', len(queries_train), len(docs_train))\n",
        "print('Total validação', len(queries_val), len(docs_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7oebY_0_y-8"
      },
      "source": [
        "Define os datasets e dataloaders:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "xzu-esgqFOv2"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from torch.utils import data\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BatchEncoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "b94xkxC-F5k2"
      },
      "outputs": [],
      "source": [
        "# Definição do Dataset\n",
        "class Dataset(data.Dataset):\n",
        "    # Recebe apenas um vetor de textos\n",
        "    def __init__(self, tokenizer, textos, max_seq_length = max_length):\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.tokenizer = tokenizer\n",
        "        self.textos = textos\n",
        "        self.cache = {}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.textos)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # Guarda os itens tokenizados num dict e apenas recupera de lá, pra não\n",
        "        # ter que ficar tokenizando a cada época\n",
        "        # Como estamos guardando no dict e idx é um slice, é necessário converter ele pra algo\n",
        "        # mapeável\n",
        "        self.cache[str(idx)] = self.cache.get(str(idx), \n",
        "                   self.tokenizer(self.textos[idx],\n",
        "                                  padding=True,\n",
        "                                  truncation=True,\n",
        "                                  max_length=self.max_seq_length\n",
        "                                  )\n",
        "                   )\n",
        "        return self.cache[str(idx)]\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "pITFVK5LF-Ov"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(nome_tokenizador)\n",
        "\n",
        "# Temos 2 datasets de cada tipo (train/val). Um pro encoder do documentos e outro pro encoder das queries\n",
        "# Datasets de treinamento\n",
        "dataset_queries_train = Dataset(tokenizer, queries_train)\n",
        "dataset_docs_train = Dataset(tokenizer, docs_train)\n",
        "\n",
        "# Datasets de validação\n",
        "dataset_queries_val = Dataset(tokenizer, queries_val)\n",
        "dataset_docs_val = Dataset(tokenizer, docs_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "fC1YppfOGXur"
      },
      "outputs": [],
      "source": [
        "# Dataloaders para os datasets\n",
        "\n",
        "#collate_fn = lambda batch: BatchEncoding(tokenizer.pad(batch, return_tensors='pt'))\n",
        "def collate_fn(batch):\n",
        "    #print('Dentro de collate_fn')\n",
        "    #print(BatchEncoding(tokenizer.pad(batch, return_tensors='pt')))\n",
        "    return BatchEncoding(tokenizer.pad(batch, return_tensors='pt'))\n",
        "\n",
        "dataloader_queries_train = DataLoader(dataset_queries_train, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "dataloader_docs_train = DataLoader(dataset_docs_train, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "dataloader_queries_val = DataLoader(dataset_queries_val, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "dataloader_docs_val = DataLoader(dataset_docs_val, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGouSrQgF-sn"
      },
      "source": [
        "Carrega os modelos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "FpWhDxhFGh2G"
      },
      "outputs": [],
      "source": [
        "# Agora vamos carregar dois modelos:\n",
        "import torch\n",
        "from transformers import AutoModel\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Se tiver que treinar os modelos, abre\n",
        "model_query = AutoModel.from_pretrained(nome_modelo_query).to(device)\n",
        "model_doc = AutoModel.from_pretrained(nome_modelo_doc).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xinVUAMdGnoa"
      },
      "source": [
        "Define função pro cálculo da loss:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "nE_Kk3ZeGp2L"
      },
      "outputs": [],
      "source": [
        "# Essa função já considera o resultado via batchs:\n",
        "def compute_loss_com_gradiente(model_query, tokenized_queries, model_doc, tokenized_docs):\n",
        "    outputs_queries = model_query(**tokenized_queries.to(device))\n",
        "    outputs_docs    = model_doc(**tokenized_docs.to(device))\n",
        "    \n",
        "    # Extrai a última camada oculta associada ao token [CLS]\n",
        "    tcls_queries = outputs_queries.last_hidden_state[:, 0, :]\n",
        "    tcls_docs    = outputs_docs.last_hidden_state[:, 0, :]\n",
        "    \n",
        "    # Normaliza os tensores\n",
        "    tcls_queries = tcls_queries / torch.norm(tcls_queries, dim=1, keepdim=True)\n",
        "    tcls_docs = tcls_docs / torch.norm(tcls_docs, dim=1, keepdim=True)\n",
        "    \n",
        "    # Agora é necessário calcular a loss. Para isso, o primeiro passo é\n",
        "    # calcular a similaridade entre uma query e documento (sim(q, d))\n",
        "    similaridade = torch.matmul(tcls_queries, torch.transpose(tcls_docs, 0, 1))\n",
        "\n",
        "    # Calcula a exponencial da similaridade\n",
        "    exp_sim = torch.exp(similaridade/0.02)\n",
        "    \n",
        "    # Calcula a loss\n",
        "    soma_linhas = exp_sim.sum(dim=1) # Isso é pro denominador, inclui os exemplos positivos e negativos\n",
        "    diagonal = torch.diag(exp_sim)\n",
        "    log_loss = -1* torch.log(diagonal/soma_linhas)\n",
        "    \n",
        "    loss = torch.mean(log_loss)\n",
        "    return loss\n",
        "\n",
        "def compute_loss_sem_gradiente(model_query, tokenized_queries, model_doc, tokenized_docs):\n",
        "    with torch.no_grad():\n",
        "        return compute_loss_com_gradiente(model_query, tokenized_queries, model_doc, tokenized_docs)\n",
        "\n",
        "def compute_loss_dataloaders(model_query, dataloader_query, model_doc, dataloader_docs):\n",
        "    loss = 0\n",
        "    n_batches = 0\n",
        "    for batch_query, batch_docs in zip(dataloader_query, dataloader_docs):\n",
        "        loss = loss + compute_loss_sem_gradiente(model_query, batch_query, model_doc, batch_docs)\n",
        "        n_batches += 1\n",
        "    return loss/n_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xTW39DGTuXY",
        "outputId": "d5831211-411c-4940-bbbf-04e52b45d103"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss de treinamento: 0.0002726383681874722\n",
            "Loss de validação: 0.07193975150585175\n",
            "CPU times: user 35.6 s, sys: 66.8 ms, total: 35.7 s\n",
            "Wall time: 35.9 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Só pra medir o tempo que ele demora para calcular a loss em todo o dataset de treinamento\n",
        "model_query.eval()\n",
        "model_doc.eval()\n",
        "print(f'Loss de treinamento: {compute_loss_dataloaders(model_query, dataloader_queries_train, model_doc, dataloader_docs_train)}')\n",
        "print(f'Loss de validação: {compute_loss_dataloaders(model_query, dataloader_queries_val, model_doc, dataloader_docs_val)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KST5dRqWGxff"
      },
      "source": [
        "Agora treina os dois encoders simulatenamente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrzMILg4GxDz",
        "outputId": "8192333d-e37d-4b77-d137-cf8bc4ed07ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 23 µs, sys: 2 µs, total: 25 µs\n",
            "Wall time: 28.4 µs\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import get_linear_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup, AdamW\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "if treinar_e_salvar_modelos:\n",
        "  # Training loop\n",
        "  optimizer_query = AdamW(model_query.parameters(), lr=lr)\n",
        "  optimizer_doc = AdamW(model_doc.parameters(), lr=lr)\n",
        "\n",
        "  num_training_steps = epochs * len(dataloader_queries_train)\n",
        "  num_warmup_steps = int(num_training_steps * 0.1)\n",
        "\n",
        "  # get_linear_schedule_with_warmup get_cosine_with_hard_restarts_schedule_with_warmup\n",
        "  scheduler_query = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer_query, num_warmup_steps, num_training_steps)   \n",
        "  scheduler_doc = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer_doc, num_warmup_steps, num_training_steps)   \n",
        "\n",
        "  for epoch in tqdm(range(epochs), desc='Epochs'):\n",
        "      model_query.train()\n",
        "      model_doc.train()\n",
        "      \n",
        "      train_losses = []\n",
        "      for batch_query, batch_docs in tqdm(list(zip(dataloader_queries_train, dataloader_docs_train)), mininterval=0.5, desc='Train', disable=False):\n",
        "          optimizer_query.zero_grad()\n",
        "          optimizer_doc.zero_grad()\n",
        "          \n",
        "          loss = compute_loss_com_gradiente(model_query, batch_query, model_doc, batch_docs)\n",
        "          loss.backward()\n",
        "          \n",
        "          optimizer_query.step()\n",
        "          optimizer_doc.step()\n",
        "\n",
        "          scheduler_query.step()\n",
        "          scheduler_doc.step()\n",
        "      \n",
        "      model_query.save_pretrained(f'{dir_modelos}{epoch}/query/')\n",
        "      model_doc.save_pretrained(f'{dir_modelos}{epoch}/doc/')\n",
        "\n",
        "      model_query.eval()\n",
        "      model_doc.eval()\n",
        "      \n",
        "      print(f'Loss de treinamento {epoch}: {compute_loss_dataloaders(model_query, dataloader_queries_train, model_doc, dataloader_docs_train)}')\n",
        "      print(f'Loss de validação {epoch}: {compute_loss_dataloaders(model_query, dataloader_queries_val, model_doc, dataloader_docs_val)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sun4lP8KCAvs"
      },
      "source": [
        "## Pesquisa completa no TREC-COVID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kLOVQsjj25U"
      },
      "source": [
        "Treinado o modelo, agora vamos aplicá-lo ao TREC-COVID:.\n",
        "\n",
        "Coloca os modelos em modo eval:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "CNXUml6aRki7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cff23d57-8f6b-4da8-f4e8-64d0a539aee0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 384, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 384)\n",
              "    (token_type_embeddings): Embedding(2, 384)\n",
              "    (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
              "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
              "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
              "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "model_query.eval()\n",
        "model_doc.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSzSdHkpqA7E"
      },
      "source": [
        "Baixa o trec-covid:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "x3t9H9Dyj8F0"
      },
      "outputs": [],
      "source": [
        "if not Path('./collections/trec-covid.zip').is_file():\n",
        "  !wget {url_trec_covid} -P collections # type: ignore\n",
        "  !unzip -o collections/trec-covid.zip -d ./collections # type: ignore\n",
        "\n",
        "# Converte o qrels que veio no trec-covid.zip pra o formato esperado:\n",
        "with open('./collections/trec-covid/qrels/test.tsv', 'r') as fin:\n",
        "  data = fin.read().splitlines(True)\n",
        "with open('./collections/trec-covid/qrels/test_corrigido.tsv', 'w') as fout:\n",
        "  for linha in data[1:]:\n",
        "    campos = linha.split()\n",
        "    fout.write(f'{campos[0]}\\t0\\t{campos[1]}\\t{campos[2]}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-d2PEMXqEWA"
      },
      "source": [
        "Carrega as queries e os documentos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "755HjJD1k_x-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d88c71d-2fb4-42e0-9eeb-f62e14f48548"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processado 0 documentos\n",
            "Processado 10000 documentos\n",
            "Processado 20000 documentos\n",
            "Processado 30000 documentos\n",
            "Processado 40000 documentos\n",
            "Processado 50000 documentos\n",
            "Processado 60000 documentos\n",
            "Processado 70000 documentos\n",
            "Processado 80000 documentos\n",
            "Processado 90000 documentos\n",
            "Processado 100000 documentos\n",
            "Processado 110000 documentos\n",
            "Processado 120000 documentos\n",
            "Processado 130000 documentos\n",
            "Processado 140000 documentos\n",
            "Processado 150000 documentos\n",
            "Processado 160000 documentos\n",
            "Processado 170000 documentos\n",
            "CPU times: user 1.21 s, sys: 132 ms, total: 1.34 s\n",
            "Wall time: 1.31 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import json\n",
        "\n",
        "def carrega_corpus_trec_covid():\n",
        "  retorno = []\n",
        "  with open('./collections/trec-covid/corpus.jsonl') as corpus:\n",
        "    for i, line in enumerate(corpus):\n",
        "      doc = json.loads(line)\n",
        "      #retorno.append({\n",
        "      #    'id': doc['_id'],\n",
        "      #    'doc': f\"{doc['title']} {doc['text']}\"\n",
        "      #})\n",
        "      retorno.append(\n",
        "          (doc['_id'], f\"{doc['title']} {doc['text']}\")\n",
        "      )\n",
        "      if (i % 10000 == 0):\n",
        "        print(f'Processado {i} documentos')\n",
        "    return retorno\n",
        "\n",
        "def carrega_queries_trec_covid():\n",
        "  retorno = []\n",
        "  with open('./collections/trec-covid/queries.jsonl') as queries:\n",
        "    for line in queries:\n",
        "      query = json.loads(line)\n",
        "      # Faz apenas uma pequena tradução de _id para id e text para texto\n",
        "      retorno.append({'id': query['_id'], 'texto': query['text']})\n",
        "  return retorno\n",
        "\n",
        "queries_trec_covid = carrega_queries_trec_covid()\n",
        "corpus_trec_covid = carrega_corpus_trec_covid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLTscG-JCLy7"
      },
      "source": [
        "A variável corpus_trec_covid contém os ids e os textos. Agora é necessário carregar a representação vetorial desses textos. Isso será feito gerando a matriz matriz_docs_trec_covid:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "pNsy7URQm4ws",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33567490-04ed-4439-d2b8-3aa34a8de94f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([171332, 384])\n",
            "CPU times: user 5.33 s, sys: 1.47 s, total: 6.81 s\n",
            "Wall time: 6.67 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "ids_trec_covid, textos_trec_covid = zip(*corpus_trec_covid)\n",
        "\n",
        "matriz_docs_trec_covid = None\n",
        "#textos_trec_covid = textos_trec_covid[0:20000]\n",
        "dataset_docs_trec_covid = Dataset(tokenizer, textos_trec_covid)\n",
        "dataloader_docs_trec_covid = DataLoader(dataset_docs_trec_covid, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "if gerar_e_salvar_matriz_docs_trec_covid:\n",
        "  with torch.no_grad():\n",
        "    for batch in tqdm(dataloader_docs_trec_covid, mininterval=0.5, desc='Convertendo documentos trec-covid', disable=False):\n",
        "      outputs_docs = model_doc(**batch.to(device))\n",
        "      tcls_docs    = outputs_docs.last_hidden_state[:, 0, :]\n",
        "      tcls_docs    = tcls_docs / torch.norm(tcls_docs, dim=1, keepdim=True)\n",
        "      # Monta a matriz de documentos na CPU\n",
        "      tcls_docs = tcls_docs.to(\"cpu\")\n",
        "\n",
        "      if matriz_docs_trec_covid is None:\n",
        "        matriz_docs_trec_covid = tcls_docs\n",
        "      else:\n",
        "        matriz_docs_trec_covid = torch.cat( (matriz_docs_trec_covid, tcls_docs), dim=0)\n",
        "\n",
        "  # Agora volta a matriz pra GPU pq ela cabe lá, não sei pq estava estourando a memória antes...\n",
        "  matriz_docs_trec_covid = matriz_docs_trec_covid.to(device)\n",
        "  torch.save(matriz_docs_trec_covid, arquivo_matriz_docs_trec_covid)\n",
        "else:\n",
        "  matriz_docs_trec_covid = torch.load(arquivo_matriz_docs_trec_covid).to(device)\n",
        "\n",
        "print(matriz_docs_trec_covid.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBFvzp3BrD0X"
      },
      "source": [
        "Agora vamos definir um método pra calcular a representação vetorial da query e para fazer a pesquisa na base de dados. O método de pesquisa só retorna o vetor de score pareado com as ids contidas na variável ids_trec_covid:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "OIQ2VNrdq_kH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "038171a7-fefd-41eb-ead1-a82616c5992e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([384])\n"
          ]
        }
      ],
      "source": [
        "def get_vetor_query(query):\n",
        "  query_tokenizada = tokenizer(query, padding=True, truncation=True, max_length=max_length, return_tensors='pt')\n",
        "\n",
        "  with torch.no_grad():\n",
        "    output_query = model_query(**query_tokenizada.to(device))\n",
        "    tcls_query    = output_query.last_hidden_state[:, 0, :]\n",
        "    # tcls_query    = tcls_query / torch.norm(tcls_query, dim=1, keepdim=True)\n",
        "\n",
        "  return tcls_query[0]\n",
        "\n",
        "vetor_query = get_vetor_query('what is this?')\n",
        "\n",
        "print(vetor_query.size())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "1F9iF_6KsB-O"
      },
      "outputs": [],
      "source": [
        "def calcula_score_documentos_para_a_query(matriz_docs, query):\n",
        "  vetor_query = get_vetor_query(query)\n",
        "  score = torch.matmul(matriz_docs, vetor_query)\n",
        "\n",
        "  return score\n",
        "\n",
        "def pesquisa_query_e_retorna_n_primeiros_docs(matriz_docs, ids_docs_na_matriz, query, n=1000):\n",
        "  # Calcula o score\n",
        "  score = calcula_score_documentos_para_a_query(matriz_docs, query)\n",
        "  # Ordena\n",
        "  sorted_score, indices_score = torch.sort(score, descending=True)\n",
        "  # Pega só os n primeiros\n",
        "  sorted_score = sorted_score[0:n]\n",
        "  indices_score = indices_score[0:n]\n",
        "  # Extrai os ids dos documentos\n",
        "  ids_docs = [ids_docs_na_matriz[i] for i in indices_score]\n",
        "\n",
        "  return zip(ids_docs, sorted_score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8B56kNR_wfCv"
      },
      "source": [
        "Agora roda todas as queries para avaliação..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "C6PFulWMsgX9"
      },
      "outputs": [],
      "source": [
        "# Roda todas as queries\n",
        "def run_all_queries(file):\n",
        "  print('Carregando as queries do arquivo queries.jsonl...\\n')\n",
        "  queries_trec_covid = carrega_queries_trec_covid()\n",
        "\n",
        "  print(f'Total de queries que serão avaliadas: {len(queries_trec_covid)}')\n",
        "  cnt = 0\n",
        "  with open(file, 'w') as runfile:\n",
        "    for query in queries_trec_covid:\n",
        "      id_query = query['id']\n",
        "      texto = query['texto']\n",
        "\n",
        "      if cnt % 5 == 0:\n",
        "        print(f'{cnt} queries completadas')\n",
        "\n",
        "      # Pega os primeiros 1000 resultados\n",
        "      docs_score = pesquisa_query_e_retorna_n_primeiros_docs(matriz_docs_trec_covid, ids_trec_covid, texto, n=1000)\n",
        "      \n",
        "      i = 0\n",
        "      for id_doc, score in docs_score:\n",
        "        _ = runfile.write('{} Q0 {} {} {:.6f} Pesquisa_densa\\n'.format(id_query, id_doc, i+1, float(score)))\n",
        "        i += 1\n",
        "\n",
        "      cnt += 1\n",
        "      # break # Pra testar, gera só a primeira query\n",
        "    print(f'{cnt} queries completadas')\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lVWkSeBwiNb"
      },
      "source": [
        "E calcula o nDCG@10:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "wuDvWPs8wYCX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdb51fb2-db3f-47a9-ed63-eace03237e39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando as queries do arquivo queries.jsonl...\n",
            "\n",
            "Total de queries que serão avaliadas: 50\n",
            "0 queries completadas\n",
            "5 queries completadas\n",
            "10 queries completadas\n",
            "15 queries completadas\n",
            "20 queries completadas\n",
            "25 queries completadas\n",
            "30 queries completadas\n",
            "35 queries completadas\n",
            "40 queries completadas\n",
            "45 queries completadas\n",
            "50 queries completadas\n",
            "Downloading https://search.maven.org/remotecontent?filepath=uk/ac/gla/dcs/terrierteam/jtreceval/0.0.5/jtreceval-0.0.5-jar-with-dependencies.jar to /root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar...\n",
            "/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar already exists!\n",
            "Skipping download.\n",
            "Running command: ['java', '-jar', '/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar', '-c', '-m', 'ndcg_cut.10', 'collections/trec-covid/qrels/test_corrigido.tsv', 'run-pesquisa-densa.txt']\n",
            "Results:\n",
            "ndcg_cut_10           \tall\t0.3567\n"
          ]
        }
      ],
      "source": [
        "run_all_queries('run-pesquisa-densa.txt')\n",
        "\n",
        "!python -m pyserini.eval.trec_eval -c -m ndcg_cut.10 collections/trec-covid/qrels/test_corrigido.tsv run-pesquisa-densa.txt #type: ignore"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}