{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXvyb_bHb_0n"
      },
      "source": [
        "# Aula 7 - Solução dos exercícios\n",
        "Leandro Carísio Fernandes\n",
        "\n",
        "<br>\n",
        "\n",
        "Implementar a fase de indexação e buscas de um modelo esparso\n",
        "\n",
        "- Usar este modelo SPLADE já treinado naver/splade_v2_distil (do distilbert) ou splade-cocondenser-selfdistil (do BERT-base 110M params). Mais informações sobre os modelos estão neste artigo: https://arxiv.org/pdf/2205.04733.pdf\n",
        "- Não é necessário treinar o modelo\n",
        "- Avaliar nDCG@10 no TREC-COVID e comparar resultados com o BM25 e buscador denso da semana passada\n",
        "- A dificuldade do exercício está em implementar a função de busca e ranqueamento usada pelo SPLADE. A implementação do índice invertido é apenas um \"dicionário python\".\n",
        "- Comparar seus resultados com a busca \"original\" do SPLADE.\n",
        "Medir latencia (s/query)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odOMvCS0cL3G"
      },
      "source": [
        "## Preparação do ambiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zn_NfMfttIH",
        "outputId": "eb54b463-e543-4cbf-a709-5d37558c4baa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Apr 25 14:23:55 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   72C    P0    32W /  70W |  13765MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM7ZWfVScOrR"
      },
      "source": [
        "### Variáveis para controlar o fluxo do caderno"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jo7ycOc_b5Gd"
      },
      "outputs": [],
      "source": [
        "# Setar como True só pra gerar a matriz de documentos pela primeira vez\n",
        "gerar_indice_invertido_docs = False\n",
        "\n",
        "# Resultados para esse conjunto de parâmetros\n",
        "# nDCG@10: 0.7269 (fp16) / Latência: 53.9 s para 50 queries\n",
        "#                           nDCG@10: 0.7282 (fp32) -> calculado em outro caderno\n",
        "# param = {\n",
        "#     'agg': 'max',\n",
        "#     'nome_modelo': 'naver/splade-cocondenser-ensembledistil',\n",
        "#     'manter_contribuicao_CLS_SEP_da_matriz_doc': True,\n",
        "#     'nome_arquivo_indice_invertido_docs': 'idx_splade_naver_splade-cocondenser-ensembledistil_max_com_cls_sep.pickle'\n",
        "# }\n",
        "\n",
        "# Resultados para esse conjunto de parâmetros\n",
        "# nDCG@10: 0.7242 / Latência: 52.6 s para 50 queries ()\n",
        "param = {\n",
        "    'agg': 'max',\n",
        "    'nome_modelo': 'naver/splade-cocondenser-ensembledistil',\n",
        "    'manter_contribuicao_CLS_SEP_da_matriz_doc': False,\n",
        "    'nome_arquivo_indice_invertido_docs': 'idx_splade_naver_splade-cocondenser-ensembledistil_max_sem_cls_sep.pickle'\n",
        "}\n",
        "\n",
        "# Resultados para esse conjunto de parâmetros\n",
        "# nDCG@10: 0.1921 / Latência: 54.8 s para 50 queries (~1,09 seg/query)\n",
        "# param = {\n",
        "#     'agg': 'sum',\n",
        "#     'nome_modelo': 'naver/splade-cocondenser-ensembledistil',\n",
        "#     'manter_contribuicao_CLS_SEP_da_matriz_doc': True,\n",
        "#     'nome_arquivo_indice_invertido_docs': 'idx_splade_naver_splade-cocondenser-ensembledistil_sum_com_cls_sep.pickle'\n",
        "# }\n",
        "\n",
        "dir_aula_7 = '/content/drive/My Drive/IA368-DD_deep_learning_busca/Aula7_splade/'\n",
        "\n",
        "batch_size_trec_covid = 32\n",
        "url_trec_covid = 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/trec-covid.zip'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpozB7TRdHUt"
      },
      "source": [
        "### Instalação de libs e montagem do drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7Ehl5pgdLJf",
        "outputId": "6b76675b-a3e9-44b0-a2f3-9bc6ff4096c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install transformers datasets -q\n",
        "!pip install sentence-transformers -q\n",
        "!pip install pyserini -q\n",
        "!pip install faiss-gpu -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dk2LI7C7crod"
      },
      "source": [
        "### Download e carga dos documentos e queries do TREC-COVID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xuiBbs5cyRG",
        "outputId": "c9ab73c6-c81f-490b-e149-b73103f918ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processado 0 documentos\n",
            "Processado 10000 documentos\n",
            "Processado 20000 documentos\n",
            "Processado 30000 documentos\n",
            "Processado 40000 documentos\n",
            "Processado 50000 documentos\n",
            "Processado 60000 documentos\n",
            "Processado 70000 documentos\n",
            "Processado 80000 documentos\n",
            "Processado 90000 documentos\n",
            "Processado 100000 documentos\n",
            "Processado 110000 documentos\n",
            "Processado 120000 documentos\n",
            "Processado 130000 documentos\n",
            "Processado 140000 documentos\n",
            "Processado 150000 documentos\n",
            "Processado 160000 documentos\n",
            "Processado 170000 documentos\n",
            "CPU times: user 1.51 s, sys: 173 ms, total: 1.69 s\n",
            "Wall time: 1.65 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "if not Path('./collections/trec-covid.zip').is_file():\n",
        "  !wget {url_trec_covid} -P collections # type: ignore\n",
        "  !unzip -o collections/trec-covid.zip -d ./collections # type: ignore\n",
        "\n",
        "# Converte o qrels que veio no trec-covid.zip pra o formato esperado:\n",
        "with open('./collections/trec-covid/qrels/test.tsv', 'r') as fin:\n",
        "  data = fin.read().splitlines(True)\n",
        "with open('./collections/trec-covid/qrels/test_corrigido.tsv', 'w') as fout:\n",
        "  for linha in data[1:]:\n",
        "    campos = linha.split()\n",
        "    fout.write(f'{campos[0]}\\t0\\t{campos[1]}\\t{campos[2]}\\n')\n",
        "\n",
        "def carrega_corpus_trec_covid():\n",
        "  retorno = []\n",
        "  with open('./collections/trec-covid/corpus.jsonl') as corpus:\n",
        "    for i, line in enumerate(corpus):\n",
        "      doc = json.loads(line)\n",
        "      #retorno.append({\n",
        "      #    'id': doc['_id'],\n",
        "      #    'doc': f\"{doc['title']} {doc['text']}\"\n",
        "      #})\n",
        "      retorno.append(\n",
        "          (doc['_id'], f\"{doc['title']} {doc['text']}\")\n",
        "      )\n",
        "      if (i % 10000 == 0):\n",
        "        print(f'Processado {i} documentos')\n",
        "    return retorno\n",
        "\n",
        "def carrega_queries_trec_covid():\n",
        "  retorno = []\n",
        "  with open('./collections/trec-covid/queries.jsonl') as queries:\n",
        "    for line in queries:\n",
        "      query = json.loads(line)\n",
        "      # Faz apenas uma pequena tradução de _id para id e text para texto\n",
        "      retorno.append({'id': query['_id'], 'texto': query['text']})\n",
        "  return retorno\n",
        "\n",
        "queries_trec_covid = carrega_queries_trec_covid()\n",
        "corpus_trec_covid = carrega_corpus_trec_covid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2bVHlInJusq"
      },
      "outputs": [],
      "source": [
        "# Ordena corpus de acordo com o tamanho do texto pra tentar diminuir o tempo\n",
        "corpus_trec_covid = sorted(corpus_trec_covid, key=lambda x: len(x[1]), reverse=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSZA3_uucvb7"
      },
      "source": [
        "## SPLADE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3pri8p4T7Gp"
      },
      "source": [
        "### Testes convertendo uma string simples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWc7wUZsdXD4",
        "outputId": "d616935c-f941-4114-a476-870bb1b09870"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 141 µs, sys: 0 ns, total: 141 µs\n",
            "Wall time: 148 µs\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
        "import torch\n",
        "from torch.nn.functional import relu\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def carregar_tokenizador_e_modelo(nome):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(nome)\n",
        "  model = AutoModelForMaskedLM.from_pretrained(nome).to(device)\n",
        "\n",
        "  return tokenizer, model\n",
        "\n",
        "def representacao_esparsa_do_texto(model, tokenizer, texto, add_special_tokens=True, manter_contribuicao_cls_sep=False):\n",
        "  # Tokeniza o texto\n",
        "  # Para texto = \"The quick brown jumps over the lazy dog\",\n",
        "  # tokens = ['the', 'quick', 'brown', 'jumps', 'over', 'the', 'lazy', 'dog'] (tamanho 8)\n",
        "  # token_ids = [1996, 4248, 2829, 103, 14523, 2058, 1996, 13971, 3899] (tamanho 8)\n",
        "\n",
        "  # Roda o modelo\n",
        "  inputs = tokenizer(texto, add_special_tokens=add_special_tokens,\n",
        "                     return_special_tokens_mask=True,\n",
        "                     return_tensors='pt',\n",
        "                     truncation=True,\n",
        "                     max_length=256)\n",
        "\n",
        "  with torch.autocast(device_type=str(device), dtype=torch.float16, enabled=True):\n",
        "    with torch.no_grad():\n",
        "      outputs = model(input_ids=inputs['input_ids'].to(device), attention_mask=inputs['attention_mask'].to(device))\n",
        "\n",
        "  # Acessa os logits  \n",
        "  # outputs.logits.size() = torch.Size([1, 8, 30522])\n",
        "  # logits.size() = torch.Size([8, 30588])\n",
        "  logits = outputs.logits[0, :]\n",
        "\n",
        "  # Pelo artigo, agora a gente calcula somatório [ log(1 + ReLU(w_ij)) ]\n",
        "  # relu(logits) vai manter o mesmo tamanho: [8, 30588]\n",
        "  # 1 + relu(logits) também vai manter o mesmo tamanho: [8, 30588]\n",
        "  # log(1 + relu(logits)) também vai manter o mesmo tamanho: [8, 30588]\n",
        "  # Feito isso, calcula o somatório na dim=0, o que vai gerar um vetor de tamanho\n",
        "  # 30588, que é o tamanho do vocabulário:\n",
        "      \n",
        "  mask_tokens_validos = 1 - inputs['special_tokens_mask']\n",
        "  mask = mask_tokens_validos.squeeze().unsqueeze(-1).expand(logits.size()).to(device)\n",
        "\n",
        "  if manter_contribuicao_cls_sep:\n",
        "    mask = torch.ones(mask.size()).to(device) # Como não tem batch envolvido, isso é o mesmo que a attention mask\n",
        "\n",
        "  if param['agg'] == 'sum':\n",
        "    wj = torch.sum(torch.log(1 + relu(logits*mask)), dim=0)\n",
        "  else:\n",
        "    wj, _ = torch.max(torch.log(1 + relu(logits*mask)), dim=0)\n",
        "\n",
        "  # Agora temos que armazenar esse vetor de forma esparsa...\n",
        "  return wj.to_sparse()\n",
        "\n",
        "def converte_token_ids_para_tokens(tokenizer, ids_tokens):\n",
        "  return tokenizer.convert_ids_to_tokens(ids_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fUhOxQ2kTgX",
        "outputId": "241f35ca-c60d-4416-ef89-1e5fa5ee7a88"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForMaskedLM(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (cls): BertOnlyMLMHead(\n",
              "    (predictions): BertLMPredictionHead(\n",
              "      (transform): BertPredictionHeadTransform(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (transform_act_fn): GELUActivation()\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ],
      "source": [
        "tokenizer, model = carregar_tokenizador_e_modelo(param['nome_modelo'])\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvRI_0_9UErN"
      },
      "source": [
        "Testa \"where eat pizza\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1qDZTn9kGUp",
        "outputId": "0daee5a4-cea0-4198-8e8b-645dad7505de"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('that', tensor(0.0597, device='cuda:0', dtype=torch.float16)),\n",
              " ('where', tensor(0.5859, device='cuda:0', dtype=torch.float16)),\n",
              " ('place', tensor(0.8374, device='cuda:0', dtype=torch.float16)),\n",
              " ('best', tensor(0.0039, device='cuda:0', dtype=torch.float16)),\n",
              " ('country', tensor(0.7222, device='cuda:0', dtype=torch.float16)),\n",
              " ('food', tensor(0.6934, device='cuda:0', dtype=torch.float16)),\n",
              " ('places', tensor(0.5732, device='cuda:0', dtype=torch.float16)),\n",
              " ('culture', tensor(0.1536, device='cuda:0', dtype=torch.float16)),\n",
              " ('location', tensor(1.4600, device='cuda:0', dtype=torch.float16)),\n",
              " ('famous', tensor(0.3958, device='cuda:0', dtype=torch.float16)),\n",
              " ('hotel', tensor(0.4551, device='cuda:0', dtype=torch.float16)),\n",
              " ('farm', tensor(0.1238, device='cuda:0', dtype=torch.float16)),\n",
              " ('visit', tensor(0.0439, device='cuda:0', dtype=torch.float16)),\n",
              " ('website', tensor(0.0010, device='cuda:0', dtype=torch.float16)),\n",
              " ('headquarters', tensor(0.3276, device='cuda:0', dtype=torch.float16)),\n",
              " ('shop', tensor(0.7900, device='cuda:0', dtype=torch.float16)),\n",
              " ('eat', tensor(2.2676, device='cuda:0', dtype=torch.float16)),\n",
              " ('restaurant', tensor(1.4004, device='cuda:0', dtype=torch.float16)),\n",
              " ('locations', tensor(0.4048, device='cuda:0', dtype=torch.float16)),\n",
              " ('favorite', tensor(0.1469, device='cuda:0', dtype=torch.float16)),\n",
              " ('anywhere', tensor(0.0411, device='cuda:0', dtype=torch.float16)),\n",
              " ('eating', tensor(1.5361, device='cuda:0', dtype=torch.float16)),\n",
              " ('habitat', tensor(0.4351, device='cuda:0', dtype=torch.float16)),\n",
              " ('venue', tensor(0.2026, device='cuda:0', dtype=torch.float16)),\n",
              " ('murphy', tensor(0.1091, device='cuda:0', dtype=torch.float16)),\n",
              " ('cafe', tensor(0.4080, device='cuda:0', dtype=torch.float16)),\n",
              " ('restaurants', tensor(0.6191, device='cuda:0', dtype=torch.float16)),\n",
              " ('pizza', tensor(3.2695, device='cuda:0', dtype=torch.float16)),\n",
              " ('pie', tensor(1.9951, device='cuda:0', dtype=torch.float16))]"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ],
      "source": [
        "wj = representacao_esparsa_do_texto(model, tokenizer, \"Where eat pizza\")\n",
        "ids = wj.indices()\n",
        "valores = wj.values()\n",
        "tokens = converte_token_ids_para_tokens(tokenizer, ids[0])\n",
        "list(zip(tokens, valores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsA-ElETrkNP",
        "outputId": "20933bb2-2ff2-4297-c312-a109969ea86a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('about', tensor(0.5698, device='cuda:0', dtype=torch.float16)),\n",
              " ('now', tensor(1.8574, device='cuda:0', dtype=torch.float16)),\n",
              " ('world', tensor(0.3521, device='cuda:0', dtype=torch.float16)),\n",
              " ('day', tensor(0.1946, device='cuda:0', dtype=torch.float16)),\n",
              " ('england', tensor(0.0241, device='cuda:0', dtype=torch.float16)),\n",
              " ('summer', tensor(0.6514, device='cuda:0', dtype=torch.float16)),\n",
              " ('today', tensor(2.9023, device='cuda:0', dtype=torch.float16)),\n",
              " ('science', tensor(0.5337, device='cuda:0', dtype=torch.float16)),\n",
              " ('event', tensor(0.2983, device='cuda:0', dtype=torch.float16)),\n",
              " ('current', tensor(0.6099, device='cuda:0', dtype=torch.float16)),\n",
              " ('earth', tensor(0.0698, device='cuda:0', dtype=torch.float16)),\n",
              " ('nature', tensor(0.0734, device='cuda:0', dtype=torch.float16)),\n",
              " ('winter', tensor(0.9053, device='cuda:0', dtype=torch.float16)),\n",
              " ('joe', tensor(0.0743, device='cuda:0', dtype=torch.float16)),\n",
              " ('storm', tensor(0.7231, device='cuda:0', dtype=torch.float16)),\n",
              " ('environment', tensor(0.1768, device='cuda:0', dtype=torch.float16)),\n",
              " ('ocean', tensor(0.0203, device='cuda:0', dtype=torch.float16)),\n",
              " ('brian', tensor(0.1809, device='cuda:0', dtype=torch.float16)),\n",
              " ('environmental', tensor(0.0317, device='cuda:0', dtype=torch.float16)),\n",
              " ('rain', tensor(0.9287, device='cuda:0', dtype=torch.float16)),\n",
              " ('weather', tensor(3.0977, device='cuda:0', dtype=torch.float16)),\n",
              " ('climate', tensor(0.7480, device='cuda:0', dtype=torch.float16)),\n",
              " ('temperature', tensor(1.1748, device='cuda:0', dtype=torch.float16)),\n",
              " ('carl', tensor(0.0505, device='cuda:0', dtype=torch.float16)),\n",
              " ('gabriel', tensor(0.0392, device='cuda:0', dtype=torch.float16)),\n",
              " ('julian', tensor(0.3135, device='cuda:0', dtype=torch.float16)),\n",
              " ('yesterday', tensor(0.4099, device='cuda:0', dtype=torch.float16)),\n",
              " ('geography', tensor(0.1029, device='cuda:0', dtype=torch.float16)),\n",
              " ('pollution', tensor(0.0797, device='cuda:0', dtype=torch.float16)),\n",
              " ('atmospheric', tensor(0.2852, device='cuda:0', dtype=torch.float16)),\n",
              " ('comet', tensor(0.0430, device='cuda:0', dtype=torch.float16)),\n",
              " ('forecast', tensor(0.3486, device='cuda:0', dtype=torch.float16)),\n",
              " ('meteor', tensor(0.3943, device='cuda:0', dtype=torch.float16))]"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ],
      "source": [
        "wj = representacao_esparsa_do_texto(model, tokenizer, \"what about the weather today\")\n",
        "ids = wj.indices()\n",
        "valores = wj.values()\n",
        "tokens = converte_token_ids_para_tokens(tokenizer, ids[0])\n",
        "list(zip(tokens, valores))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hwbfg2oTUNU8"
      },
      "source": [
        "### Testes em batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luWLKnkszv6p"
      },
      "outputs": [],
      "source": [
        "from torch.utils import data\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Definição do Dataset\n",
        "class Dataset(data.Dataset):\n",
        "    # Recebe apenas um vetor de textos\n",
        "    def __init__(self, tokenizer, textos, max_seq_length):\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.tokenizer = tokenizer\n",
        "        self.textos = textos\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.textos)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # Aqui é só uma passada no eval, não precisa de cache\n",
        "        item = self.tokenizer(self.textos[idx],\n",
        "                       padding=True,\n",
        "                       return_special_tokens_mask=True,\n",
        "                       # No exemplo dos autores eles não removem o CLS/SEP\n",
        "                       # https://github.com/naver/splade/blob/main/inference_splade.ipynb\n",
        "                       add_special_tokens=True, \n",
        "                       truncation=True,\n",
        "                       max_length=self.max_seq_length\n",
        "                )\n",
        "        return item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzCkfrgx0UTW"
      },
      "outputs": [],
      "source": [
        "from transformers import BatchEncoding\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return BatchEncoding(tokenizer.pad(batch, return_tensors='pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4wvptqNQlI3"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "def representacao_esparsa_dataloader(model, tokenizer, dataloader, func_executar_apos_batch = lambda idx_batch, wj_batch : None):\n",
        "\n",
        "  # Ideia do Marcus Piau de usar f16 - otimiza o cálculo\n",
        "  with torch.autocast(device_type=str(device), dtype=torch.float16, enabled=True):\n",
        "    with torch.no_grad():\n",
        "      for i_batch, batch in enumerate(tqdm(dataloader)):\n",
        "        outputs = model(input_ids = batch['input_ids'].to(device), attention_mask= batch['attention_mask'].to(device))\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Na hora de recuperar os logits, temos duas opções. Ou consideramos só a attention mask (ou seja, \n",
        "        # exclui apenas os PAD) ou consideramos a special tokens mask. Nessa situação remove (CLS, SEP e PAD).\n",
        "        # Deixa configurável pra testar os dois\n",
        "              # OBS.: No modelo do autor ele usa special_tokens e só remove o attention_mask. Vamos \n",
        "              # Mas tem uma classe SpaceDoc que parece que depois tira manualmente. Então vamos tentar simular\n",
        "              # tirando os special_tokens_mask e depois se não der certo, tentamos tirar só o attention_mask\n",
        "              # implementar assim também então: https://github.com/naver/splade/blob/main/splade/models/transformer_rep.py\n",
        "        if param['manter_contribuicao_CLS_SEP_da_matriz_doc']:\n",
        "          mask_tokens_validos = batch['attention_mask']\n",
        "        else:\n",
        "          mask_tokens_validos = 1 - batch['special_tokens_mask']\n",
        "        # Expande a máscara criando uma terceira dimensão (vocab_size) \n",
        "        # e colocando do mesmo tamanho que os logits (batch_size, x, vocab_size):\n",
        "        mask = mask_tokens_validos.unsqueeze(-1).expand(logits.size()).to(device)\n",
        "\n",
        "        # Calcula a saída (os pesos wj)\n",
        "        if param['agg'] == 'sum':\n",
        "          wj = torch.sum(torch.log(1 + relu(logits*mask)), dim=1)\n",
        "        else:\n",
        "          wj, _ = torch.max(torch.log(1 + relu(logits*mask)), dim=1)\n",
        "\n",
        "        # Antes eu estava retornando wj.to_sparse() pois estava salvando\n",
        "        # a matriz em disco. Pra salvar num índice invertido não tem mais\n",
        "        # necessidade disso.\n",
        "\n",
        "        # Callback\n",
        "        idx_inicio = i_batch * dataloader.batch_size\n",
        "        idx_fim = idx_inicio + min(dataloader.batch_size, wj.size()[0])\n",
        "        indices_tratados = list(range(idx_inicio, idx_fim))\n",
        "        func_executar_apos_batch(indices_tratados, wj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Amx5aSw-URB3"
      },
      "source": [
        "Testas os mesmos textos anteriores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0FyBfR4zxx9"
      },
      "outputs": [],
      "source": [
        "textos = ['where eat pizza', 'what about the weather today']\n",
        "dataset_textos = Dataset(tokenizer, textos, 256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKbkUnRwUclV"
      },
      "source": [
        "Primeiro, com um batch de tamanho 2 (vai inserir padding no primeiro elemento)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "In7sFUrqUnnE",
        "outputId": "5b7bd716-6576-4075-df30-01772b6ca662"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [[101, 2073, 4521, 10733, 102, 0, 0], [101, 2054, 2055, 1996, 4633, 2651, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1]], 'special_tokens_mask': [[1, 0, 0, 0, 1, 1, 1], [1, 0, 0, 0, 0, 0, 1]]}"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ],
      "source": [
        "# Os input_ids com a máscara:\n",
        "dataset_textos[0:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "eed7f571b042429c885a0d4cd805465a",
            "e56e028ca95945f7b6c797fea07efa2f",
            "25f53b2f3cd44718aa1524aa53225dca",
            "cc8215b0855b48b4b269c60a02148974",
            "1f5ecb38f4ad4d08a22da4773d68e90c",
            "a449faab56e240ad8f127b5488320500",
            "26ee989002a3494795be7f43e4e324f8",
            "f81b4425335a4036a7ed53cd0658aad9",
            "71162205aeda43ae956028e640b236d2",
            "e9b26da6f5dc419db821c4124f105d6c",
            "556c0208b05140f6806401b3564eb7f7"
          ]
        },
        "id": "9MI8q_jBR5p7",
        "outputId": "4ffa31fa-dbbb-45c3-da93-1d23e8e4ec2c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eed7f571b042429c885a0d4cd805465a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29\n",
            "[('that', tensor(0.0597, device='cuda:0')), ('where', tensor(0.5860, device='cuda:0')), ('place', tensor(0.8375, device='cuda:0')), ('best', tensor(0.0039, device='cuda:0')), ('country', tensor(0.7220, device='cuda:0')), ('food', tensor(0.6931, device='cuda:0')), ('places', tensor(0.5735, device='cuda:0')), ('culture', tensor(0.1536, device='cuda:0')), ('location', tensor(1.4597, device='cuda:0')), ('famous', tensor(0.3957, device='cuda:0')), ('hotel', tensor(0.4550, device='cuda:0')), ('farm', tensor(0.1238, device='cuda:0')), ('visit', tensor(0.0439, device='cuda:0')), ('website', tensor(0.0010, device='cuda:0')), ('headquarters', tensor(0.3276, device='cuda:0')), ('shop', tensor(0.7899, device='cuda:0')), ('eat', tensor(2.2676, device='cuda:0')), ('restaurant', tensor(1.4008, device='cuda:0')), ('locations', tensor(0.4048, device='cuda:0')), ('favorite', tensor(0.1469, device='cuda:0')), ('anywhere', tensor(0.0411, device='cuda:0')), ('eating', tensor(1.5365, device='cuda:0')), ('habitat', tensor(0.4350, device='cuda:0')), ('venue', tensor(0.2026, device='cuda:0')), ('murphy', tensor(0.1091, device='cuda:0')), ('cafe', tensor(0.4081, device='cuda:0')), ('restaurants', tensor(0.6192, device='cuda:0')), ('pizza', tensor(3.2689, device='cuda:0')), ('pie', tensor(1.9949, device='cuda:0'))]\n",
            "33\n",
            "[('about', tensor(0.5696, device='cuda:0')), ('now', tensor(1.8579, device='cuda:0')), ('world', tensor(0.3520, device='cuda:0')), ('day', tensor(0.1946, device='cuda:0')), ('england', tensor(0.0241, device='cuda:0')), ('summer', tensor(0.6513, device='cuda:0')), ('today', tensor(2.9016, device='cuda:0')), ('science', tensor(0.5336, device='cuda:0')), ('event', tensor(0.2984, device='cuda:0')), ('current', tensor(0.6097, device='cuda:0')), ('earth', tensor(0.0698, device='cuda:0')), ('nature', tensor(0.0734, device='cuda:0')), ('winter', tensor(0.9053, device='cuda:0')), ('joe', tensor(0.0743, device='cuda:0')), ('storm', tensor(0.7230, device='cuda:0')), ('environment', tensor(0.1768, device='cuda:0')), ('ocean', tensor(0.0203, device='cuda:0')), ('brian', tensor(0.1809, device='cuda:0')), ('environmental', tensor(0.0317, device='cuda:0')), ('rain', tensor(0.9287, device='cuda:0')), ('weather', tensor(3.0974, device='cuda:0')), ('climate', tensor(0.7482, device='cuda:0')), ('temperature', tensor(1.1744, device='cuda:0')), ('carl', tensor(0.0505, device='cuda:0')), ('gabriel', tensor(0.0393, device='cuda:0')), ('julian', tensor(0.3135, device='cuda:0')), ('yesterday', tensor(0.4100, device='cuda:0')), ('geography', tensor(0.1029, device='cuda:0')), ('pollution', tensor(0.0797, device='cuda:0')), ('atmospheric', tensor(0.2852, device='cuda:0')), ('comet', tensor(0.0430, device='cuda:0')), ('forecast', tensor(0.3485, device='cuda:0')), ('meteor', tensor(0.3943, device='cuda:0'))]\n"
          ]
        }
      ],
      "source": [
        "batch_size = 2\n",
        "dataloader_textos = DataLoader(dataset_textos, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "def callback_imprime(idx_batch, wj):\n",
        "  nonzero = wj.nonzero()\n",
        "\n",
        "  for i in range(wj.size()[0]):\n",
        "    idx_linha_i = nonzero[:, 0] == i\n",
        "    idx_token_em_wj_i = nonzero[idx_linha_i, 1] # além de ser o índice na matriz wj, é tb o id do token\n",
        "    val_token_em_wj_i = wj[i, idx_token_em_wj_i]\n",
        "\n",
        "    tokens = converte_token_ids_para_tokens(tokenizer, idx_token_em_wj_i.tolist())\n",
        "    print(len(tokens))\n",
        "    print(list(zip(tokens, val_token_em_wj_i)))\n",
        "\n",
        "representacao_esparsa_dataloader(model, tokenizer, dataloader_textos, callback_imprime)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdEvK-2YUuU3"
      },
      "source": [
        "Agora faz com batch_size = 1, tem que convergir pro caso de chamar cada um separadamente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "104f3e03555940f39da26ce99b6dc700",
            "633c1116e60b4fc4a0d6da99516e5158",
            "043cbbf710524091a996ee61c6bba5d0",
            "ed9b160f12af4dabb80d3f9d98a06be8",
            "e4ab75c6ebd24d7abbc4aef4e94f5e8d",
            "bd3ae65f77b44e14808097b06ecc1d5c",
            "9c0ddf133e1d4e6fa016b370f1c0d376",
            "b16929f09de4435689d5d0a7247e8b65",
            "9b6daca4531a4afc9f6832b9228bb007",
            "b5cebbb21fee452ba311dd25626a6ad1",
            "5d154e9f30674aea9f38c3baf2e57bac"
          ]
        },
        "id": "ZOLBgyzmT12b",
        "outputId": "75e0595c-f3b1-4175-c780-3ddc15169cfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [101, 2073, 4521, 10733, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1], 'special_tokens_mask': [1, 0, 0, 0, 1]}\n",
            "{'input_ids': [101, 2054, 2055, 1996, 4633, 2651, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1], 'special_tokens_mask': [1, 0, 0, 0, 0, 0, 1]}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "104f3e03555940f39da26ce99b6dc700"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29\n",
            "[('that', tensor(0.0597, device='cuda:0')), ('where', tensor(0.5860, device='cuda:0')), ('place', tensor(0.8375, device='cuda:0')), ('best', tensor(0.0039, device='cuda:0')), ('country', tensor(0.7220, device='cuda:0')), ('food', tensor(0.6931, device='cuda:0')), ('places', tensor(0.5735, device='cuda:0')), ('culture', tensor(0.1536, device='cuda:0')), ('location', tensor(1.4597, device='cuda:0')), ('famous', tensor(0.3957, device='cuda:0')), ('hotel', tensor(0.4550, device='cuda:0')), ('farm', tensor(0.1238, device='cuda:0')), ('visit', tensor(0.0439, device='cuda:0')), ('website', tensor(0.0010, device='cuda:0')), ('headquarters', tensor(0.3276, device='cuda:0')), ('shop', tensor(0.7899, device='cuda:0')), ('eat', tensor(2.2676, device='cuda:0')), ('restaurant', tensor(1.4008, device='cuda:0')), ('locations', tensor(0.4048, device='cuda:0')), ('favorite', tensor(0.1469, device='cuda:0')), ('anywhere', tensor(0.0411, device='cuda:0')), ('eating', tensor(1.5365, device='cuda:0')), ('habitat', tensor(0.4350, device='cuda:0')), ('venue', tensor(0.2026, device='cuda:0')), ('murphy', tensor(0.1091, device='cuda:0')), ('cafe', tensor(0.4081, device='cuda:0')), ('restaurants', tensor(0.6192, device='cuda:0')), ('pizza', tensor(3.2689, device='cuda:0')), ('pie', tensor(1.9949, device='cuda:0'))]\n",
            "33\n",
            "[('about', tensor(0.5696, device='cuda:0')), ('now', tensor(1.8579, device='cuda:0')), ('world', tensor(0.3520, device='cuda:0')), ('day', tensor(0.1946, device='cuda:0')), ('england', tensor(0.0241, device='cuda:0')), ('summer', tensor(0.6513, device='cuda:0')), ('today', tensor(2.9016, device='cuda:0')), ('science', tensor(0.5336, device='cuda:0')), ('event', tensor(0.2984, device='cuda:0')), ('current', tensor(0.6097, device='cuda:0')), ('earth', tensor(0.0698, device='cuda:0')), ('nature', tensor(0.0734, device='cuda:0')), ('winter', tensor(0.9053, device='cuda:0')), ('joe', tensor(0.0743, device='cuda:0')), ('storm', tensor(0.7230, device='cuda:0')), ('environment', tensor(0.1768, device='cuda:0')), ('ocean', tensor(0.0203, device='cuda:0')), ('brian', tensor(0.1809, device='cuda:0')), ('environmental', tensor(0.0317, device='cuda:0')), ('rain', tensor(0.9287, device='cuda:0')), ('weather', tensor(3.0974, device='cuda:0')), ('climate', tensor(0.7482, device='cuda:0')), ('temperature', tensor(1.1744, device='cuda:0')), ('carl', tensor(0.0505, device='cuda:0')), ('gabriel', tensor(0.0393, device='cuda:0')), ('julian', tensor(0.3135, device='cuda:0')), ('yesterday', tensor(0.4100, device='cuda:0')), ('geography', tensor(0.1029, device='cuda:0')), ('pollution', tensor(0.0797, device='cuda:0')), ('atmospheric', tensor(0.2852, device='cuda:0')), ('comet', tensor(0.0430, device='cuda:0')), ('forecast', tensor(0.3485, device='cuda:0')), ('meteor', tensor(0.3943, device='cuda:0'))]\n"
          ]
        }
      ],
      "source": [
        "# Não tem que ter padding\n",
        "print(dataset_textos[0])\n",
        "print(dataset_textos[1])\n",
        "\n",
        "batch_size = 1\n",
        "dataloader_textos = DataLoader(dataset_textos, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "representacao_esparsa_dataloader(model, tokenizer, dataloader_textos, callback_imprime)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MQrE-T1fTwb"
      },
      "source": [
        "### Dataset e Dataloader TREC-COVID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bv5b-CpOfV6E",
        "outputId": "6361b9e1-15da-4bf0-e879-f7cf8fb7f2fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.2 s, sys: 25.1 ms, total: 1.22 s\n",
            "Wall time: 1.21 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "ids_trec_covid, textos_trec_covid = zip(*corpus_trec_covid)\n",
        "\n",
        "dataset_trec_covid = Dataset(tokenizer, textos_trec_covid, 256)\n",
        "dataloader_trec_covid = DataLoader(dataset_trec_covid, batch_size=batch_size_trec_covid, shuffle=False, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYaDoABQMHNW"
      },
      "source": [
        "### Classe para índice invertido SPLADE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6D1y2vO-f32r"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import array\n",
        "import pickle\n",
        "import math\n",
        "\n",
        "# Definição de uma classe para índice invertido\n",
        "class IndiceInvertidoSplade:\n",
        "\n",
        "  def __init__(self):\n",
        "    # Cria um índice invertido vazio\n",
        "    self.indice = {}\n",
        "\n",
        "  def adiciona_docs(self, ids_docs, wjs_docs):\n",
        "    nonzero = wjs_docs.nonzero()\n",
        "\n",
        "    for i in range(wjs_docs.size()[0]):\n",
        "      idx_linha_i = nonzero[:, 0] == i # i'ésimo doc do batch\n",
        "      idx_token_em_wj_i = nonzero[idx_linha_i, 1] # além de ser o índice na matriz wj, é tb o id do token\n",
        "      val_token_em_wj_i = wjs_docs[i, idx_token_em_wj_i]\n",
        "\n",
        "      self.adiciona_doc(ids_docs[i], idx_token_em_wj_i, val_token_em_wj_i)\n",
        "\n",
        "  def adiciona_doc(self, id_doc, idx_tokens, wj_tokens):\n",
        "    for id, wj in zip(idx_tokens.tolist(), wj_tokens.tolist()):\n",
        "      self.indice.setdefault(id, {\"id_doc\": [], \"wj\": array.array(\"f\", [])})['id_doc'].append(id_doc)\n",
        "      self.indice.setdefault(id, {\"id_doc\": [], \"wj\": array.array(\"f\", [])})['wj'].append(wj)\n",
        "    \n",
        "  def pesquisar(self, wjs_query, splade='v1'):\n",
        "    # Guarda um dicionário onde a chave é o id do documento e o valor é o score desse documento para a query pesquisada\n",
        "    docs_retornado_com_score = Counter({})\n",
        "\n",
        "    # Faz a pesquisa de documentos. Para isso iteramos todos os tokens da query\n",
        "    wjs_query = wjs_query.coalesce()\n",
        "    for id_token_query, wj_do_token_na_query in zip(wjs_query.indices()[0].tolist(), wjs_query.values().tolist()):\n",
        "      # É possível que a query contenha algum termo que não foi indexado. Se isso ocorrer, apenas pula o termo\n",
        "      if id_token_query not in self.indice:\n",
        "        continue\n",
        "\n",
        "      # Pega a lista de documentos que será analisado\n",
        "      docs_que_tem_token = self.indice[id_token_query]['id_doc']\n",
        "      wj_do_token_nos_docs = self.indice[id_token_query]['wj']\n",
        "\n",
        "      # Agora já temos calculado o score de todos os documentos desse token. Só adiciona ao acumulador de score atual\n",
        "      # docs_retornado_com_score += score_dos_docs_deste_token -> Se fosse usar dict direto no índice seria assim, mas a memória não está aguentando guardar os scores de ambos\n",
        "      multiplicador_token_query = wj_do_token_na_query if splade == 'v1' else 1\n",
        "\n",
        "      for id_doc, wj_do_token_no_doc in zip(docs_que_tem_token, wj_do_token_nos_docs):\n",
        "        docs_retornado_com_score[id_doc] += wj_do_token_no_doc * multiplicador_token_query\n",
        "      \n",
        "    # Agora converte esse dict em uma lista de tuplas com a chave (id_doc) e valor (score_do_doc)\n",
        "    docs_com_score = list(docs_retornado_com_score.items())\n",
        "\n",
        "    # E ordena do mais relevante para o menos relevante\n",
        "    return sorted(docs_com_score, key=lambda x: x[1], reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "idx_splade = IndiceInvertidoSplade()\n",
        "\n",
        "def popular_indice_invertido(idx_batch, wj_batch):\n",
        "  ids_doc_batch = [ids_trec_covid[i] for i in idx_batch]\n",
        "  idx_splade.adiciona_docs(ids_doc_batch, wj_batch)\n",
        "\n",
        "def salvar_indice(idx_splade):\n",
        "  nome_arquivo_pickle = param[\"nome_arquivo_indice_invertido_docs\"]\n",
        "  diretorio_destino_cp = f\"'{dir_aula_7}'\"\n",
        "  with open(nome_arquivo_pickle, 'wb') as f:\n",
        "    pickle.dump(idx_splade.indice, f)\n",
        "  !cp {nome_arquivo_pickle} {diretorio_destino_cp}\n",
        "\n",
        "def recuperar_indice(idx_splade):\n",
        "  with open(arq_indice_pickle, 'rb') as f:\n",
        "    idx_splade.indice = pickle.load(f)\n",
        "\n",
        "arq_indice_pickle = f'{dir_aula_7}{param[\"nome_arquivo_indice_invertido_docs\"]}'\n",
        "\n",
        "if gerar_indice_invertido_docs:\n",
        "  representacao_esparsa_dataloader(model, tokenizer, dataloader_trec_covid, popular_indice_invertido)    \n",
        "  salvar_indice(idx_splade)\n",
        "else:\n",
        "  recuperar_indice(idx_splade)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTJgqhhjGMua",
        "outputId": "63a1392d-2adc-4174-8b8b-f7de77d027c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3.79 s, sys: 223 ms, total: 4.02 s\n",
            "Wall time: 4.01 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wfHPnbAw28n"
      },
      "source": [
        "### Testes no TREC-COVID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RleR4e-A0I_I"
      },
      "outputs": [],
      "source": [
        "def run_all_queries_indice_invertido_splade(file, model, tokenizer, idx_splade, splade='v1'):\n",
        "  print('Carregando as queries do arquivo queries.jsonl...\\n')\n",
        "  queries_trec_covid = carrega_queries_trec_covid()\n",
        "\n",
        "  print(f'Total de queries que serão avaliadas: {len(queries_trec_covid)}')\n",
        "  cnt = 0\n",
        "  with open(file, 'w') as runfile:\n",
        "    for query in queries_trec_covid:\n",
        "      id = query['id']\n",
        "      texto = query['texto']\n",
        "\n",
        "      wj_query = representacao_esparsa_do_texto(model, tokenizer, texto, True, param['manter_contribuicao_CLS_SEP_da_matriz_doc'])\n",
        "      if cnt % 10 == 0:\n",
        "        print(f'{cnt} queries completadas')\n",
        "\n",
        "      # Usa o índice invertido pra pesquisar\n",
        "      docs_score = idx_splade.pesquisar(wj_query, splade)\n",
        "\n",
        "      for i in range(0, min(1000, len(docs_score))): # Pega os primeiros 1000 resultados\n",
        "        _ = runfile.write('{} Q0 {} {} {:.6f} BM_25\\n'.format(id, docs_score[i][0], i+1, docs_score[i][1]))\n",
        "\n",
        "      cnt += 1\n",
        "    print(f'{cnt} queries completadas')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kamq0S3qFFN5",
        "outputId": "1abc4b24-5420-437c-c535-3aef5953f246"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando as queries do arquivo queries.jsonl...\n",
            "\n",
            "Total de queries que serão avaliadas: 50\n",
            "0 queries completadas\n",
            "10 queries completadas\n",
            "20 queries completadas\n",
            "30 queries completadas\n",
            "40 queries completadas\n",
            "50 queries completadas\n",
            "Downloading https://search.maven.org/remotecontent?filepath=uk/ac/gla/dcs/terrierteam/jtreceval/0.0.5/jtreceval-0.0.5-jar-with-dependencies.jar to /root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar...\n",
            "/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar already exists!\n",
            "Skipping download.\n",
            "Running command: ['java', '-jar', '/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar', '-c', '-m', 'ndcg_cut.10', 'collections/trec-covid/qrels/test_corrigido.tsv', 'run-splade-iidx-v1.txt']\n",
            "Results:\n",
            "ndcg_cut_10           \tall\t0.7242\n",
            "CPU times: user 54.5 s, sys: 342 ms, total: 54.8 s\n",
            "Wall time: 58.9 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "run_all_queries_indice_invertido_splade('run-splade-iidx-v1.txt', model, tokenizer, idx_splade, 'v1')\n",
        "!python -m pyserini.eval.trec_eval -c -m ndcg_cut.10 collections/trec-covid/qrels/test_corrigido.tsv run-splade-iidx-v1.txt #type: ign"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "## Roda as 50 queries sem fazer mais nada, só pra contar o tempo de execução pra rodar as queries\n",
        "for query in queries_trec_covid:\n",
        "  wj_query = representacao_esparsa_do_texto(model, tokenizer, query['texto'], True, param['manter_contribuicao_CLS_SEP_da_matriz_doc'])\n",
        "  docs_score = idx_splade.pesquisar(wj_query, 'v1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBhj_eqgbrv3",
        "outputId": "b362bf12-b8f8-439b-f7ee-142cfec8aee6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 53.2 s, sys: 224 ms, total: 53.4 s\n",
            "Wall time: 52.6 s\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "eed7f571b042429c885a0d4cd805465a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e56e028ca95945f7b6c797fea07efa2f",
              "IPY_MODEL_25f53b2f3cd44718aa1524aa53225dca",
              "IPY_MODEL_cc8215b0855b48b4b269c60a02148974"
            ],
            "layout": "IPY_MODEL_1f5ecb38f4ad4d08a22da4773d68e90c"
          }
        },
        "e56e028ca95945f7b6c797fea07efa2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a449faab56e240ad8f127b5488320500",
            "placeholder": "​",
            "style": "IPY_MODEL_26ee989002a3494795be7f43e4e324f8",
            "value": "100%"
          }
        },
        "25f53b2f3cd44718aa1524aa53225dca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f81b4425335a4036a7ed53cd0658aad9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_71162205aeda43ae956028e640b236d2",
            "value": 1
          }
        },
        "cc8215b0855b48b4b269c60a02148974": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9b26da6f5dc419db821c4124f105d6c",
            "placeholder": "​",
            "style": "IPY_MODEL_556c0208b05140f6806401b3564eb7f7",
            "value": " 1/1 [00:00&lt;00:00, 12.37it/s]"
          }
        },
        "1f5ecb38f4ad4d08a22da4773d68e90c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a449faab56e240ad8f127b5488320500": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26ee989002a3494795be7f43e4e324f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f81b4425335a4036a7ed53cd0658aad9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71162205aeda43ae956028e640b236d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e9b26da6f5dc419db821c4124f105d6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "556c0208b05140f6806401b3564eb7f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "104f3e03555940f39da26ce99b6dc700": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_633c1116e60b4fc4a0d6da99516e5158",
              "IPY_MODEL_043cbbf710524091a996ee61c6bba5d0",
              "IPY_MODEL_ed9b160f12af4dabb80d3f9d98a06be8"
            ],
            "layout": "IPY_MODEL_e4ab75c6ebd24d7abbc4aef4e94f5e8d"
          }
        },
        "633c1116e60b4fc4a0d6da99516e5158": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd3ae65f77b44e14808097b06ecc1d5c",
            "placeholder": "​",
            "style": "IPY_MODEL_9c0ddf133e1d4e6fa016b370f1c0d376",
            "value": "100%"
          }
        },
        "043cbbf710524091a996ee61c6bba5d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b16929f09de4435689d5d0a7247e8b65",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9b6daca4531a4afc9f6832b9228bb007",
            "value": 2
          }
        },
        "ed9b160f12af4dabb80d3f9d98a06be8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5cebbb21fee452ba311dd25626a6ad1",
            "placeholder": "​",
            "style": "IPY_MODEL_5d154e9f30674aea9f38c3baf2e57bac",
            "value": " 2/2 [00:00&lt;00:00, 24.06it/s]"
          }
        },
        "e4ab75c6ebd24d7abbc4aef4e94f5e8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd3ae65f77b44e14808097b06ecc1d5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c0ddf133e1d4e6fa016b370f1c0d376": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b16929f09de4435689d5d0a7247e8b65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b6daca4531a4afc9f6832b9228bb007": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b5cebbb21fee452ba311dd25626a6ad1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d154e9f30674aea9f38c3baf2e57bac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}