{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fy9O181greFV"
   },
   "source": [
    "# Aula 5 - Solução dos exercícios - Parte 1\n",
    "Leandro Carísio Fernandes\n",
    "\n",
    "<br>\n",
    "\n",
    "Treinar um modelo seq2seq (a partir do T5-base) na tarefa de expansão de documentos.\n",
    "\n",
    "- Usar como treino o dataset \"tiny\" do MS MARCO na tarefa doc2query\n",
    "https://storage.googleapis.com/unicamp-dl/ia368dd_2023s1/msmarco/msmarco_triples.train.tiny.tsv\n",
    "- doc2query: A entrada é a passagem e o target é a query\n",
    "- Note que apenas pares (query, passagem relevante) são usados como treino.\n",
    "- O treino é relativamente rápido (<1 hora).\n",
    "- Validar a cada X steps usando o sacreBLEU \n",
    "- A parte lenta deste exercício é a pré-indexação: para cada documento da coleção, temos que gerar uma ou mais queries, que depois são concatenadas ao documento original, e esse documento \"expandido\" é indexado.\n",
    "- Avaliar no TREC-COVID (171K docs), pois é menor que o MS MARCO/TREC-DL 2020 (8.8M passagens). \n",
    " - Indice invertido do Trec-covid no pyserini: beir-v1.0.0-trec-covid-flat\n",
    " - Corpus e queries na HF: https://huggingface.co/datasets/BeIR/trec-covid\n",
    " - qrels: https://huggingface.co/datasets/BeIR/trec-covid-qrels\n",
    " - Usar nDCG@10\n",
    " - Comparar com o BM25 com e sem os documentos expandidos pelo doc2query\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "Esse caderno contém apenas a parte 1 do exercício, a implementação do BM25 sem expansão de query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYh8vgNZ3cXG"
   },
   "source": [
    "## Preparação do ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ImSgg3xsChy"
   },
   "source": [
    "### Variáveis usadas para controlar o fluxo do caderno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MeN0iAv6x9nx"
   },
   "outputs": [],
   "source": [
    "# Forçar refazer o índice invertido\n",
    "forcar_recalcular_indice_trec_covid = False\n",
    "\n",
    "calcular_bm25_pre_built_index = True\n",
    "\n",
    "# Link de download da base TREC-COVID. O link foi retirado de https://huggingface.co/datasets/BeIR/trec-covid\n",
    "url_trec_covid = 'https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/trec-covid.zip'\n",
    "\n",
    "# O qrels que está no url_trec_covid tem umas poucas linhas a mais do que o qrels no repositório do anserini. Vou baixar aqui também pra testar\n",
    "url_qrels_anserini = 'https://raw.githubusercontent.com/castorini/anserini/master/src/main/resources/topics-and-qrels/qrels.beir-v1.0.0-trec-covid.test.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCPB7MdEsXU_"
   },
   "source": [
    "### Instalação de libs e montagem do Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PfN0LwYqsebw",
    "outputId": "deaf8737-4316-4091-cfcf-0a8c9f72005e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pyserini\n",
      "  Downloading pyserini-0.21.0-py3-none-any.whl (154.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.1/154.1 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy>=3.2.1 in /usr/local/lib/python3.9/dist-packages (from pyserini) (3.5.1)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from pyserini) (6.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from pyserini) (1.10.1)\n",
      "Collecting sentencepiece>=0.1.95\n",
      "  Downloading sentencepiece-0.1.98-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from pyserini) (1.5.3)\n",
      "Collecting pyjnius>=1.4.0\n",
      "  Downloading pyjnius-1.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.9/dist-packages (from pyserini) (1.22.4)\n",
      "Collecting onnxruntime>=1.8.1\n",
      "  Downloading onnxruntime-1.14.1-cp39-cp39-manylinux_2_27_x86_64.whl (5.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m99.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from pyserini) (4.65.0)\n",
      "Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.9/dist-packages (from pyserini) (1.2.2)\n",
      "Collecting transformers>=4.6.0\n",
      "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: lightgbm>=3.3.2 in /usr/local/lib/python3.9/dist-packages (from pyserini) (3.3.5)\n",
      "Requirement already satisfied: Cython>=0.29.21 in /usr/local/lib/python3.9/dist-packages (from pyserini) (0.29.34)\n",
      "Collecting nmslib>=2.1.1\n",
      "  Downloading nmslib-2.1.1-cp39-cp39-manylinux2010_x86_64.whl (13.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from lightgbm>=3.3.2->pyserini) (0.40.0)\n",
      "Collecting pybind11<2.6.2\n",
      "  Downloading pybind11-2.6.1-py2.py3-none-any.whl (188 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from nmslib>=2.1.1->pyserini) (5.9.4)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from onnxruntime>=1.8.1->pyserini) (1.11.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from onnxruntime>=1.8.1->pyserini) (23.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.9/dist-packages (from onnxruntime>=1.8.1->pyserini) (3.20.3)\n",
      "Collecting coloredlogs\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.9/dist-packages (from onnxruntime>=1.8.1->pyserini) (23.3.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.4.0->pyserini) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.4.0->pyserini) (2.8.2)\n",
      "Requirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from pyjnius>=1.4.0->pyserini) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.22.1->pyserini) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.22.1->pyserini) (1.2.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.2.1->pyserini) (1.0.9)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.2.1->pyserini) (2.0.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.2.1->pyserini) (3.3.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.2.1->pyserini) (8.1.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.2.1->pyserini) (2.0.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.2.1->pyserini) (2.27.1)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.2.1->pyserini) (0.7.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.2.1->pyserini) (3.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.2.1->pyserini) (2.4.6)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.2.1->pyserini) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.2.1->pyserini) (6.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.2.1->pyserini) (3.0.12)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.2.1->pyserini) (1.10.7)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy>=3.2.1->pyserini) (67.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.2.1->pyserini) (3.1.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.2.1->pyserini) (1.1.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy>=3.2.1->pyserini) (1.0.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.6.0->pyserini) (2022.10.31)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers>=4.6.0->pyserini) (3.11.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers>=4.6.0->pyserini) (4.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.1->pyserini) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.1->pyserini) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.1->pyserini) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.1->pyserini) (3.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.2.1->pyserini) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=3.2.1->pyserini) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy>=3.2.1->pyserini) (8.1.3)\n",
      "Collecting humanfriendly>=9.1\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy>=3.2.1->pyserini) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->onnxruntime>=1.8.1->pyserini) (1.3.0)\n",
      "Installing collected packages: tokenizers, sentencepiece, pyjnius, pybind11, humanfriendly, nmslib, huggingface-hub, coloredlogs, transformers, onnxruntime, pyserini\n",
      "Successfully installed coloredlogs-15.0.1 huggingface-hub-0.13.4 humanfriendly-10.0 nmslib-2.1.1 onnxruntime-1.14.1 pybind11-2.6.1 pyjnius-1.4.2 pyserini-0.21.0 sentencepiece-0.1.98 tokenizers-0.13.3 transformers-4.27.4\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.7.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.7.3\n",
      "CPU times: user 290 ms, sys: 56.1 ms, total: 346 ms\n",
      "Wall time: 31.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Já monta o drive, pois vamos usar o índice invertido da Aula 1 para usar o BM25 implementado também na aula 1\n",
    "# Além disso, é necessário para salvar/recuperar o modelo tunado\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Instala libs\n",
    "!pip install pyserini\n",
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HWCfwsXWxzdy"
   },
   "source": [
    "### Download do TREC-COVID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VdIzngktx5fc",
    "outputId": "c4dd543d-2506-468a-ef37-12cfaf94bda5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-04-12 13:56:47--  https://raw.githubusercontent.com/castorini/anserini/master/src/main/resources/topics-and-qrels/qrels.beir-v1.0.0-trec-covid.test.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2023-04-12 13:56:47 ERROR 404: Not Found.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import hashlib\n",
    "\n",
    "# Baixa o trec-covid.zip\n",
    "if not Path('./collections/trec-covid.zip').is_file():\n",
    "  # O arquivo ainda não foi baixado. Verifica se está no drive:\n",
    "  if Path('/content/drive/My Drive/IA368-DD_deep_learning_busca/Aula5-t5-doc2query/trec-covid.zip').is_file():\n",
    "    !mkdir -p './collections/' && cp '/content/drive/My Drive/IA368-DD_deep_learning_busca/Aula5-t5-doc2query/trec-covid.zip' './collections/trec-covid.zip'  # type: ignore\n",
    "    !unzip -o collections/trec-covid.zip -d ./collections # type: ignore\n",
    "  else:\n",
    "    !wget {url_trec_covid} -P collections # type: ignore\n",
    "    !mkdir -p '/content/drive/My Drive/IA368-DD_deep_learning_busca/Aula5-t5-doc2query/' && cp './collections/trec-covid.zip' '/content/drive/My Drive/IA368-DD_deep_learning_busca/Aula5-t5-doc2query/'\n",
    "    !unzip -o collections/trec-covid.zip -d ./collections # type: ignore\n",
    "\n",
    "# Converte o qrels que veio no trec-covid.zip pra um formato mais adequado:\n",
    "with open('./collections/trec-covid/qrels/test.tsv', 'r') as fin:\n",
    "  data = fin.read().splitlines(True)\n",
    "with open('./collections/trec-covid/qrels/test_corrigido.tsv', 'w') as fout:\n",
    "  for linha in data[1:]:\n",
    "    campos = linha.split()\n",
    "    fout.write(f'{campos[0]}\\t0\\t{campos[1]}\\t{campos[2]}\\n')\n",
    "\n",
    "# Carrega o qrels do repositório do Anserini\n",
    "if not Path('./collections/trec-covid/qrels/qrels.beir-v1.0.0-trec-covid.test.txt').is_file():\n",
    "  !wget {url_qrels_anserini} -P collections/trec-covid/qrels # type ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGo5E4nMs1cw"
   },
   "source": [
    "### Classes para índice invertido e pesquisa com BM25\n",
    "\n",
    "Nessa seção vou usar as implementações de índice invertido e BM25 que eu já havia feito na Aula 2.\n",
    "\n",
    "Entretanto, algumas adaptações foram necessárias. Por exemplo:\n",
    "\n",
    "- A ID dos documentos na avaliação anterior era um inteiro, e isso era armazenado na classe IndiceInvertido como um array.array. No TREC_COVID a ID é uma string, então a classe teve que ser alterada para usar uma lista em vez de array.array.\n",
    "- A classe de pesquisa do BM25 antes calculava o score tanto pelo BM25 quanto pelo TF/IDF. Removi essa última parte pois não é relevante para esse exercício.\n",
    "\n",
    "\n",
    "Implementação das classes IndiceInvertido e BM25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GhvWHnQztCiC",
    "outputId": "57a83f6d-dab8-4d78-9028-45ec06e1f85c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 379 ms, sys: 48.8 ms, total: 427 ms\n",
      "Wall time: 627 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyserini.analysis import Analyzer, get_lucene_analyzer\n",
    "from collections import Counter\n",
    "import array\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "# Definição de uma classe para índice invertido\n",
    "class IndiceInvertido:\n",
    "  lista_stopwords_nltk = set([\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"])\n",
    "\n",
    "  # Recebe 'tokenizar', uma função tokenizadora\n",
    "  def __init__(self):\n",
    "    # Cria um índice invertido vazio\n",
    "    self.indice = {}\n",
    "    # Cria um índice de tamanho de documentos vazio\n",
    "    self.tamanho_doc = {}\n",
    "    # Guarda o total de documentos adicionados\n",
    "    self.n_docs = 0\n",
    "    # Tokenizador\n",
    "    self.lucene_analyser = Analyzer(get_lucene_analyzer(stemmer='porter'))\n",
    "\n",
    "  def tokenizar(self, texto):\n",
    "    return self.lucene_analyser.analyze(texto)\n",
    "\n",
    "  def adiciona_doc(self, id_doc, conteudo_doc=None):\n",
    "    tokens = self.tokenizar(conteudo_doc)\n",
    "\n",
    "    contador_tokens_do_documento = Counter(tokens)\n",
    "    for token, n_ocorrencias in contador_tokens_do_documento.items():\n",
    "      self.indice.setdefault(token, {\"id_doc\": [], \"n_ocorrencias\": array.array(\"L\", [])})['id_doc'].append(id_doc)\n",
    "      self.indice.setdefault(token, {\"id_doc\": [], \"n_ocorrencias\": array.array(\"L\", [])})['n_ocorrencias'].append(n_ocorrencias)\n",
    "    \n",
    "    self.n_docs += 1\n",
    "    self.tamanho_doc[id_doc] = len(tokens)\n",
    "\n",
    "class BM25:\n",
    "\n",
    "  def __init__(self, indiceInvertido=IndiceInvertido(), k1 = 0.9, b = 0.4, bias_adicionar_ao_idf = 0):\n",
    "    self.indiceInvertido = indiceInvertido\n",
    "    self.bias_adicionar_ao_idf = bias_adicionar_ao_idf\n",
    "    self.calcula_tam_medio_doc_no_indice()\n",
    "    self.k1 = k1\n",
    "    self.b = b\n",
    "    self.precalcula_idf()\n",
    "    self.reinicia_score_dos_indices()\n",
    "\n",
    "  def reinicia_score_dos_indices(self):\n",
    "    for token in self.indiceInvertido.indice.keys():\n",
    "      self.indiceInvertido.indice[token].pop('score', None)\n",
    "  \n",
    "  def calcula_tam_medio_doc_no_indice(self):\n",
    "    self.avgdl = sum(self.indiceInvertido.tamanho_doc.values()) / self.indiceInvertido.n_docs\n",
    "\n",
    "  def precalcula_idf(self):\n",
    "    # Número de documento do corpus está presente no objeto indiceInvertido\n",
    "    N = self.indiceInvertido.n_docs\n",
    "    # Varre todos os tokens do índice. Os tokens são as chaves do indiceInvertido.indice\n",
    "    for token in self.indiceInvertido.indice.keys():\n",
    "      # O número de documentos que possui o token é calculado pelo tamanho da lista de id_doc:\n",
    "      n_doc_token = len(self.indiceInvertido.indice[token]['id_doc'])\n",
    "      # Isso já é o suficiente pra calcular o idf\n",
    "      idf_token = math.log( ((self.indiceInvertido.n_docs - n_doc_token + 0.5)/(n_doc_token + 0.5)) + self.bias_adicionar_ao_idf )\n",
    "      # E agora, vamos colocar essa informação no índice\n",
    "      self.indiceInvertido.indice[token]['idf'] = idf_token\n",
    "\n",
    "  def calcula_score_para_um_token_e_salva(self, token):\n",
    "    # O cálculo do BM25 para determinada query é a multiplicação do idf pela frequência do termo no documento * (k1 + 1)\n",
    "    # Além disso, é dividido pela frequencia do termo no documento + k1 * (1 - b + b * tamanho_doc/avgdl)\n",
    "    idf = self.indiceInvertido.indice[token]['idf']\n",
    "    # Juntando tudo, podemos calcular o score pelo BM25\n",
    "    zip_id_freq = zip(self.indiceInvertido.indice[token]['id_doc'], self.indiceInvertido.indice[token]['n_ocorrencias'])   \n",
    "    bm25 = array.array(\"f\", [ idf * freq_token_no_doc * (self.k1 + 1) / (freq_token_no_doc + self.k1 * (1 - self.b + self.b * self.indiceInvertido.tamanho_doc[id_doc] / self.avgdl)) for (id_doc, freq_token_no_doc) in zip_id_freq ])\n",
    "    # Salva o bm25 no índice\n",
    "    self.indiceInvertido.indice[token]['score'] = bm25\n",
    "\n",
    "  def tokenizar(self, query):\n",
    "    return self.indiceInvertido.tokenizar(query)\n",
    "\n",
    "  def pesquisar(self, query):\n",
    "    # Tokeniza a query\n",
    "    tokens = self.tokenizar(query)\n",
    "\n",
    "    # Se não tem token para ser pesquisado, retorna conjunto vazio\n",
    "    if (len(tokens) == 0):\n",
    "      return []\n",
    "\n",
    "    # Guarda um dicionário onde a chave é o id do documento e o valor é o score desse documento para a query pesquisada\n",
    "    docs_retornado_com_score = Counter({})\n",
    "\n",
    "    # Faz a pesquisa de documentos. Para isso iteramos todos os tokens da query\n",
    "    for token in tokens:\n",
    "      # É possível que a query contenha algum termo que não foi indexado. Se isso ocorrer,\n",
    "      # entende-se que a frequência desse token em qualquer documento é 0, já que não pode ser encontrado\n",
    "      if token not in self.indiceInvertido.indice:\n",
    "        continue\n",
    "\n",
    "      # Pega a lista de documentos que será analisado\n",
    "      docs_que_tem_token = self.indiceInvertido.indice[token]['id_doc']\n",
    "      \n",
    "      # Se for a primeira vez que esse token é pesquisado, é necessário calcular o score relacionado\n",
    "      # a ele e salvar. Se já tiver sido feito antes, já podemos buscar o cálculo pronto (que funciona\n",
    "      # como um cache. Isso é útil no caso de várias pesquisas seguidas)\n",
    "      if 'score' not in self.indiceInvertido.indice[token].keys():\n",
    "        self.calcula_score_para_um_token_e_salva(token)\n",
    "      score_dos_docs_deste_token = self.indiceInvertido.indice[token]['score']\n",
    "\n",
    "      # Agora já temos calculado o score de todos os documentos desse token. Só adiciona ao acumulador de score atual\n",
    "      # docs_retornado_com_score += score_dos_docs_deste_token -> Se fosse usar dict direto no índice seria assim, mas a memória não está aguentando guardar os scores de ambos\n",
    "      for id_doc, score_par_doc_token in zip(docs_que_tem_token, score_dos_docs_deste_token):\n",
    "        docs_retornado_com_score[id_doc] += score_par_doc_token\n",
    "\n",
    "    # Agora converte esse dict em uma lista de tuplas com a chave (id_doc) e valor (score_do_doc)\n",
    "    docs_com_score = list(docs_retornado_com_score.items())\n",
    "\n",
    "    # E ordena do mais relevante para o menos relevante\n",
    "    return sorted(docs_com_score, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QdcTjoJe7Zam"
   },
   "source": [
    "### Funções para carregar e processar o corpus TREC_COVID e carregar as queries e qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-kKnxyGW6y7v",
    "outputId": "36a76c2c-57a7-4c41-c7f1-8c9fb037470d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 517 µs, sys: 0 ns, total: 517 µs\n",
      "Wall time: 864 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import json\n",
    "\n",
    "def processa_corpus_trec_covid(func_processa_registro):\n",
    "  with open('./collections/trec-covid/corpus.jsonl') as corpus:\n",
    "    for i, line in enumerate(corpus):\n",
    "      doc = json.loads(line)\n",
    "      func_processa_registro(doc)\n",
    "      if (i % 10000 == 0):\n",
    "        print(f'Processado {i} documentos')\n",
    "\n",
    "def carrega_queries_trec_covid():\n",
    "  retorno = []\n",
    "  with open('./collections/trec-covid/queries.jsonl') as queries:\n",
    "    for line in queries:\n",
    "      query = json.loads(line)\n",
    "      # Faz apenas uma pequena tradução de _id para id e text para texto\n",
    "      retorno.append({'id': query['_id'], 'texto': query['text']})\n",
    "  return retorno\n",
    "\n",
    "queries_trec_covid = carrega_queries_trec_covid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6z-n4skCO3T"
   },
   "source": [
    "### Função para executar as queries em um buscador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "QAcapLO_CSx5"
   },
   "outputs": [],
   "source": [
    "# Roda todas as queries\n",
    "def run_all_queries(file, buscador_bm25):\n",
    "  print('Carregando as queries do arquivo queries.jsonl...\\n')\n",
    "  queries_trec_covid = carrega_queries_trec_covid()\n",
    "\n",
    "  print(f'Total de queries que serão avaliadas: {len(queries_trec_covid)}')\n",
    "  cnt = 0\n",
    "  with open(file, 'w') as runfile:\n",
    "    for query in queries_trec_covid:\n",
    "      id = query['id']\n",
    "      texto = query['texto']\n",
    "\n",
    "      if cnt % 10 == 0:\n",
    "        print(f'{cnt} queries completadas')\n",
    "\n",
    "      # Usa o BM25 para buscar\n",
    "      docs_score = buscador_bm25.pesquisar(texto)\n",
    "\n",
    "      for i in range(0, min(1000, len(docs_score))): # Pega os primeiros 1000 resultados\n",
    "        _ = runfile.write('{} Q0 {} {} {:.6f} BM_25\\n'.format(id, docs_score[i][0], i+1, docs_score[i][1]))\n",
    "\n",
    "      cnt += 1\n",
    "    print(f'{cnt} queries completadas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jt9l014a3nM5"
   },
   "source": [
    "## Análise do BM25 na base TREC-COVID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqJgOgME3wRH"
   },
   "source": [
    "### Carregando o corpus.jsonl no índice invertido\n",
    "\n",
    "O arquivo corpus.jsonl possui um documento por linha, sendo que todos possuem o seguinte formato:\n",
    "\n",
    "    {\"_id\": ..., \"title\": ..., \"metadata\": ...}\n",
    "\n",
    "Nos interessa aqui apenas o title e o _id. Vamos criar o índice invertido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CGaQK1kc4gab",
    "outputId": "7b7b4a48-1efc-4a24-d2d2-fceec73ced4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processado 0 documentos\n",
      "Processado 10000 documentos\n",
      "Processado 20000 documentos\n",
      "Processado 30000 documentos\n",
      "Processado 40000 documentos\n",
      "Processado 50000 documentos\n",
      "Processado 60000 documentos\n",
      "Processado 70000 documentos\n",
      "Processado 80000 documentos\n",
      "Processado 90000 documentos\n",
      "Processado 100000 documentos\n",
      "Processado 110000 documentos\n",
      "Processado 120000 documentos\n",
      "Processado 130000 documentos\n",
      "Processado 140000 documentos\n",
      "Processado 150000 documentos\n",
      "Processado 160000 documentos\n",
      "Processado 170000 documentos\n",
      "CPU times: user 1min 35s, sys: 1.28 s, total: 1min 36s\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def salvar_indice_em_arquivo_pickle(indice_invertido, nome_arquivo):\n",
    "  with open(f'{nome_arquivo}_indice.pickle', 'wb') as f:\n",
    "    pickle.dump(indice_invertido.indice, f)\n",
    "  with open(f'{nome_arquivo}_tamanho_doc.pickle', 'wb') as f:\n",
    "    pickle.dump(indice_invertido.tamanho_doc, f)\n",
    "\n",
    "def montar_indice_a_partir_arquivo_pickle(nome_arquivo):\n",
    "  idx = IndiceInvertido()\n",
    "\n",
    "  with open(f'{nome_arquivo}_indice.pickle', 'rb') as f:\n",
    "    idx.indice = pickle.load(f)\n",
    "  with open(f'{nome_arquivo}_tamanho_doc.pickle', 'rb') as f:\n",
    "    idx.tamanho_doc = pickle.load(f)\n",
    "  idx.n_docs = len(idx.tamanho_doc)\n",
    "\n",
    "  return idx\n",
    "\n",
    "def criar_indice_invertido_trec_covid():\n",
    "  iidx_trec_covid = IndiceInvertido()\n",
    "\n",
    "  # Carrega todo o corpus no índice invertido\n",
    "  processa_corpus_trec_covid(lambda doc : iidx_trec_covid.adiciona_doc(doc['_id'], f\"{doc['title']} {doc['text']}\"))\n",
    "\n",
    "  # Salva no drive a última versão do índice\n",
    "  salvar_indice_em_arquivo_pickle(iidx_trec_covid, 'iidx_trec_covid')\n",
    "\n",
    "  # Copia arquivos de índice e tamanho do documento para o google drive:\n",
    "  !cp iidx_trec_covid*.pickle '/content/drive/My Drive/IA368-DD_deep_learning_busca/Aula5-t5-doc2query/'  # type: ignore\n",
    "  \n",
    "  return iidx_trec_covid\n",
    "\n",
    "def copiar_indice_invertido_do_drive():\n",
    "  iidx_trec_covid = IndiceInvertido()\n",
    "\n",
    "  if Path('/content/drive/My Drive/IA368-DD_deep_learning_busca/Aula5-t5-doc2query/iidx_trec_covid_indice.pickle').is_file():\n",
    "    print('Copiando índice do drive...')\n",
    "    !cp '/content/drive/My Drive/IA368-DD_deep_learning_busca/Aula5-t5-doc2query/iidx_trec_covid_indice.pickle' './iidx_trec_covid_indice.pickle'  # type: ignore\n",
    "    !cp '/content/drive/My Drive/IA368-DD_deep_learning_busca/Aula5-t5-doc2query/iidx_trec_covid_tamanho_doc.pickle' './iidx_trec_covid_tamanho_doc.pickle'  # type: ignore\n",
    "    \n",
    "    iidx_trec_covid = montar_indice_a_partir_arquivo_pickle('iidx_trec_covid')\n",
    "\n",
    "  else:\n",
    "    iidx_trec_covid = criar_indice_invertido_trec_covid()\n",
    "\n",
    "  return iidx_trec_covid\n",
    "\n",
    "iidx_trec_covid = criar_indice_invertido_trec_covid() if forcar_recalcular_indice_trec_covid else copiar_indice_invertido_do_drive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQ9MOwXj6UVF"
   },
   "source": [
    "### Avalia o nDCG@10 nas 50 queries disponíveis\n",
    "\n",
    "Nota: Vou avaliar duas vezes o qrels, uma no arquivo que está no trec-covid.zip e outra no arquivo equivalente do repositório do Anserini. Ambos os arquivos são equivalentes, exceto por duas entradas com score negativo que existem no trec-covid.zip que não existe no Anserini:\n",
    "\n",
    "- 38\t9hbib8b3\t-1\n",
    "- 50\tsvo94kuo\t-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h2QQs2yp6RU0",
    "outputId": "91161f70-7bb7-4f71-ec57-e4314b645756"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando as queries do arquivo queries.jsonl...\n",
      "\n",
      "Total de queries que serão avaliadas: 50\n",
      "0 queries completadas\n",
      "10 queries completadas\n",
      "20 queries completadas\n",
      "30 queries completadas\n",
      "40 queries completadas\n",
      "50 queries completadas\n",
      "Avaliando nDCG usando o qrels que está no HugingFace:\n",
      "\n",
      "Downloading https://search.maven.org/remotecontent?filepath=uk/ac/gla/dcs/terrierteam/jtreceval/0.0.5/jtreceval-0.0.5-jar-with-dependencies.jar to /root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar...\n",
      "jtreceval-0.0.5-jar-with-dependencies.jar: 1.79MB [00:00, 4.26MB/s]                \n",
      "Running command: ['java', '-jar', '/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar', '-c', '-m', 'ndcg_cut.10', 'collections/trec-covid/qrels/test_corrigido.tsv', 'run-bm25-sem-expansao-query-trec-covid.txt']\n",
      "Results:\n",
      "ndcg_cut_10           \tall\t0.5956\n",
      "CPU times: user 14 s, sys: 143 ms, total: 14.2 s\n",
      "Wall time: 27.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#buscador = BM25(iidx_trec_covid, 0.9, 0.4, 1)\n",
    "buscador = BM25(iidx_trec_covid, 0.82, 0.68, 1)\n",
    "run_all_queries('run-bm25-sem-expansao-query-trec-covid.txt', buscador)\n",
    "\n",
    "print('Avaliando nDCG usando o qrels que está no HugingFace:\\n')\n",
    "!python -m pyserini.eval.trec_eval -c -m ndcg_cut.10 collections/trec-covid/qrels/test_corrigido.tsv run-bm25-sem-expansao-query-trec-covid.txt #type: ignore\n",
    "\n",
    "#print('\\nAvaliando nDCG usando o qrels que está no Anserini:\\n')\n",
    "#!python -m pyserini.eval.trec_eval -c -m ndcg_cut.10 collections/trec-covid/qrels/qrels.beir-v1.0.0-trec-covid.test.txt run-bm25-sem-expansao-query-trec-covid.txt #type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jx5khxCZ18ul"
   },
   "source": [
    "Faz o mesmo teste com o índice disponível no Lucene pra validar a implementação do BM25 usada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5m2KQQ7tFwx-",
    "outputId": "b4513c57-2ef0-4beb-a760-9a04ff9b0fc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading index at https://rgw.cs.uwaterloo.ca/pyserini/indexes/lucene-index.beir-v1.0.0-trec-covid.flat.20221116.505594.tar.gz...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lucene-index.beir-v1.0.0-trec-covid.flat.20221116.505594.tar.gz: 216MB [00:08, 26.1MB/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lucene-index.beir-v1.0.0-trec-covid.flat.20221116.505594.57b812594b11d064a23123137ae7dade\n",
      "Running 50 queries in total\n",
      "Downloading https://search.maven.org/remotecontent?filepath=uk/ac/gla/dcs/terrierteam/jtreceval/0.0.5/jtreceval-0.0.5-jar-with-dependencies.jar to /root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar...\n",
      "/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar already exists!\n",
      "Skipping download.\n",
      "Running command: ['java', '-jar', '/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar', '-c', '-m', 'ndcg_cut.10', 'collections/trec-covid/qrels/test_corrigido.tsv', 'run.trec-covid-bm25-pyserini.txt']\n",
      "Results:\n",
      "ndcg_cut_10           \tall\t0.5963\n"
     ]
    }
   ],
   "source": [
    "from pyserini.search.lucene import LuceneSearcher\n",
    "\n",
    "def run_all_queries_pyserini(file, searcher):\n",
    "    with open(file, 'w') as runfile:\n",
    "        cnt = 0\n",
    "        print('Running {} queries in total'.format(len(queries_trec_covid)))\n",
    "        for q in queries_trec_covid:\n",
    "            query = q['texto']\n",
    "            hits = searcher.search(query, 10) # Gera só os 10 primeiros resultados da query\n",
    "            for i in range(0, len(hits)):\n",
    "                _ = runfile.write('{} Q0 {} {} {:.6f} Anserini\\n'.format(q['id'], hits[i].docid, i+1, hits[i].score))\n",
    "            cnt += 1\n",
    "            if cnt % 100 == 0:\n",
    "                print(f'{cnt} queries completed')\n",
    "\n",
    "if calcular_bm25_pre_built_index:\n",
    "  LuceneSearcher.from_prebuilt_index('beir-v1.0.0-trec-covid.flat')\n",
    "  !ls ~/.cache/pyserini/indexes/ #type: ignore\n",
    "  !mv ~/.cache/pyserini/indexes/lucene-index.beir-v1.0.0-trec-covid.flat.20221116.505594.57b812594b11d064a23123137ae7dade ./\n",
    "\n",
    "  searcher = LuceneSearcher('./lucene-index.beir-v1.0.0-trec-covid.flat.20221116.505594.57b812594b11d064a23123137ae7dade')\n",
    "  searcher.set_bm25(k1=float(0.82), b=float(0.68))\n",
    "\n",
    "  run_all_queries_pyserini('run.trec-covid-bm25-pyserini.txt', searcher)\n",
    "\n",
    "  !python -m pyserini.eval.trec_eval -c -m ndcg_cut.10 collections/trec-covid/qrels/test_corrigido.tsv run.trec-covid-bm25-pyserini.txt #type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7g31myIe7yEs"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6aoyNb0NvygE",
    "outputId": "388bbce3-bf2f-491a-b9b3-1d39bef040f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando as queries do arquivo queries.jsonl...\n",
      "\n",
      "Total de queries que serão avaliadas: 50\n",
      "0 queries completadas\n",
      "10 queries completadas\n",
      "20 queries completadas\n",
      "30 queries completadas\n",
      "40 queries completadas\n",
      "50 queries completadas\n",
      "Avaliando nDCG usando o qrels que está no HugingFace:\n",
      "\n",
      "Downloading https://search.maven.org/remotecontent?filepath=uk/ac/gla/dcs/terrierteam/jtreceval/0.0.5/jtreceval-0.0.5-jar-with-dependencies.jar to /root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar...\n",
      "/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar already exists!\n",
      "Skipping download.\n",
      "Running command: ['java', '-jar', '/root/.cache/pyserini/eval/jtreceval-0.0.5-jar-with-dependencies.jar', '-c', '-m', 'ndcg_cut.10', 'collections/trec-covid/qrels/test_corrigido.tsv', 'teste.txt']\n",
      "Results:\n",
      "ndcg_cut_10           \tall\t0.6255\n",
      "CPU times: user 13 s, sys: 126 ms, total: 13.1 s\n",
      "Wall time: 20.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Esse é só um teste rodando com outros parâmetros de k1 e b\n",
    "buscador = BM25(iidx_trec_covid, 1.16, 0.42, 1)\n",
    "run_all_queries('teste.txt', buscador)\n",
    "\n",
    "print('Avaliando nDCG usando o qrels que está no HugingFace:\\n')\n",
    "!python -m pyserini.eval.trec_eval -c -m ndcg_cut.10 collections/trec-covid/qrels/test_corrigido.tsv teste.txt #type: ignore\n",
    "\n",
    "#print('\\nAvaliando nDCG usando o qrels que está no Anserini:\\n')\n",
    "#!python -m pyserini.eval.trec_eval -c -m ndcg_cut.10 collections/trec-covid/qrels/qrels.beir-v1.0.0-trec-covid.test.txt run-bm25-sem-expansao-query-trec-covid.txt #type: ignore"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
