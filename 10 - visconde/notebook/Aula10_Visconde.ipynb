{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Aula 10 - Visconde\n",
        "\n",
        "Leandro Carísio Fernandes\n",
        "\n",
        "\n",
        "Implementar um pipeline multidoc QA: dado uma pergunta do usuário, buscamos em uma grande coleção as passagens mais relevantes e as enviamos para um sistema agregador, que irá gerar uma resposta final.\n",
        "\n",
        "- Avaliar no dataset do IIRC\n",
        "- Métrica principal: F1\n",
        "- Limitar dataset de teste para 50 exemplos para economizar.\n",
        "- Usar o gpt-3.5-turbo como modelo agregador. Usar vicuna-13B como alternativa open-source:\n",
        "\n",
        " - https://huggingface.co/helloollel/vicuna-13b\n",
        " - https://chat.lmsys.org/\n",
        "\n",
        "Dicas:\n",
        "\n",
        "- Se inspirar no pipeline do Visconde: https://github.com/neuralmind-ai/visconde\n",
        "\n"
      ],
      "metadata": {
        "id": "3R9PWpMqnDAJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup - variáveis, instalação de libs, conexão com o Drive etc"
      ],
      "metadata": {
        "id": "CVkwwuT_nS71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se True, segmenta os artigos e salva num pickle. Se False, apenas recupera o pickle do drive\n",
        "gerar_artigos_segmentados = False\n",
        "arquivo_artigos_segmentados = 'conteudo_artigos_segmentados.pickle'\n",
        "\n",
        "# Se True, gera o índice invertido e salva em um pickle. Se False, apenas recupera o pickle do drive\n",
        "gerar_indice_invertido = False\n",
        "arquivo_indice_invertido = 'iidx_artigos_segmentados' # A função salva em 2 arquivos, então ela mesma coloca a extensão pickle\n",
        "\n",
        "# Se True, faz o rankeamento (nesse ponto é bom ter a GPU ligada, senão vai demorar umas 3-4 horas pra fazer apenas na CPU) e salva\n",
        "# Se False, recupera o reranking do arquivo pickle\n",
        "gerar_segmentos_relevantes_para_queries = False\n",
        "arquivo_seg_relevantes_para_queries = 'seg_relevantes_para_queries.pickle'\n",
        "batch_size = 1024 # Para reranking (1024 precisa de menos de 11 GB de RAM pro minilm)\n",
        "\n",
        "# Se True, faz a agregação usando GPT-3.5-turbo e salva. Se False, recupera do arquivo pickle\n",
        "agregar_usando_gpt35turbo = False\n",
        "arquivo_respostas_geradas = 'respostas_geradas_usando_gpt3.5-turbo-few-shot.pickle'\n",
        "\n",
        "dir_aula10 = \"/content/drive/My Drive/IA368-DD_deep_learning_busca/Aula10-visconde/\"\n",
        "\n",
        "# Não estou conseguindo usar o MonoT5. Vou testar com o classificador binário da aula 2\n",
        "modelo_classificador_binario = '/content/drive/My Drive/IA368-DD_deep_learning_busca/Aula2-classificador-binario-mini-bert/'"
      ],
      "metadata": {
        "id": "rvvS3v4BnYQj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71HpQXf9Ru5-",
        "outputId": "841a1d1c-6c8b-468e-a29f-713d537b7884"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyserini -q\n",
        "!pip install faiss-cpu -q\n",
        "!pip install transformers -q\n",
        "!pip install openai -q"
      ],
      "metadata": {
        "id": "oFZr1IJVVXxV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download da base de dados\n",
        "\n",
        "Link para a base IIRC: https://github.com/jferguson144/IIRC-baseline\n",
        "\n",
        "Baixa arquivo iirc.tar.gz e context_articles.tar.gz conforme setup.sh do IIRC: https://github.com/jferguson144/IIRC-baseline/blob/main/setup.sh"
      ],
      "metadata": {
        "id": "JWvjqvGTmiTQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nu4c9G_rmZn_",
        "outputId": "a4c9616c-49be-4256-8dfa-869e95554b4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-17 19:02:46--  http://jamesf-incomplete-qa.s3.amazonaws.com/iirc.tar.gz\n",
            "Resolving jamesf-incomplete-qa.s3.amazonaws.com (jamesf-incomplete-qa.s3.amazonaws.com)... 52.218.242.243, 52.92.227.17, 52.92.229.249, ...\n",
            "Connecting to jamesf-incomplete-qa.s3.amazonaws.com (jamesf-incomplete-qa.s3.amazonaws.com)|52.218.242.243|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5713947 (5.4M) [application/x-gzip]\n",
            "Saving to: ‘iirc.tar.gz’\n",
            "\n",
            "iirc.tar.gz         100%[===================>]   5.45M  10.6MB/s    in 0.5s    \n",
            "\n",
            "2023-05-17 19:02:47 (10.6 MB/s) - ‘iirc.tar.gz’ saved [5713947/5713947]\n",
            "\n",
            "--2023-05-17 19:02:47--  http://jamesf-incomplete-qa.s3.amazonaws.com/context_articles.tar.gz\n",
            "Resolving jamesf-incomplete-qa.s3.amazonaws.com (jamesf-incomplete-qa.s3.amazonaws.com)... 52.218.242.243, 52.92.227.17, 52.92.229.249, ...\n",
            "Connecting to jamesf-incomplete-qa.s3.amazonaws.com (jamesf-incomplete-qa.s3.amazonaws.com)|52.218.242.243|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 385263479 (367M) [application/x-gzip]\n",
            "Saving to: ‘context_articles.tar.gz’\n",
            "\n",
            "context_articles.ta 100%[===================>] 367.42M  45.8MB/s    in 8.8s    \n",
            "\n",
            "2023-05-17 19:02:56 (41.9 MB/s) - ‘context_articles.tar.gz’ saved [385263479/385263479]\n",
            "\n",
            "--2023-05-17 19:03:09--  https://iirc-dataset.s3.us-west-2.amazonaws.com/iirc_test.json\n",
            "Resolving iirc-dataset.s3.us-west-2.amazonaws.com (iirc-dataset.s3.us-west-2.amazonaws.com)... 52.92.227.34, 52.218.232.105, 52.92.248.130, ...\n",
            "Connecting to iirc-dataset.s3.us-west-2.amazonaws.com (iirc-dataset.s3.us-west-2.amazonaws.com)|52.92.227.34|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2874825 (2.7M) [application/json]\n",
            "Saving to: ‘iirc_test.json.1’\n",
            "\n",
            "iirc_test.json.1    100%[===================>]   2.74M  6.40MB/s    in 0.4s    \n",
            "\n",
            "2023-05-17 19:03:10 (6.40 MB/s) - ‘iirc_test.json.1’ saved [2874825/2874825]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://jamesf-incomplete-qa.s3.amazonaws.com/iirc.tar.gz\n",
        "!tar -xzf iirc.tar.gz\n",
        "!rm iirc.tar.gz\n",
        "!cd iirc\n",
        "\n",
        "!wget http://jamesf-incomplete-qa.s3.amazonaws.com/context_articles.tar.gz\n",
        "!tar -xzf context_articles.tar.gz\n",
        "!rm context_articles.tar.gz\n",
        "\n",
        "!wget https://iirc-dataset.s3.us-west-2.amazonaws.com/iirc_test.json "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "dev_set = json.load(open('iirc/dev.json','r'))\n",
        "test_set = json.load(open('iirc_test.json', 'r'))\n",
        "context_articles = json.load(open(\"context_articles.json\",'r'))"
      ],
      "metadata": {
        "id": "fkE4heX3nb-L"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entendendo os dados de teste. Daqui dá pra pegar 50 questões e respostas pra gente testar depois que tiver o resultado."
      ],
      "metadata": {
        "id": "xHSEuL_8oTp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprime as 50 primeiras perguntas e a sua resposta esperada. E já salva em um array de objetos\n",
        "# Observação: Se mudar isso daqui (por exemplo, de test_set pra dev_set) é necessário\n",
        "# regerar o arquivo arquivo_seg_relevantes_para_queries (ou seja, setar gerar_segmentos_relevantes_para_queries = True)\n",
        "# e rodar o caderno com um GPU ligada. \n",
        "perg_resposta_esperada = []\n",
        "\n",
        "for i in range(len(test_set)):\n",
        "  pr = test_set[i]['questions'][0]\n",
        "  \n",
        "  pergunta = pr['question']\n",
        "  resposta_obj = pr['answer']\n",
        "  tipo_resposta = resposta_obj['type']\n",
        "\n",
        "  if tipo_resposta == 'binary' or tipo_resposta == 'value':\n",
        "    resposta = resposta_obj['answer_value']\n",
        "  elif tipo_resposta == 'span':\n",
        "    resposta = resposta_obj['answer_spans'][0]['text']\n",
        "  elif tipo_resposta == 'none':\n",
        "    resposta = 'none'\n",
        "  else:\n",
        "    resposta = 'Não pode chegar aqui, os tipos parece que são só binary/value/span/none!'\n",
        "    print(tipo_resposta)\n",
        "  \n",
        "  perg_resposta_esperada.append({\"pergunta\": pergunta, \"resposta\": resposta})\n",
        "\n",
        "  if i < 50:\n",
        "    print(f\"{i}: {pergunta}\\n{resposta}\")\n",
        "    print('----------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8dsyu6Vntvt",
        "outputId": "778b13a5-811e-45b0-dc36-9aa63d36bdb7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: What is Zeus know for in Greek mythology?\n",
            "sky and thunder god\n",
            "----------------------------------------------------------------------\n",
            "1: How long had the First World War been over when Messe was named aide-de-camp?\n",
            "5\n",
            "----------------------------------------------------------------------\n",
            "2: How long had Angela Scoular been acting professionally when she appeared in the movie \"On Her Majesty's Secret Service\"?\n",
            "2\n",
            "----------------------------------------------------------------------\n",
            "3: What is the capacity of the stadium where Brunt returned to action after a torn ACL?\n",
            "26,688\n",
            "----------------------------------------------------------------------\n",
            "4: In which country was Wilhelm Müller born?\n",
            "Germany \n",
            "----------------------------------------------------------------------\n",
            "5: In which Italian region did Pesce studied medicine?\n",
            "Liguria\n",
            "----------------------------------------------------------------------\n",
            "6: What albums were ranked higher than \"It Takes a Nation of Millions to Hold Us Back\" in Rolling Stone's the 500 Greatest Albums of All Time?\n",
            "none\n",
            "----------------------------------------------------------------------\n",
            "7: When was the sports organization that the Turks and Caicos Islands became affiliate members in 2002 founded?\n",
            "1909\n",
            "----------------------------------------------------------------------\n",
            "8: When was the port established at the Port Phillip District?\n",
            "none\n",
            "----------------------------------------------------------------------\n",
            "9: At which tournament were more goals scored, 1984 Canada Cup or 1994 Ice Hockey World Championship?\n",
            "none\n",
            "----------------------------------------------------------------------\n",
            "10: How much money did IBM earn the year Delicious was founded?\n",
            "none\n",
            "----------------------------------------------------------------------\n",
            "11: How many years had Mike Elizondo been a producer when Nightmare was released?\n",
            "none\n",
            "----------------------------------------------------------------------\n",
            "12: How long had Alex Rance been a professional player when he suffered an ACL injury during a season opening match?\n",
            "11\n",
            "----------------------------------------------------------------------\n",
            "13: In what country is the city located where the Arabic dialect is spoken by Jews?\n",
            "Iraq\n",
            "----------------------------------------------------------------------\n",
            "14: How many years were there between Pompeii's destruction and the discovery of the Maya site of Cerén?\n",
            "1897\n",
            "----------------------------------------------------------------------\n",
            "15: Which university did Don Branby played college football for?\n",
            "Colorado Buffaloes\n",
            "----------------------------------------------------------------------\n",
            "16: How long had the Perthshire Regiment been a going concern for when Harison joined it as a captain?\n",
            "69\n",
            "----------------------------------------------------------------------\n",
            "17: When was the university where Tookey served as Editor of Isis founded?\n",
            "none\n",
            "----------------------------------------------------------------------\n",
            "18: Which team won the first game in the 2004 National League Division Series?\n",
            "Cardinals \n",
            "----------------------------------------------------------------------\n",
            "19: Which is the oldest of the schools where Stirner's work was shown between 1985 and 2013?\n",
            "none\n",
            "----------------------------------------------------------------------\n",
            "20: How long had Mystic Records been in business when they recorded \"Be a Man Go to War\" and \"Night Stix\"?\n",
            "none\n",
            "----------------------------------------------------------------------\n",
            "21: How much money did the Dallas Burn spend in total when they selected Glenn in the 1996 MLS Player Draft?\n",
            "none\n",
            "----------------------------------------------------------------------\n",
            "22: How many championship appearances has the team where Ashton played junior hockey had?\n",
            "5\n",
            "----------------------------------------------------------------------\n",
            "23: How long is the most important river of  the Chota Nagpur Plateau?\n",
            "none\n",
            "----------------------------------------------------------------------\n",
            "24: Had the Chicago White Sox been around longer than the Birminham Barons when he was assigned to the White Sox's minor league system?\n",
            "yes\n",
            "----------------------------------------------------------------------\n",
            "25: How old was the person who shot Waitkus the year he was shot?\n",
            "20\n",
            "----------------------------------------------------------------------\n",
            "26: In what race was Sohn Kee-chung the gold medalist?\n",
            "marathon\n",
            "----------------------------------------------------------------------\n",
            "27: Was Eduard van Beinum still alive whnen Gilmore retired?\n",
            "no\n",
            "----------------------------------------------------------------------\n",
            "28: What percentage of the Philippine Revolutionary Army consisted of the Luna Sharpshooters?\n",
            "none\n",
            "----------------------------------------------------------------------\n",
            "29: How far away from Lewis' birthplace was the house he built in Alabama?\n",
            "none\n",
            "----------------------------------------------------------------------\n",
            "30: Which of the battles Sherman fought during a winter campaign had higher Union casualty rate?\n",
            " Donelson\n",
            "----------------------------------------------------------------------\n",
            "31: Which country did some of the tribes escaped to after their deportation from northern Mexico?\n",
            "United States\n",
            "----------------------------------------------------------------------\n",
            "32: In what county did Hirsch study under Edward C. Tolman and Robert Tryon?\n",
            "United States\n",
            "----------------------------------------------------------------------\n",
            "33: What states do the roads that go around flathead lake end in?\n",
            "Arizona\n",
            "----------------------------------------------------------------------\n",
            "34: What is the birthplace of the person who used Palmer's 1910 photograph of the illuminated Grand Pier Pavilion as found object art in his Note 78?\n",
            "Normandy, France\n",
            "----------------------------------------------------------------------\n",
            "35: How many students were enrolled at the Lomonosov Moscow State University the year Yuri Tschinkel graduated from that same school?\n",
            "none\n",
            "----------------------------------------------------------------------\n",
            "36: Of the schools where Bernard Avishai taught, which one has the highest current enrollment?\n",
            "Duke University\n",
            "----------------------------------------------------------------------\n",
            "37: Of the universities where Paranikas studied, which was founded first?\n",
            "University of Munich\n",
            "----------------------------------------------------------------------\n",
            "38: Is Josh Heupel older than Scott Frost?\n",
            "no\n",
            "----------------------------------------------------------------------\n",
            "39: How long had Gorbacheve been in power before the attempted coup?\n",
            "6\n",
            "----------------------------------------------------------------------\n",
            "40: How many months was the Northwest Indian War?\n",
            "120\n",
            "----------------------------------------------------------------------\n",
            "41: In what city was Romeo's father born?\n",
            "New Orleans\n",
            "----------------------------------------------------------------------\n",
            "42: How long had the Spruce Grove Mets been a team when they won the Centennial Cup?\n",
            "1\n",
            "----------------------------------------------------------------------\n",
            "43: Which team won the 1964 championship where Swimburne made his debut?\n",
            "none\n",
            "----------------------------------------------------------------------\n",
            "44: Which singer was younger when they recorded a cover of Madonna's song, Jody Watley or Nivek Ogre?\n",
            "Nivek Ogre\n",
            "----------------------------------------------------------------------\n",
            "45: What was the first named storm of the 2019 hurricane season?\n",
            "Andrea \n",
            "----------------------------------------------------------------------\n",
            "46: What was the construction cost of the stadium that the Cardinals moved into in Glendale, Arizona?\n",
            "$455 million\n",
            "----------------------------------------------------------------------\n",
            "47: How long had the French Foreign Legion been in existence when Wolff signed up?\n",
            "none\n",
            "----------------------------------------------------------------------\n",
            "48: What is the population of the island nation that suffered minor damage from Hurricane Betsy?\n",
            "332,634\n",
            "----------------------------------------------------------------------\n",
            "49: In what months did Hurricane Katrine and Hurricane Rita hit?\n",
            "August 2005\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Indexa pra rodar um BM25\n",
        "\n",
        "Remove o html da coleção:"
      ],
      "metadata": {
        "id": "mCsl5SXFx3di"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Remover o html igual está na base IIRC é bem mais rápido que usar o BeuatifulSoup\n",
        "# https://github.com/jferguson144/IIRC-baseline/blob/main/util.py\n",
        "def remove_html(html):\n",
        "  return re.sub(\"<[^>]*>\", \"\", html).strip()"
      ],
      "metadata": {
        "id": "vkYXJ7hdxXH0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "documents = []\n",
        "all_titles = []\n",
        "\n",
        "for item in test_set:\n",
        "    if item['title'].lower() not in all_titles:\n",
        "        documents.append({\n",
        "                \"title\": item['title'],\n",
        "                \"content\": item[\"text\"]\n",
        "            }\n",
        "        )\n",
        "        all_titles.append(item['title'].lower())\n",
        "    for link in item[\"links\"]:\n",
        "        if link['target'].lower() in context_articles and link['target'].lower() not in all_titles:\n",
        "            documents.append({\n",
        "                \"title\": link['target'],\n",
        "                \"content\": context_articles[link['target'].lower()]\n",
        "            })\n",
        "            all_titles.append(link['target'].lower())\n",
        "        # else:\n",
        "            # print(link['target'].lower())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OU-6iC9D0GVP",
        "outputId": "49dbba42-4f91-4fd5-df88-b476482bd759"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 727 ms, sys: 6.15 ms, total: 733 ms\n",
            "Wall time: 797 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NiMK9pg1DBV",
        "outputId": "c2c26ff9-b61a-4b50-acf7-4290206ee719"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'title': 'Palici',\n",
              " 'content': \"The Palici (Παλικοί in Greek), or Palaci, were a pair of indigenous Sicilian chthonic deities in Roman mythology, and to a lesser extent in Greek mythology. They are mentioned in Ovid's Metamorphoses V, 406, and in Virgil's Aeneid IX, 585. Their cult centered on three small lakes that emitted sulphurous vapors in the Palagonia plain, and as a result these twin brothers were associated with geysers and the underworld. There was also a shrine to the Palaci in Palacia, where people could subject themselves or others to tests of reliability through divine judgement; passing meant that an oath could be trusted. The mythological lineage of the Palici is uncertain; one legend made the Palici the sons of Zeus, or possibly Hephaestus, by Aetna or Thalia, but another claimed that the Palici were the sons of the Sicilian deity Adranus.\\n\"}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "artigos_sem_html = []\n",
        "\n",
        "# Se for usar todos os context_articles para indexar:\n",
        "#for titulo in context_articles.keys():\n",
        "#  artigo_html = context_articles[titulo]\n",
        "#  artigos_sem_html.append({\"title\": titulo, \"content\": remove_html(artigo_html)})\n",
        "\n",
        "# Se for usar só os documentos que estão na base de teste\n",
        "for doc in documents:\n",
        "  titulo = doc['title']\n",
        "  conteudo = doc['content']\n",
        "  artigos_sem_html.append({\"title\": titulo, \"content\": remove_html(conteudo)})"
      ],
      "metadata": {
        "id": "INw74ON8x9gz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quebra cada artigo em grupos de 3 sentenças. Usa a biblioteca Spacy para separar as sentenças.\n",
        "\n",
        "Código de origem: https://github.com/neuralmind-ai/visconde/blob/main/iirc_create_indices.ipynb"
      ],
      "metadata": {
        "id": "WuR7Zkk1NuAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from tqdm import tqdm\n",
        "import spacy\n",
        "import pickle\n",
        "\n",
        "nlp = spacy.blank(\"en\")\n",
        "nlp.add_pipe(\"sentencizer\")\n",
        "\n",
        "stride = 2\n",
        "max_length = 3\n",
        "\n",
        "def window(documents, stride=2, max_length=3):\n",
        "  treated_documents = []\n",
        "\n",
        "  for j, document in enumerate(tqdm(documents)):\n",
        "    doc_text = document['content']\n",
        "    doc = nlp(doc_text)\n",
        "    sentences = [sent.text.strip() for sent in doc.sents]\n",
        "    for i in range(0, len(sentences), stride):\n",
        "      segment = ' '.join(sentences[i:i + max_length])\n",
        "      treated_documents.append({\n",
        "          \"title\": document['title'],\n",
        "          \"contents\": document['title']+\". \"+segment,\n",
        "          \"segment\": segment\n",
        "      })\n",
        "      if i + max_length >= len(sentences):\n",
        "        break\n",
        "  return treated_documents\n",
        "\n",
        "\n",
        "if gerar_artigos_segmentados:\n",
        "  artigos_segmentados = window(artigos_sem_html)\n",
        "  with open(f'{dir_aula10}{arquivo_artigos_segmentados}', 'wb') as f:\n",
        "    pickle.dump(artigos_segmentados, f)\n",
        "else:\n",
        "  with open(f'{dir_aula10}{arquivo_artigos_segmentados}', 'rb') as f:\n",
        "    artigos_segmentados = pickle.load(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShBOvQZ9N5kh",
        "outputId": "39f42206-09a7-4f9d-9040-30f6e10c4c68"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 11.5 s, sys: 4.82 s, total: 16.3 s\n",
            "Wall time: 17 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(artigos_segmentados)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mP6A4gi2bHC",
        "outputId": "93046bea-07bd-4086-e07e-7ae5b605a2b3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "498082"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define classes para índice invertido e BM25:"
      ],
      "metadata": {
        "id": "oJeUpTM1Vp6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from pyserini.analysis import Analyzer, get_lucene_analyzer\n",
        "from collections import Counter\n",
        "import array\n",
        "import pickle\n",
        "import math\n",
        "\n",
        "# Definição de uma classe para índice invertido\n",
        "class IndiceInvertido:\n",
        "\n",
        "  # Recebe 'tokenizar', uma função tokenizadora\n",
        "  def __init__(self):\n",
        "    # Cria um índice invertido vazio\n",
        "    self.indice = {}\n",
        "    # Cria um índice de tamanho de documentos vazio\n",
        "    self.tamanho_doc = {}\n",
        "    # Guarda o total de documentos adicionados\n",
        "    self.n_docs = 0\n",
        "    # Tokenizador\n",
        "    self.lucene_analyser = Analyzer(get_lucene_analyzer(stemmer='porter'))\n",
        "\n",
        "  def tokenizar(self, texto):\n",
        "    return self.lucene_analyser.analyze(texto)\n",
        "\n",
        "  def adiciona_doc(self, id_doc, conteudo_doc=None):\n",
        "    tokens = self.tokenizar(conteudo_doc)\n",
        "\n",
        "    contador_tokens_do_documento = Counter(tokens)\n",
        "    for token, n_ocorrencias in contador_tokens_do_documento.items():\n",
        "      self.indice.setdefault(token, {\"id_doc\": [], \"n_ocorrencias\": array.array(\"L\", [])})['id_doc'].append(id_doc)\n",
        "      self.indice.setdefault(token, {\"id_doc\": [], \"n_ocorrencias\": array.array(\"L\", [])})['n_ocorrencias'].append(n_ocorrencias)\n",
        "    \n",
        "    self.n_docs += 1\n",
        "    self.tamanho_doc[id_doc] = len(tokens)\n",
        "\n",
        "class BM25:\n",
        "\n",
        "  def __init__(self, indiceInvertido=IndiceInvertido(), k1 = 0.9, b = 0.4, bias_adicionar_ao_idf = 0):\n",
        "    self.indiceInvertido = indiceInvertido\n",
        "    self.bias_adicionar_ao_idf = bias_adicionar_ao_idf\n",
        "    self.calcula_tam_medio_doc_no_indice()\n",
        "    self.k1 = k1\n",
        "    self.b = b\n",
        "    self.precalcula_idf()\n",
        "    self.reinicia_score_dos_indices()\n",
        "\n",
        "  def reinicia_score_dos_indices(self):\n",
        "    for token in self.indiceInvertido.indice.keys():\n",
        "      self.indiceInvertido.indice[token].pop('score', None)\n",
        "  \n",
        "  def calcula_tam_medio_doc_no_indice(self):\n",
        "    self.avgdl = sum(self.indiceInvertido.tamanho_doc.values()) / self.indiceInvertido.n_docs\n",
        "\n",
        "  def precalcula_idf(self):\n",
        "    # Número de documento do corpus está presente no objeto indiceInvertido\n",
        "    N = self.indiceInvertido.n_docs\n",
        "    # Varre todos os tokens do índice. Os tokens são as chaves do indiceInvertido.indice\n",
        "    for token in self.indiceInvertido.indice.keys():\n",
        "      # O número de documentos que possui o token é calculado pelo tamanho da lista de id_doc:\n",
        "      n_doc_token = len(self.indiceInvertido.indice[token]['id_doc'])\n",
        "      # Isso já é o suficiente pra calcular o idf\n",
        "      idf_token = math.log( ((self.indiceInvertido.n_docs - n_doc_token + 0.5)/(n_doc_token + 0.5)) + self.bias_adicionar_ao_idf )\n",
        "      # E agora, vamos colocar essa informação no índice\n",
        "      self.indiceInvertido.indice[token]['idf'] = idf_token\n",
        "\n",
        "  def calcula_score_para_um_token_e_salva(self, token):\n",
        "    # O cálculo do BM25 para determinada query é a multiplicação do idf pela frequência do termo no documento * (k1 + 1)\n",
        "    # Além disso, é dividido pela frequencia do termo no documento + k1 * (1 - b + b * tamanho_doc/avgdl)\n",
        "    idf = self.indiceInvertido.indice[token]['idf']\n",
        "    # Juntando tudo, podemos calcular o score pelo BM25\n",
        "    zip_id_freq = zip(self.indiceInvertido.indice[token]['id_doc'], self.indiceInvertido.indice[token]['n_ocorrencias'])   \n",
        "    bm25 = array.array(\"f\", [ idf * freq_token_no_doc * (self.k1 + 1) / (freq_token_no_doc + self.k1 * (1 - self.b + self.b * self.indiceInvertido.tamanho_doc[id_doc] / self.avgdl)) for (id_doc, freq_token_no_doc) in zip_id_freq ])\n",
        "    # Salva o bm25 no índice\n",
        "    self.indiceInvertido.indice[token]['score'] = bm25\n",
        "\n",
        "  def tokenizar(self, query):\n",
        "    return self.indiceInvertido.tokenizar(query)\n",
        "\n",
        "  def pesquisar(self, query):\n",
        "    # Tokeniza a query\n",
        "    tokens = self.tokenizar(query)\n",
        "\n",
        "    # Se não tem token para ser pesquisado, retorna conjunto vazio\n",
        "    if (len(tokens) == 0):\n",
        "      return []\n",
        "\n",
        "    # Guarda um dicionário onde a chave é o id do documento e o valor é o score desse documento para a query pesquisada\n",
        "    docs_retornado_com_score = Counter({})\n",
        "\n",
        "    # Faz a pesquisa de documentos. Para isso iteramos todos os tokens da query\n",
        "    for token in tokens:\n",
        "      # É possível que a query contenha algum termo que não foi indexado. Se isso ocorrer,\n",
        "      # entende-se que a frequência desse token em qualquer documento é 0, já que não pode ser encontrado\n",
        "      if token not in self.indiceInvertido.indice:\n",
        "        continue\n",
        "\n",
        "      # Pega a lista de documentos que será analisado\n",
        "      docs_que_tem_token = self.indiceInvertido.indice[token]['id_doc']\n",
        "      \n",
        "      # Se for a primeira vez que esse token é pesquisado, é necessário calcular o score relacionado\n",
        "      # a ele e salvar. Se já tiver sido feito antes, já podemos buscar o cálculo pronto (que funciona\n",
        "      # como um cache. Isso é útil no caso de várias pesquisas seguidas)\n",
        "      if 'score' not in self.indiceInvertido.indice[token].keys():\n",
        "        self.calcula_score_para_um_token_e_salva(token)\n",
        "      score_dos_docs_deste_token = self.indiceInvertido.indice[token]['score']\n",
        "\n",
        "      # Agora já temos calculado o score de todos os documentos desse token. Só adiciona ao acumulador de score atual\n",
        "      # docs_retornado_com_score += score_dos_docs_deste_token -> Se fosse usar dict direto no índice seria assim, mas a memória não está aguentando guardar os scores de ambos\n",
        "      for id_doc, score_par_doc_token in zip(docs_que_tem_token, score_dos_docs_deste_token):\n",
        "        docs_retornado_com_score[id_doc] += score_par_doc_token\n",
        "\n",
        "    # Agora converte esse dict em uma lista de tuplas com a chave (id_doc) e valor (score_do_doc)\n",
        "    docs_com_score = list(docs_retornado_com_score.items())\n",
        "\n",
        "    # E ordena do mais relevante para o menos relevante\n",
        "    return sorted(docs_com_score, key=lambda x: x[1], reverse=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW4fSB0OVfds",
        "outputId": "92adfa1a-ad74-40d5-8ab3-99fbb6f96af1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 966 ms, sys: 145 ms, total: 1.11 s\n",
            "Wall time: 1.47 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora efetivamente indexa os artigos segmentados:"
      ],
      "metadata": {
        "id": "QTNnSZ8UV3mn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def salvar_indice_em_arquivo_pickle(indice_invertido):\n",
        "  nome_arquivo_base = f\"{dir_aula10}{arquivo_indice_invertido}\"\n",
        "\n",
        "  with open(f'{nome_arquivo_base}_indice.pickle', 'wb') as f:\n",
        "    pickle.dump(indice_invertido.indice, f)\n",
        "  with open(f'{nome_arquivo_base}_tamanho_doc.pickle', 'wb') as f:\n",
        "    pickle.dump(indice_invertido.tamanho_doc, f)\n",
        "\n",
        "def recuperar_indice_em_arquivo_pickle():\n",
        "  nome_arquivo_base = f\"{dir_aula10}{arquivo_indice_invertido}\"\n",
        "  idx = IndiceInvertido()\n",
        "\n",
        "  with open(f'{nome_arquivo_base}_indice.pickle', 'rb') as f:\n",
        "    idx.indice = pickle.load(f)\n",
        "  with open(f'{nome_arquivo_base}_tamanho_doc.pickle', 'rb') as f:\n",
        "    idx.tamanho_doc = pickle.load(f)\n",
        "  idx.n_docs = len(idx.tamanho_doc)\n",
        "\n",
        "  return idx"
      ],
      "metadata": {
        "id": "bGAUgNi8XV4f"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "iidx = IndiceInvertido()\n",
        "\n",
        "if gerar_indice_invertido:\n",
        "  # Vamos usar o próprio índice como um id para o segmento. Vai facilitar a vida depois\n",
        "  # art_seg['contents'] tem o título e 3 frases do artigo\n",
        "  for id, art_seg in enumerate(artigos_segmentados):\n",
        "    iidx.adiciona_doc(id, art_seg['contents'])\n",
        "\n",
        "    if id % 50000 == 0:\n",
        "      print(f'{id} segmentos indexados')\n",
        "\n",
        "  salvar_indice_em_arquivo_pickle(iidx)\n",
        "else:\n",
        "  iidx = recuperar_indice_em_arquivo_pickle()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6RCz38nV7GM",
        "outputId": "15bce2d4-23b2-4220-d40e-59f628ba7db9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 5.07 s, sys: 2.87 s, total: 7.93 s\n",
            "Wall time: 8.27 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora sim cria o buscador BM25 e testa:"
      ],
      "metadata": {
        "id": "ASyrSc5-alWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "buscador = BM25(iidx, 0.82, 0.68, 1)"
      ],
      "metadata": {
        "id": "Yk8giIsTZXCE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Faz um teste pesquisando a primeira pergunta e exibindo os 10 primeiros resultados do BM25. É interessante ver que só usar o BM25 não resolve o problema, visto que nesse caso os 10 primeiros resultados nem tem o resultado esperado. O reranking é mesmo uma necessidade!"
      ],
      "metadata": {
        "id": "k2e_ZjKJe1C1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Pergunta e resposta esperada: ')\n",
        "print(perg_resposta_esperada[0])\n",
        "\n",
        "resultado_bm25 = buscador.pesquisar(perg_resposta_esperada[0]['pergunta'])\n",
        "\n",
        "print('-----------------------------------------------------------------')\n",
        "print('10 primeiros conteúdos retornados pelo BM25\\n')\n",
        "for i in range(10):\n",
        "  segmento_id = resultado_bm25[i][0]\n",
        "  score = resultado_bm25[i][1]\n",
        "\n",
        "  title = artigos_segmentados[segmento_id]['title']\n",
        "  contents = artigos_segmentados[segmento_id]['contents']\n",
        "  segment = artigos_segmentados[segmento_id]['segment']\n",
        "\n",
        "  print(f'Segmento_id: {segmento_id}. Score: {score}')\n",
        "  print(f\"Title: {title}\")\n",
        "  print(f\"Contents: {contents}\")\n",
        "  print('-----------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMQjGC-MctRW",
        "outputId": "33b3329b-d2b1-4e90-ecb9-715b702f8a47"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pergunta e resposta esperada: \n",
            "{'pergunta': 'What is Zeus know for in Greek mythology?', 'resposta': 'sky and thunder god'}\n",
            "-----------------------------------------------------------------\n",
            "10 primeiros conteúdos retornados pelo BM25\n",
            "\n",
            "Segmento_id: 1140. Score: 26.877893447875977\n",
            "Title: Zeus\n",
            "Contents: Zeus. - Rohde, Erwin, Psyche: The Cult of Souls and Belief in Immortality among the Greeks, 1925. - Smith, William, Dictionary of Greek and Roman Biography and Mythology, 1870, Ancientlibrary.com, William Smith, Dictionary: \"Zeus\" Ancientlibrary.com\n",
            "\n",
            "External links.- Greek Mythology Link, Zeus stories of Zeus in myth\n",
            "- Theoi Project, Zeus summary, stories, classical art\n",
            "- Theoi Project, Cult Of Zeus cult and statues\n",
            "- Photo: Pagans Honor Zeus at Ancient Athens Temple from National Geographic\n",
            "-----------------------------------------------------------------\n",
            "Segmento_id: 510. Score: 25.476608276367188\n",
            "Title: Greek mythology\n",
            "Contents: Greek mythology. In Ancient Roman times, a new Roman mythology was born through syncretization of numerous Greek and other foreign gods. This occurred because the Romans had little mythology of their own, and inheritance of the Greek mythological tradition caused the major Roman gods to adopt characteristics of their Greek equivalents. The gods Zeus and Jupiter are an example of this mythological overlap.\n",
            "-----------------------------------------------------------------\n",
            "Segmento_id: 434. Score: 24.9974684715271\n",
            "Title: Greek mythology\n",
            "Contents: Greek mythology. Greek pantheon. According to Classical-era mythology, after the overthrow of the Titans, the new pantheon of gods and goddesses was confirmed. Among the principal Greek gods were the Olympians, residing on Mount Olympus under the eye of Zeus. (\n",
            "-----------------------------------------------------------------\n",
            "Segmento_id: 511. Score: 23.775321006774902\n",
            "Title: Greek mythology\n",
            "Contents: Greek mythology. The gods Zeus and Jupiter are an example of this mythological overlap. In addition to the combination of the two mythological traditions, the association of the Romans with eastern religions led to further syncretizations. For instance, the cult of Sun was introduced in Rome after Aurelian's successful campaigns in Syria.\n",
            "-----------------------------------------------------------------\n",
            "Segmento_id: 136545. Score: 22.478649139404297\n",
            "Title: Balkan Mountains\n",
            "Contents: Balkan Mountains. The name of the place where the range meets the Black Sea, Cape Emine, is derived from Aemon. A folk etymology holds that 'Haemus' derives from the Greek word \"haima\" () meaning 'blood', and is based on Greek mythology. During a fight between Zeus and the monster/titan Typhon, Zeus injured Typhon with thunder; and Typhon's blood fell on the mountains, which were then named for this battle.\n",
            "-----------------------------------------------------------------\n",
            "Segmento_id: 1142. Score: 22.231764316558838\n",
            "Title: Hephaestus\n",
            "Contents: Hephaestus. In Greek mythology, Hephaestus was either the son of Zeus and Hera or he was Hera's parthenogenous child. He was cast off Mount Olympus by his mother because of his deformity or, in another account, by Zeus for protecting Hera from his advances. As a smithing god, Hephaestus made all the weapons of the gods in Olympus.\n",
            "-----------------------------------------------------------------\n",
            "Segmento_id: 1141. Score: 21.898592472076416\n",
            "Title: Hephaestus\n",
            "Contents: Hephaestus. Hephaestus (; eight spellings; Hēphaistos) is the Greek god of blacksmiths, metalworking, carpenters, craftsmen, artisans, sculptors, metallurgy, fire (compare, however, with Hestia), and volcanoes. Hephaestus' Roman equivalent is Vulcan. In Greek mythology, Hephaestus was either the son of Zeus and Hera or he was Hera's parthenogenous child.\n",
            "-----------------------------------------------------------------\n",
            "Segmento_id: 1161. Score: 21.86091375350952\n",
            "Title: Hephaestus\n",
            "Contents: Hephaestus. In one branch of Greek mythology, Hera ejected Hephaestus from the heavens because he was \"shrivelled of foot\". He fell into the ocean and was raised by Thetis (mother of Achilles and one of the 50 Nereids) and the Oceanid Eurynome. In another account, Hephaestus, attempting to rescue his mother from Zeus' advances, was flung down from the heavens by Zeus.\n",
            "-----------------------------------------------------------------\n",
            "Segmento_id: 423. Score: 21.700679302215576\n",
            "Title: Greek mythology\n",
            "Contents: Greek mythology. At last, with the help of the Cyclopes (whom Zeus freed from Tartarus), Zeus and his siblings were victorious, while Cronus and the Titans were hurled down to imprisonment in Tartarus. Zeus was plagued by the same concern, and after a prophecy that the offspring of his first wife, Metis, would give birth to a god \"greater than he\", Zeus swallowed her. She was already pregnant with Athena, however, and she burst forth from his head—fully-grown and dressed for war.\n",
            "-----------------------------------------------------------------\n",
            "Segmento_id: 529. Score: 21.645485401153564\n",
            "Title: Greek mythology\n",
            "Contents: Greek mythology. In 1891, he claimed that \"the most important discovery which has been made during the nineteenth century with respect to the ancient history of mankind ... was this sample equation: Sanskrit Dyaus-pitar = Greek Zeus = Latin Jupiter = Old Norse Tyr\". The question of Greek mythology's place in Indo-European studies has generated much scholarship since Müller's time. For example, philologist Georges Dumézil draws a comparison between the Greek Uranus and the Sanskrit Varuna, although there is no hint that he believes them to be originally connected.\n",
            "-----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reranking dos resultados\n",
        "\n",
        "Código adaptado do Visconde: https://github.com/neuralmind-ai/visconde/blob/main/qasper_rerank.ipynb"
      ],
      "metadata": {
        "id": "d9q1F7eefRtq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(modelo_classificador_binario)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(modelo_classificador_binario).to(device)"
      ],
      "metadata": {
        "id": "k67QQsKEfUO-"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como não consegui instalar o PyGaggle, vou me inspirar no código do PyGaggle pra tentar fazer o reranking com o monoT5:"
      ],
      "metadata": {
        "id": "FpbC4nMCrb3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils import data\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BatchEncoding\n",
        "\n",
        "collate_fn = lambda batch: BatchEncoding(tokenizer.pad(batch, return_tensors='pt'))\n",
        "\n",
        "class Dataset(data.Dataset):\n",
        "  def __init__(self, tokenizer, query, textos_para_rankear, max_seq_length = 256):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_seq_length = max_seq_length    \n",
        "    self.lista_query_textos = [f'{query} [SEP] {t}' for t in textos_para_rankear]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.lista_query_textos)\n",
        "    \n",
        "  def __getitem__(self, idx):\n",
        "    batch_encoding = self.tokenizer.encode_plus(self.lista_query_textos[idx],\n",
        "                                                add_special_tokens=True,\n",
        "                                                truncation=True,\n",
        "                                                max_length=self.max_seq_length,\n",
        "                                                padding='max_length',\n",
        "                                                return_attention_mask=True,\n",
        "                                                return_tensors='pt')\n",
        "    return {\n",
        "      'input_ids': batch_encoding['input_ids'].squeeze(0),\n",
        "      'attention_mask': batch_encoding['attention_mask'],\n",
        "    }\n",
        "\n",
        "@torch.no_grad()\n",
        "def pesquisa_e_rerank(query):\n",
        "  # Pesquisa\n",
        "  docs_score = buscador.pesquisar(query)\n",
        "\n",
        "  # Pega apenas os 1000 primeiros elementos e extrai o conteúdo deles\n",
        "  docs_score = docs_score[:1000]\n",
        "  doc_ids, _ = zip(*docs_score)\n",
        "  doc_contents = []\n",
        "  for idx, id in enumerate(doc_ids):\n",
        "    doc_contents.append(artigos_segmentados[id]['contents'])\n",
        "\n",
        "  # Cria um dataset pra mandar pro reranking\n",
        "  dataset_reranking = Dataset(tokenizer, query, doc_contents)\n",
        "  # Agora cria um dataloader\n",
        "  dataloader_reranking = DataLoader(dataset_reranking, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "  # Agora sim, faz o reranking\n",
        "  scores = []\n",
        "  for batch in tqdm(dataloader_reranking, desc=f'Reranking para query: {query}', disable=False):\n",
        "    outputs = model(**batch.to(device))\n",
        "    probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
        "    _, classe1 = zip(*probs)\n",
        "    scores.extend(list(classe1))\n",
        "  \n",
        "  scores_float = [t.item() for t in scores]\n",
        "  docs_scores = sorted(zip(doc_ids, scores_float), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "  return docs_scores"
      ],
      "metadata": {
        "id": "yDNoi5XLftKe"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos pesquisar as 50 primeiras queries e guardar os resultados:"
      ],
      "metadata": {
        "id": "cJKS91yhFc6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "segmentos_relevantes_por_query = {}\n",
        "\n",
        "if gerar_segmentos_relevantes_para_queries:\n",
        "  for i in range(len(perg_resposta_esperada)):\n",
        "    print(f'Query: {i}')\n",
        "    docs_scores = pesquisa_e_rerank(perg_resposta_esperada[i]['pergunta'])\n",
        "    segmentos_relevantes_por_query[i] = docs_scores\n",
        "\n",
        "  with open(f'{dir_aula10}{arquivo_seg_relevantes_para_queries}', 'wb') as f:\n",
        "    pickle.dump(segmentos_relevantes_por_query, f)\n",
        "else:\n",
        "  with open(f'{dir_aula10}{arquivo_seg_relevantes_para_queries}', 'rb') as f:\n",
        "    segmentos_relevantes_por_query = pickle.load(f)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCNBWMbNuzg2",
        "outputId": "59a9540a-e0b7-4eca-d700-7487325d73c7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 95.1 ms, sys: 64.1 ms, total: 159 ms\n",
            "Wall time: 166 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifica o resultado para alguma query específica\n",
        "# Pode ser de 0 a 513\n",
        "\n",
        "idx_query = 502\n",
        "print(perg_resposta_esperada[idx_query]['pergunta'])\n",
        "print(f'Resposta esperada: {perg_resposta_esperada[idx_query][\"resposta\"]}')\n",
        "print('------------------------------------')\n",
        "print('TOP 5 documentos retornados:')\n",
        "print('------------------------------------')\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "  docs_scores = segmentos_relevantes_por_query[idx_query]\n",
        "  print(f'{i} {artigos_segmentados[docs_scores[i][0]]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlKDP3RyApe2",
        "outputId": "e3d42663-ed37-418c-ccfb-28744ceeb93b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Does the writer of the song \"Who Did That to You\" have more than two children?\n",
            "Resposta esperada: no\n",
            "------------------------------------\n",
            "TOP 5 documentos retornados:\n",
            "------------------------------------\n",
            "0 {'title': 'Dream Theater', 'contents': 'Dream Theater. The whole band substantially reworked the song, and it appeared on the album as \"You Not Me\" with a chorus that bore little resemblance to the original. Child also had a noticeable impact on the album, with a shift towards less complex and more radio-friendly compositions. The band wrote almost two CDs worth of material, including a 20-minute-long follow-up to the Images and Words song \"Metropolis–Part I: The Miracle and the Sleeper\".', 'segment': 'The whole band substantially reworked the song, and it appeared on the album as \"You Not Me\" with a chorus that bore little resemblance to the original. Child also had a noticeable impact on the album, with a shift towards less complex and more radio-friendly compositions. The band wrote almost two CDs worth of material, including a 20-minute-long follow-up to the Images and Words song \"Metropolis–Part I: The Miracle and the Sleeper\".'}\n",
            "1 {'title': 'I Got a Name (song)', 'contents': 'I Got a Name (song). Background. Croce composed most of his own material, however he did not write \"I Got A Name\". In an interview with Billboard magazine, writer Norman Gimbel revealed the reason Croce chose to record the song, stating that \"Jim liked it because his father had a dream for him but had died before his son\\'s first success.\"', 'segment': 'Background. Croce composed most of his own material, however he did not write \"I Got A Name\". In an interview with Billboard magazine, writer Norman Gimbel revealed the reason Croce chose to record the song, stating that \"Jim liked it because his father had a dream for him but had died before his son\\'s first success.\"'}\n",
            "2 {'title': 'Stephen Schwartz (composer)', 'contents': \"Stephen Schwartz (composer). They have two children, Jessica and Scott. In 2009 Schwartz was elected president of the Dramatists Guild of America, succeeding John Weidman; he stepped down in 2014, to be succeeded by Doug Wright. Stage.- Butterflies Are Free (1969) - title song (play and movie)\\n- Godspell (1971) - composer, lyricist\\n- Mass (1971) - English texts (in collaboration with Leonard Bernstein)\\n- Pippin (1972) - composer, lyricist\\n- The Magic Show (1974) - composer, lyricist\\n- The Baker's Wife (1976) - composer, lyricist\\n- Working (1978) - adaptation, direction, composer, lyricist of 4 songs\\n- Personals (1985) - composer of 3 songs\\n- The Trip (1986) - children's show – composer, lyricist\\n- Rags - (1986) lyricist\\n- Children of Eden (1991) - composer, lyricist\\n- Der Glöckner von Notre Dame (1999 Berlin) - lyricist to Alan Menken (stage version of Disney's The Hunchback of Notre Dame); Michael Kunze translated the lyrics to German; English version in 2013\\n- Wicked (2003) - composer, lyricist\\n- Tiruvasakam – 2005 English translation of selected verses of the Tamil hymn on Lord Siva by Manickavasagar; Indian composer Ilaiyaraaja wrote the music.\", 'segment': \"They have two children, Jessica and Scott. In 2009 Schwartz was elected president of the Dramatists Guild of America, succeeding John Weidman; he stepped down in 2014, to be succeeded by Doug Wright. Stage.- Butterflies Are Free (1969) - title song (play and movie)\\n- Godspell (1971) - composer, lyricist\\n- Mass (1971) - English texts (in collaboration with Leonard Bernstein)\\n- Pippin (1972) - composer, lyricist\\n- The Magic Show (1974) - composer, lyricist\\n- The Baker's Wife (1976) - composer, lyricist\\n- Working (1978) - adaptation, direction, composer, lyricist of 4 songs\\n- Personals (1985) - composer of 3 songs\\n- The Trip (1986) - children's show – composer, lyricist\\n- Rags - (1986) lyricist\\n- Children of Eden (1991) - composer, lyricist\\n- Der Glöckner von Notre Dame (1999 Berlin) - lyricist to Alan Menken (stage version of Disney's The Hunchback of Notre Dame); Michael Kunze translated the lyrics to German; English version in 2013\\n- Wicked (2003) - composer, lyricist\\n- Tiruvasakam – 2005 English translation of selected verses of the Tamil hymn on Lord Siva by Manickavasagar; Indian composer Ilaiyaraaja wrote the music.\"}\n",
            "3 {'title': \"You're the One That I Want\", 'contents': 'You\\'re the One That I Want. \"You\\'re the One That I Want\" is one of the best-selling singles in history, having sold over 6 million copies in the United States, the United Kingdom, and France alone, with estimates of more than 15 million copies sold overall. Background. \"You\\'re the One That I Want\" was one of the two singles, along with \"Hopelessly Devoted to You\", that Farrar wrote specifically for Newton-John\\'s appearance in the film that had not been in the original stage musical.', 'segment': '\"You\\'re the One That I Want\" is one of the best-selling singles in history, having sold over 6 million copies in the United States, the United Kingdom, and France alone, with estimates of more than 15 million copies sold overall. Background. \"You\\'re the One That I Want\" was one of the two singles, along with \"Hopelessly Devoted to You\", that Farrar wrote specifically for Newton-John\\'s appearance in the film that had not been in the original stage musical.'}\n",
            "4 {'title': 'Philip Lawrence (songwriter)', 'contents': 'Philip Lawrence (songwriter). He credits his wife as the inspiration for many of the songs he has written. They have four children and currently reside in Los Angeles. Influences.', 'segment': 'He credits his wife as the inspiration for many of the songs he has written. They have four children and currently reside in Los Angeles. Influences.'}\n",
            "5 {'title': 'I Got a Name (song)', 'contents': 'I Got a Name (song). In an interview with Billboard magazine, writer Norman Gimbel revealed the reason Croce chose to record the song, stating that \"Jim liked it because his father had a dream for him but had died before his son\\'s first success.\" Content. The song features a narrator who is proud of who he is and where he is going in life, undeterred by the naysaying of others.', 'segment': 'In an interview with Billboard magazine, writer Norman Gimbel revealed the reason Croce chose to record the song, stating that \"Jim liked it because his father had a dream for him but had died before his son\\'s first success.\" Content. The song features a narrator who is proud of who he is and where he is going in life, undeterred by the naysaying of others.'}\n",
            "6 {'title': \"I'll Make a Man Out of You\", 'contents': 'I\\'ll Make a Man Out of You. \"I\\'ll Make a Man Out of You\" was written by composer Matthew Wilder and lyricist David Zippel, who were hired to write Mulan\\'s songs because Disney was interested in recruiting songwriters \"that ... would give kind of different sound to each of the songs.\" While Zippel, a Broadway lyricist, was recruited because the directors were impressed with his work on Disney\\'s Hercules (1997), at the time Wilder, a pop singer, had been best known for his song \"Break My Stride\". Bancroft believes that, although the songwriters \"had two different sensibilities ... the blend [of their styles] worked well together, especially on [\\'I\\'ll Make a Man Out of You\\']\".', 'segment': '\"I\\'ll Make a Man Out of You\" was written by composer Matthew Wilder and lyricist David Zippel, who were hired to write Mulan\\'s songs because Disney was interested in recruiting songwriters \"that ... would give kind of different sound to each of the songs.\" While Zippel, a Broadway lyricist, was recruited because the directors were impressed with his work on Disney\\'s Hercules (1997), at the time Wilder, a pop singer, had been best known for his song \"Break My Stride\". Bancroft believes that, although the songwriters \"had two different sensibilities ... the blend [of their styles] worked well together, especially on [\\'I\\'ll Make a Man Out of You\\']\".'}\n",
            "7 {'title': 'Philip Lawrence (songwriter)', 'contents': 'Philip Lawrence (songwriter). He also wanted to be present to spend more time with his family and young children. He is married to stylist, dancer, and fashion designer Urbana Chapa Lawrence. He credits his wife as the inspiration for many of the songs he has written.', 'segment': 'He also wanted to be present to spend more time with his family and young children. He is married to stylist, dancer, and fashion designer Urbana Chapa Lawrence. He credits his wife as the inspiration for many of the songs he has written.'}\n",
            "8 {'title': \"Livin' on a Prayer\", 'contents': 'Livin\\' on a Prayer. It\\'s working class and it\\'s real… I wanted to incorporate the movie element, and tell a story about people I knew. So instead of doing what I did on \\'Runaway\\', where the girl didn\\'t have a name, I gave them names, which gave them an identity… Tommy and Gina aren\\'t two specific people; they represent a lifestyle.\" Bon Jovi explained that he \"wrote that song during the Reagan era and the trickle-down economics are really inspirational to writing songs\".', 'segment': 'It\\'s working class and it\\'s real… I wanted to incorporate the movie element, and tell a story about people I knew. So instead of doing what I did on \\'Runaway\\', where the girl didn\\'t have a name, I gave them names, which gave them an identity… Tommy and Gina aren\\'t two specific people; they represent a lifestyle.\" Bon Jovi explained that he \"wrote that song during the Reagan era and the trickle-down economics are really inspirational to writing songs\".'}\n",
            "9 {'title': 'Lullaby', 'contents': 'Lullaby. It was made politically correct in the 1990s: The word negerdreng (Negro boy) was changed to kokosnød (coconut). The song was written in 1948 by the Danish writer and poet Harald H. Lund with music composed by writer-musician Mogens Jermiin Nissen (1906–72). \"Godnatsang\" (\"Goodnight Song\") – This is a popular lullaby that was composed (lyrics and music) by Sigurd Barrett (born 1967), pianist, composer and host of a children\\'s TV programme in Denmark, and fellow musician Steen Nikolaj Hansen.', 'segment': 'It was made politically correct in the 1990s: The word negerdreng (Negro boy) was changed to kokosnød (coconut). The song was written in 1948 by the Danish writer and poet Harald H. Lund with music composed by writer-musician Mogens Jermiin Nissen (1906–72). \"Godnatsang\" (\"Goodnight Song\") – This is a popular lullaby that was composed (lyrics and music) by Sigurd Barrett (born 1967), pianist, composer and host of a children\\'s TV programme in Denmark, and fellow musician Steen Nikolaj Hansen.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Etapa de agregação com GPT-3.5-turbo\n",
        "\n"
      ],
      "metadata": {
        "id": "mlc3xZxhW_uW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos selecionar algumas perguntas/respostas pra gente passar no prompt do GPT-3.5:"
      ],
      "metadata": {
        "id": "OFcEAPF5aT5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx_query = 305\n",
        "\n",
        "pergunta = perg_resposta_esperada[idx_query]['pergunta']\n",
        "resposta_esperada = perg_resposta_esperada[idx_query][\"resposta\"]\n",
        "print(pergunta)\n",
        "print(f'Resposta esperada: {resposta_esperada}')\n",
        "\n",
        "for i in range(5):\n",
        "  docs_scores = segmentos_relevantes_por_query[idx_query]\n",
        "  print(artigos_segmentados[docs_scores[i][0]]['segment'])\n",
        "print('-------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVHtcW37at1g",
        "outputId": "ddf9ed69-b77c-445e-e471-00f34fd594d4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When was Article Two of the US Constitution drafted?\n",
            "Resposta esperada: September 17\n",
            "The electors cast their votes on the Monday following the second Wednesday in December (the first Monday after December 12) of that year. Thereafter, the votes are opened and counted by the vice president, as president of the Senate, in a joint session of Congress. Clause 5: Qualifications for office.\n",
            "When President Bill Clinton attempted to shield the records of the President's Task Force on Health Care Reform as essential to his functions under the Recommendations Clause, a federal circuit court rejected the argument and noted in Ass'n of American Physicians & Surgeons v. Clinton (1993): \"[T]he Recommendation Clause is less an obligation than a right. The president has the undisputed authority to recommend legislation, but he need not exercise that authority with respect to any particular subject or, for that matter, any subject.\" Clause 3: Convening and adjourning of Congress.\n",
            "A quorum in the House consisted of at least one member from two-thirds of the state delegations; there was no special quorum for the Senate. This procedure was followed in 1801 after the electoral vote produced a tie, and nearly resulted in a deadlock in the House. While the Constitution reflects the Framers' clear preference for the president to be elected by a constituency independent of the Congress, one of the most palpable limitations created by the stipulation that electors meet in their respective states as opposed to a single venue was that given the constraints of eighteenth-century technology there was no practical means for that constituency to resolve deadlocked elections in a timely manner, thus necessitating the involvement of Congress in resolving deadlocked elections.\n",
            "The first abrogation of a treaty occurred in 1798, when Congress passed a law terminating a 1778 Treaty of Alliance with France. In the nineteenth century, several presidents terminated treaties after Congress passed resolutions requesting the same. In 1854, however, President Franklin Pierce terminated a treaty with Denmark with the consent of the Senate alone.\n",
            "Congress sets a national Election Day. Currently, electors are chosen on the Tuesday following the first Monday in November (the first Tuesday after November 1), in the year before the president's term is to expire. The electors cast their votes on the Monday following the second Wednesday in December (the first Monday after December 12) of that year.\n",
            "-------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "artigos_segmentados[docs_scores[1][0]]['segment']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "UQEzx-wkbjdu",
        "outputId": "9c1ea84f-ca3f-4c08-ff70-0f1fd2b1be25"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'When President Bill Clinton attempted to shield the records of the President\\'s Task Force on Health Care Reform as essential to his functions under the Recommendations Clause, a federal circuit court rejected the argument and noted in Ass\\'n of American Physicians & Surgeons v. Clinton (1993): \"[T]he Recommendation Clause is less an obligation than a right. The president has the undisputed authority to recommend legislation, but he need not exercise that authority with respect to any particular subject or, for that matter, any subject.\" Clause 3: Convening and adjourning of Congress.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx_query = 99\n",
        "print(perg_resposta_esperada[idx_query]['pergunta'])\n",
        "print(f'Resposta esperada: {perg_resposta_esperada[idx_query][\"resposta\"]}')\n",
        "print('------------------------------------')\n",
        "print('TOP 5 documentos retornados:')\n",
        "print('------------------------------------')\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "  docs_scores = segmentos_relevantes_por_query[idx_query]\n",
        "  print(f'{i} {artigos_segmentados[docs_scores[i][0]]}')"
      ],
      "metadata": {
        "id": "Nuam6XaqZwe_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e6180e8-7a8f-40e7-cfed-c44e143aa48c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On what date did the Loma Prieta earthquake occur?\n",
            "Resposta esperada: 1989-10-18\n",
            "------------------------------------\n",
            "TOP 5 documentos retornados:\n",
            "------------------------------------\n",
            "0 {'title': '1989 Loma Prieta earthquake', 'contents': '1989 Loma Prieta earthquake. The 1989 Loma Prieta earthquake occurred on California’s Central Coast on October 17 at local time (1989-10-18 00:04 UTC). The shock was centered in The Forest of Nisene Marks State Park approximately 10 mi northeast of Santa Cruz on a section of the San Andreas Fault System and was named for the nearby Loma Prieta Peak in the Santa Cruz Mountains. With an magnitude of 6.9 and a maximum Modified Mercalli intensity of IX (Violent), the shock was responsible for 63 deaths and 3,757 injuries.', 'segment': 'The 1989 Loma Prieta earthquake occurred on California’s Central Coast on October 17 at local time (1989-10-18 00:04 UTC). The shock was centered in The Forest of Nisene Marks State Park approximately 10 mi northeast of Santa Cruz on a section of the San Andreas Fault System and was named for the nearby Loma Prieta Peak in the Santa Cruz Mountains. With an magnitude of 6.9 and a maximum Modified Mercalli intensity of IX (Violent), the shock was responsible for 63 deaths and 3,757 injuries.'}\n",
            "1 {'title': '1989 Loma Prieta earthquake', 'contents': '1989 Loma Prieta earthquake. Foreshocks. The 5.3 June 1988 and the 5.4 August 1989 events also occurred on previously unknown oblique reverse faults and were within 3 mi of the M6.9 Loma Prieta mainshock epicenter, near the intersection of the San Andreas and Sargent faults. Total displacement for these shocks was relatively small (approximately 4 in of strike-slip and substantially less reverse-slip) and although they occurred on separate faults and well before the mainshock, a group of seismologists considered these to be foreshocks due to their location in space and time relative to the main event.', 'segment': 'Foreshocks. The 5.3 June 1988 and the 5.4 August 1989 events also occurred on previously unknown oblique reverse faults and were within 3 mi of the M6.9 Loma Prieta mainshock epicenter, near the intersection of the San Andreas and Sargent faults. Total displacement for these shocks was relatively small (approximately 4 in of strike-slip and substantially less reverse-slip) and although they occurred on separate faults and well before the mainshock, a group of seismologists considered these to be foreshocks due to their location in space and time relative to the main event.'}\n",
            "2 {'title': '1989 Loma Prieta earthquake', 'contents': '1989 Loma Prieta earthquake. The June 27, 1988, shock occurred with a maximum intensity of VI (Strong). Its effects included broken windows in Los Gatos, and other light damage in Holy City, where increased flow was observed at a water well. Farther away from the Santa Cruz Mountains, pieces of concrete fell from a parking structure at the Sunnyvale Town Center, a two-level shopping mall in Santa Clara County.', 'segment': 'The June 27, 1988, shock occurred with a maximum intensity of VI (Strong). Its effects included broken windows in Los Gatos, and other light damage in Holy City, where increased flow was observed at a water well. Farther away from the Santa Cruz Mountains, pieces of concrete fell from a parking structure at the Sunnyvale Town Center, a two-level shopping mall in Santa Clara County.'}\n",
            "3 {'title': 'Oakland, California', 'contents': 'Oakland, California. The 6.9 Loma Prieta earthquake occurred on October 17, 1989. The rupture was related to the San Andreas fault system and affected the entire San Francisco Bay Area with a maximum Mercalli intensity of IX (Violent). Many structures in Oakland were badly damaged including the double-decker portion of Interstate 880 that collapsed.', 'segment': 'The 6.9 Loma Prieta earthquake occurred on October 17, 1989. The rupture was related to the San Andreas fault system and affected the entire San Francisco Bay Area with a maximum Mercalli intensity of IX (Violent). Many structures in Oakland were badly damaged including the double-decker portion of Interstate 880 that collapsed.'}\n",
            "4 {'title': '1989 Loma Prieta earthquake', 'contents': '1989 Loma Prieta earthquake. Occurs in the episode \"August\". In a later episode (\"Amber 31422\", November 4, 2010), the alternate Walter Bishop refers to October 17, 1989, as the first time he used his amber-based protocol to heal breaches between the two universes. - The Grateful Dead performed the Rodney Crowell song \"California Earthquake\" at their concert in Philadelphia, Pennsylvania, on October 20, 1989, then again three nights later in Charlotte, North Carolina – the only times the band ever performed the song.', 'segment': 'Occurs in the episode \"August\". In a later episode (\"Amber 31422\", November 4, 2010), the alternate Walter Bishop refers to October 17, 1989, as the first time he used his amber-based protocol to heal breaches between the two universes. - The Grateful Dead performed the Rodney Crowell song \"California Earthquake\" at their concert in Philadelphia, Pennsylvania, on October 20, 1989, then again three nights later in Charlotte, North Carolina – the only times the band ever performed the song.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "# Pra rodar no Colab tem que colocar a chave aqui.\n",
        "openai.api_key = os.getenv('API_KEY_OPENAI')\n",
        "\n",
        "def mensagem_user(numExemplo, pergunta, lista_documentos):\n",
        "  doc_com_id = [ f\"[Document {i+1}]: {lista_documentos[i]}\" for i in range(len(lista_documentos))]\n",
        "  return f\"Example {numExemplo}\\n\\n\" + \"\\n\\n\".join(doc_com_id) + f\"\\n\\nQuestion: {pergunta}\"\n",
        "\n",
        "def mensagem_assistant(evidencia, resposta):\n",
        "  return f\"Evidence: {evidencia}\\n\\nAnswer: {resposta}\"\n",
        "\n",
        "few_shot_examples = [\n",
        "    {\"user\": \"\", \"assistant\": \"\"},\n",
        "    {\"user\": \"\", \"assistant\": \"\"},\n",
        "]\n",
        "few_shot_examples[0][\"user\"] = mensagem_user(1, \"Did Pink Floyd have a song about the French Riviera?\", [\n",
        "    \"\\\"San Tropez\\\" is the fourth track from the album Meddle by the band Pink Floyd. This song was one of several to be considered for the band's \\\"best of\\\" album, Echoes: The Best of Pink Floyd.\",\n",
        "    \"The French Riviera (known in French as the Côte d'Azur [kot daˈzyʁ]; Occitan: Còsta d'Azur [ˈkɔstɔ daˈzyɾ]; literal translation \\\"Azure Coast\\\") is the Mediterranean coastline of the southeast corner of France. There is no official boundary, but it is usually considered to extend from Cassis, Toulon or Saint-Tropez on the west to Menton at the France–Italy border in the east, where the Italian Riviera joins. The coast is entirely within the Provence-Alpes-Côte d'Azur (Région Sud) region of France. The Principality of Monaco is a semi-enclave within the region, surrounded on three sides by France and fronting the Mediterranean.\",\n",
        "    \"Moon also promised transparency in his presidency, moving the presidential residence from the palatial and isolated Blue House to an existing government complex in downtown Seoul.\",\n",
        "    \"Saint-Tropez (US: /ˌsæn troʊˈpeɪ/ SAN-troh-PAY, French: [sɛ̃ tʁɔpe]; Occitan: Sant-Tropetz , pronounced [san(t) tʀuˈpes]) is a town on the French Riviera, 68 kilometres (42 miles) west of Nice and 100 kilometres (62 miles) east of Marseille in the Var department of the Provence-Alpes-Côte d'Azur region of Occitania, Southern France.\"\n",
        "])\n",
        "few_shot_examples[0][\"assistant\"] = mensagem_assistant(\"\\\"San Tropez\\\" is a song by Pink Floyd about the French Riviera [Document 1]. Saint-Tropez is a town on the French Riviera [Document 4]\", \"yes\")\n",
        "few_shot_examples[1][\"user\"] = mensagem_user(2, \"On what date did the Loma Prieta earthquake occur?\", [\n",
        "    \"The 1989 Loma Prieta earthquake occurred on California’s Central Coast on October 17 at local time (1989-10-18 00:04 UTC). The shock was centered in The Forest of Nisene Marks State Park approximately 10 mi northeast of Santa Cruz on a section of the San Andreas Fault System and was named for the nearby Loma Prieta Peak in the Santa Cruz Mountains. With an magnitude of 6.9 and a maximum Modified Mercalli intensity of IX (Violent), the shock was responsible for 63 deaths and 3,757 injuries.\",\n",
        "    \"'Foreshocks. The 5.3 June 1988 and the 5.4 August 1989 events also occurred on previously unknown oblique reverse faults and were within 3 mi of the M6.9 Loma Prieta mainshock epicenter, near the intersection of the San Andreas and Sargent faults. Total displacement for these shocks was relatively small (approximately 4 in of strike-slip and substantially less reverse-slip) and although they occurred on separate faults and well before the mainshock, a group of seismologists considered these to be foreshocks due to their location in space and time relative to the main event.'\",\n",
        "    \"The June 27, 1988, shock occurred with a maximum intensity of VI (Strong). Its effects included broken windows in Los Gatos, and other light damage in Holy City, where increased flow was observed at a water well. Farther away from the Santa Cruz Mountains, pieces of concrete fell from a parking structure at the Sunnyvale Town Center, a two-level shopping mall in Santa Clara County.\",\n",
        "    \"The 6.9 Loma Prieta earthquake occurred on October 17, 1989. The rupture was related to the San Andreas fault system and affected the entire San Francisco Bay Area with a maximum Mercalli intensity of IX (Violent). Many structures in Oakland were badly damaged including the double-decker portion of Interstate 880 that collapsed.\",\n",
        "])\n",
        "few_shot_examples[1][\"assistant\"] = mensagem_assistant(\"According to [Document 1] and [Document 4], the Loma Prieta earthquake occur on October 17, 1989\", \"1989-10-18\")\n",
        "\n",
        "def agrega(query = \"\", docs_para_agregar = []):\n",
        "  response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",\n",
        "              messages=[\n",
        "                  {\"role\": \"system\", \"content\": \"You are a helpful reader that reads a list of documents and use them as evidence to answer a question. You provide detailed explanations and concise answers.\"},\n",
        "                  {\"role\": \"user\", \"content\": \"For each example, write an explanation to the answer given the documents and the question. The final answer should be short (less than 10 words).\"},\n",
        "                  {\"role\": \"user\", \"content\": few_shot_examples[0][\"user\"]},\n",
        "                  {\"role\": \"assistant\", \"content\": few_shot_examples[0][\"assistant\"]},\n",
        "                  {\"role\": \"user\", \"content\": few_shot_examples[1][\"user\"]},\n",
        "                  {\"role\": \"assistant\", \"content\": few_shot_examples[1][\"assistant\"]},\n",
        "                  {\"role\": \"user\", \"content\": mensagem_user(2, query, docs_para_agregar)}\n",
        "              ],\n",
        "              temperature=0,\n",
        "              max_tokens=1000)\n",
        "  return response['choices'][0]['message']['content']"
      ],
      "metadata": {
        "id": "R-_dgDPPYBrH"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_docs = 5\n",
        "\n",
        "respostas_llm = {}\n",
        "\n",
        "if agregar_usando_gpt35turbo:\n",
        "  for idx_query in range(100):\n",
        "    query = perg_resposta_esperada[idx_query]['pergunta']\n",
        "    resposta_esperada = perg_resposta_esperada[idx_query]['resposta']\n",
        "    docs_scores = segmentos_relevantes_por_query[idx_query]\n",
        "    # índice 0 é o id do segmento, índice 1 é o score\n",
        "    textos = [ artigos_segmentados[docs_scores[idx_doc][0]]['segment'] for idx_doc in range(total_docs)]\n",
        "\n",
        "    respostas_llm[idx_query] = agrega(query, textos)\n",
        "\n",
        "    print(f'Query {idx_query}: {query}')\n",
        "    print(f'Resposta esperada: {resposta_esperada}')\n",
        "    print(f'Resposta LLM: {respostas_llm[idx_query]}')\n",
        "\n",
        "  with open(f'{dir_aula10}{arquivo_respostas_geradas}', 'wb') as f:\n",
        "    pickle.dump(respostas_llm, f)\n",
        "else:\n",
        "  with open(f'{dir_aula10}{arquivo_respostas_geradas}', 'rb') as f:\n",
        "    respostas_llm = pickle.load(f)"
      ],
      "metadata": {
        "id": "7XuclJZ5XDAX"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cálculo do F1"
      ],
      "metadata": {
        "id": "3Qw5IBPtmcyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Funções normize_answer e token_f1_score foram retiradas do Visconde (arquivo qasper_evaluator.py)\n",
        "import re\n",
        "import string\n",
        "\n",
        "def normalize_answer(s):\n",
        "  \"\"\"\n",
        "  Taken from the official evaluation script for v1.1 of the SQuAD dataset.\n",
        "  Lower text and remove punctuation, articles and extra whitespace.\n",
        "  \"\"\"\n",
        "\n",
        "  def remove_articles(text):\n",
        "      return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
        "\n",
        "  def white_space_fix(text):\n",
        "      return \" \".join(text.split())\n",
        "\n",
        "  def remove_punc(text):\n",
        "      exclude = set(string.punctuation)\n",
        "      return \"\".join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "  def lower(text):\n",
        "      return text.lower()\n",
        "\n",
        "  return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def token_f1_score(prediction, ground_truth):\n",
        "  \"\"\"\n",
        "  Taken from the official evaluation script for v1.1 of the SQuAD dataset.\n",
        "  \"\"\"\n",
        "  prediction_tokens = normalize_answer(prediction).split()\n",
        "  ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "  common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "  num_same = sum(common.values())\n",
        "  if num_same == 0:\n",
        "      return 0\n",
        "  precision = 1.0 * num_same / len(prediction_tokens)\n",
        "  recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "  f1 = (2 * precision * recall) / (precision + recall)\n",
        "  return f1\n",
        "\n",
        "def extrai_resposta(resp):\n",
        "  idx_resposta_llm = resp.find('Answer: ') + 8\n",
        "  return resp[idx_resposta_llm:]\n",
        "\n",
        "# Trata respostas vazias do GPT-3.5\n",
        "def uniformiza_resposta_vazia(resp):\n",
        "  valores_possiveis = [\"n/a\", \"it is not possible to determine\", \"no information in the documents\", \"no information provided\", \"none of the documents\", \"documents do not provide\", \"unknown\", \"cannot be answered\", \"not mentioned\", \"not specified\", \"n/a\", \"not available\", \"insufficient data\", \"not enough information\", \"cannot be determined\", \"there is no mention\"]\n",
        "  \n",
        "  if resp == '':\n",
        "    return 'none'\n",
        "\n",
        "  resp = resp.lower()\n",
        "\n",
        "  for valor_none in valores_possiveis:\n",
        "    if valor_none in resp:\n",
        "      return 'none'\n",
        "  \n",
        "  return resp"
      ],
      "metadata": {
        "id": "TSPhHb9-olcQ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "col_queries = []\n",
        "col_resp_esperadas = []\n",
        "col_respostas_llm = []\n",
        "col_f1_score = []\n",
        "\n",
        "for idx_query in respostas_llm:\n",
        "  query = perg_resposta_esperada[idx_query]['pergunta']\n",
        "  resposta_esperada = normalize_answer(perg_resposta_esperada[idx_query]['resposta'])\n",
        "  resposta_llm = normalize_answer(uniformiza_resposta_vazia(extrai_resposta(respostas_llm[idx_query])))\n",
        "  f1_score = token_f1_score(resposta_llm, resposta_esperada)\n",
        "\n",
        "  #print(f'{query}\\n{resposta_esperada}\\n{resposta_llm}\\n---------------------------------------------')\n",
        "\n",
        "  col_queries.append(query)\n",
        "  col_resp_esperadas.append(resposta_esperada)\n",
        "  col_respostas_llm.append(resposta_llm)\n",
        "  col_f1_score.append(f1_score)\n",
        "\n",
        "df = pd.DataFrame({'Query': col_queries, 'Resp_esperada': col_resp_esperadas, 'Resposta_LLM': col_respostas_llm, 'F1_score': col_f1_score})\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "mZUHYHZNme90",
        "outputId": "acc28a88-d4e9-4382-d7ea-480a5934c1f3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                Query        Resp_esperada  \\\n",
              "0           What is Zeus know for in Greek mythology?  sky and thunder god   \n",
              "1   How long had the First World War been over whe...                    5   \n",
              "2   How long had Angela Scoular been acting profes...                    2   \n",
              "3   What is the capacity of the stadium where Brun...                26688   \n",
              "4           In which country was Wilhelm Müller born?              germany   \n",
              "..                                                ...                  ...   \n",
              "95  How far did Schooley travel from his hometown ...                 none   \n",
              "96  How old was the artist whose song Isa Raja san...                   36   \n",
              "97  What other ships were in the fleet with the Dr...                 none   \n",
              "98  How many years after Chess Records was founded...                    2   \n",
              "99  On what date did the Loma Prieta earthquake oc...             19891018   \n",
              "\n",
              "              Resposta_LLM  F1_score  \n",
              "0   god of sky and thunder  0.888889  \n",
              "1                     none  0.000000  \n",
              "2                  2 years  0.666667  \n",
              "3                     none  0.000000  \n",
              "4                  germany  1.000000  \n",
              "..                     ...       ...  \n",
              "95                    none  1.000000  \n",
              "96                    none  0.000000  \n",
              "97                    none  1.000000  \n",
              "98                 2 years  0.666667  \n",
              "99         october 17 1989  0.000000  \n",
              "\n",
              "[100 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3a1da62f-92fd-45f1-9758-47ab61edd0b5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Query</th>\n",
              "      <th>Resp_esperada</th>\n",
              "      <th>Resposta_LLM</th>\n",
              "      <th>F1_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is Zeus know for in Greek mythology?</td>\n",
              "      <td>sky and thunder god</td>\n",
              "      <td>god of sky and thunder</td>\n",
              "      <td>0.888889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How long had the First World War been over whe...</td>\n",
              "      <td>5</td>\n",
              "      <td>none</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How long had Angela Scoular been acting profes...</td>\n",
              "      <td>2</td>\n",
              "      <td>2 years</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What is the capacity of the stadium where Brun...</td>\n",
              "      <td>26688</td>\n",
              "      <td>none</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>In which country was Wilhelm Müller born?</td>\n",
              "      <td>germany</td>\n",
              "      <td>germany</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>How far did Schooley travel from his hometown ...</td>\n",
              "      <td>none</td>\n",
              "      <td>none</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>How old was the artist whose song Isa Raja san...</td>\n",
              "      <td>36</td>\n",
              "      <td>none</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>What other ships were in the fleet with the Dr...</td>\n",
              "      <td>none</td>\n",
              "      <td>none</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>How many years after Chess Records was founded...</td>\n",
              "      <td>2</td>\n",
              "      <td>2 years</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>On what date did the Loma Prieta earthquake oc...</td>\n",
              "      <td>19891018</td>\n",
              "      <td>october 17 1989</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3a1da62f-92fd-45f1-9758-47ab61edd0b5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3a1da62f-92fd-45f1-9758-47ab61edd0b5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3a1da62f-92fd-45f1-9758-47ab61edd0b5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dados com resposta esperada\n",
        "df_com_resp_esperada = df[df['Resp_esperada'] != 'none']\n",
        "\n",
        "print(len(df_com_resp_esperada))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMAYoeXssxtw",
        "outputId": "3da5b47a-a973-4c20-e33b-51700d920c68"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "69\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exact match\n",
        "print('Considerando todas as questões')\n",
        "print(sum(df['Resp_esperada'] == df['Resposta_LLM'])/len(df))\n",
        "print('\\nConsiderando só questões que tem uma resposta esperada')\n",
        "print(sum(df_com_resp_esperada['Resp_esperada'] == df_com_resp_esperada['Resposta_LLM'])/len(df_com_resp_esperada))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LM26DN9-pqFK",
        "outputId": "d9426bf4-01b8-4b1b-9fa1-0ebcd43da0f2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Considerando todas as questões\n",
            "0.31\n",
            "\n",
            "Considerando só questões que tem uma resposta esperada\n",
            "0.15942028985507245\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# F1 score\n",
        "print('Considerando todas as questões')\n",
        "print(df['F1_score'].mean())\n",
        "print('\\nConsiderando só questões que tem uma resposta esperada')\n",
        "print(df_com_resp_esperada['F1_score'].mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4Fd5iJruiBC",
        "outputId": "5bcb3f52-c3a0-4aa1-80d9-628bc01b1fb3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Considerando todas as questões\n",
            "0.3777222222222222\n",
            "\n",
            "Considerando só questões que tem uma resposta esperada\n",
            "0.2575684380032206\n"
          ]
        }
      ]
    }
  ]
}